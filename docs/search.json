[
  {
    "objectID": "index.html#requirements",
    "href": "index.html#requirements",
    "title": "twri r-manual",
    "section": "Requirements",
    "text": "Requirements\n\nR version 4.2 or higher. This can be installed from https://cran.r-project.org/ or using the Ivanti Installation Manager if you are on an AgriLife computer.\nRStudio Desktop is the integrated development environment used to write and execute R code. Available for free from RStudio or Posit. Admin privileges are required to install, so install from Ivanti on AgriLife computers.\nThe manual assumes some basic understanding of using R and RStudio, reach out to colleagues if you are having difficulty.\nMost of the functions and code make heavy use of the tidyverse ecosystem of packages and functions. In particular tibble, dplyr, ggplot2. You are encouraged to become familiar with these particular packages. If you are just starting out with R, the R for Data Science book (free!) is a recommended reading.\nProject oriented workflows are an absolute requirement. Create projects in RStudio that align with each project you work on. Store your source and raw data within the project. Export your processed data and figures within the project. The complexity of the project will vary according to the project needs and the analysts comfort in various R packages."
  },
  {
    "objectID": "intro.html#project-oriented-workflow",
    "href": "intro.html#project-oriented-workflow",
    "title": "\n1  Fundamentals\n",
    "section": "\n1.1 Project-oriented workflow",
    "text": "1.1 Project-oriented workflow\n\n\n\n\n\n\nNote\n\n\n\nThis subsection is largely based on Jenny Bryan’s What They Forgot (WTF) to Teach You About R.\n\n\n1 - Organize your work into a project. This means within the file system store all your data, code, figures, notes, and related material within the same folder and subfolders.\n2 - RStudio Projects enforces this approach. When you create a new project in RStudio, it creates a folder with some metadata and user options for that specific project (stored in the .Rproj file inside the folder it created for the project).\n3 - RStudio Projects establish a working directory and use relative file paths by default. Usually this is what you want so when you share a project or move it from one computer to the next, it. just. works. This is also why it is critical to store your data and scripts within the project.\nA typical project might have a file and folder structure like this:\n\n\n\n Project Folder\n|\n|-- Data\n|   |\n|   |-- Raw Data\n|   |-- More Raw Data\n|\n|-- Scripts\n|   |\n|   |-- Analysis.R file\n|   |-- Figures.R file\n|\n|-- Figures\n|   |\n|   |-- Plot\n|   |-- Another figure\n|\n|-- Exported Data\n|   |\n|   |-- Results\n|\n|--- Reports\n|   |\n|   |-- Report\n|\n|- Readme file (usually .Rmd, .md, or .txt file)\n\n\n\n\n\n1.1.1 Your RStudio Project\nStart a new project! Open RStudio, in the upper left click “File” -> “New Project.” We generally want to start a project in a New Directory, so click that. One the next window click New Project. Now you can choose the subdirectory name of your project (folder name) followed by where you want that subdirectory to be stored. Click “Create Project” and RStudio create the subdirectory and puts a .Rproj file with specific project info in there for you.\n\n\nExample RStudio workspace\n\n\nThe RStudio workspace includes 3 major components. In the upper left, the script area shows the content of open R scripts (or any text based file that you open will show up here). You can edit, save, and run lines of code from this window.\nAt the bottom left, is the R console. This is where R operates. The code you wrote in the script gets loaded into the console and R does whatever is in the script. Output, messages, and warnings from your R code will probably show up here.\nAt the bottom right, are a couple of tabs. This is where graphical outputs are displayed. There are also tabs for files, packages, and help. The file tab lets you navigate, create, delete, and open files and folders. It defaults to your projects working directory. The packages tab is for exploring the packages you have installed, more on that below. Technically you can load and unload packages from here by clicking boxes next to each package. Don’t do that. The help tab is just that, it lets you search functions in each package and displays the documentation for packages and functions. Learn to use this tab, it will help you just like it says!\n\n1.1.2 Running Code\nYou should generally write your code in the script window and execute it from there. This will save you from retyping code again and again.\nIf you have your cursor on an expression in your R script, use the keyboard shortcut: Ctrl+Enter to execute that expression. The cursor will automatically move to the next statement and the code will run in the console. If you want to execute the entire script at once, use the keyboard shortcut: Ctrl+Shift+S.\n\n1.1.3 Basic coding\nBoxes with the grey background and blue vertical bar indicate chunks of R code. If there is an output when that code chunk is run by R, the output (text output, tables or figures) will follow directly below the chunk. For example, here is a code chunk:\n\n10*100\n\nAnd this is the output:\n\n\n[1] 1000\n\n\n\n\n\n\n\n\nNote\n\n\n\nMuch of this subsection is from R for Data Science which you are encouraged to explore.\n\n\nThis: <-, is called an assignment operator in R. We use it to create objects by assigning a value to a variable name. We name objects so we can easily refer to whatever you assigned later on in your script:\n\nx <- 10\ny <- 100\n\nx * y\n\n[1] 1000\n\n\nYou don’t have to assign numbers:\n\nx <- \"Hello\"\n\nprint(x)\n\n[1] \"Hello\"\n\n\nAssignment operators go either direction, you might find it useful to use the left to right assinment operator in some situations:\n\n\"Hello\" -> x\n\nprint(x)\n\n[1] \"Hello\"\n\n\nHowever, for the most part, standard practice is to assign right to left so you can easily find the variable name receiving the value. Whatever you choose, use the same direction throughout your project.\nAs your scripts get more complicated, it is important to use descriptive object names. Object names can only contain letters, numbers, _, and ., so we recommend using “snake_case” to create object names:\nstreamflow\nstreamflow_site_a\nObject names are case sensitive, streamflow_site_a is not the same as streamflow_site_A.\nThe # symbol is ignored by R and used to include human readable comments in your script. Use comments liberally in your code.\n\n## I can write what I want\n## and R does not evaluate\n## this\na <- 1\na"
  },
  {
    "objectID": "intro.html#functions",
    "href": "intro.html#functions",
    "title": "\n1  Fundamentals\n",
    "section": "\n1.2 Functions",
    "text": "1.2 Functions\nFunctions are essentially tools that take input arguments and output some kind of value. Functions are the basis for most everything you do in R. For example, seq() is a function to generate a regular sequence of numbers. You can get to the help documentation by entering ?seq() in the console. It takes the arguments from, to, by, length.out, along.with. Use = for argument values:\n\nseq(from = 0, to = 10, by = 2)\n\n[1]  0  2  4  6  8 10\n\n\nWriting your own functions is one of the reasons for using R. Here is a simplistic function that generates a message in the console screen depending on the condition of the first argument.\n\nprint_hello <- function(x) {\n  if (x < 1) message(\"Hello!\")\n  else message(\"Bye\")\n}\n\nprint_hello(x = -1)\n\nHello!\n\nprint_hello(x = 1)\n\nBye\n\n\nWhy write a function in the first place? Sometimes you might need to repeatedly run the same set of functions on different data or subsets of data. You will find yourself copy and pasting code and changing some values within. If the output is dependent on some values you forgot to change when you cut and paste, instant problems! Functions let you skip that copy and paste action, and just update the arguments. Here is an example of some code to calculate the confidence interval around the mean for a vector of numbers:\n\nmin <- 0\nmax <- 10\nn <- 1000\nci <- 0.95\nx <- runif(n = n, min = min, max = max)\nse <- sd(x)/sqrt(length(x))\nalpha <- 1 - ci\nmean(x) + se * qnorm(c(alpha/2, 1-alpha/2))\n\n[1] 5.026550 5.377243\n\n\nIf we need to recalculate the confidence interval for different values or combinations of values of x, n, and ci we would have to cut and paste the chunk each time with the potential for data entry errors if the wrong values are entered. Instead, create a function and change the arguments as needed.\n\nci <- function(min, max, n, ci) {\n  x <- runif(n = n, min = min, max = max)\n  se <- sd(x)/sqrt(length(x))\n  alpha <- 1 - ci\n  mean(x) + se * qnorm(c(alpha/2, 1-alpha/2))\n}\n\nci(min = 0, max = 10, n = 1000, ci = 0.95)\n\n[1] 4.689721 5.044329\n\nci(min = 10, max = 100, n = 1000, ci = 0.90)\n\n[1] 55.14154 57.86932\n\nci(min = 10, max = 1000, n = 1000, ci = 0.80)\n\n[1] 493.4929 516.3571"
  },
  {
    "objectID": "intro.html#packages",
    "href": "intro.html#packages",
    "title": "\n1  Fundamentals\n",
    "section": "\n1.3 Packages",
    "text": "1.3 Packages\nPackages might be considered the toolboxes of R. They are generally a collection of functions and classes the expand the capabilities of the base R functions. Many packages have dependencies from other packages. This mean when you install one package, you may end up installing multiple other packages automatically that are required for the package that you chose to work. Normally this works without hiccup. However, before installing packages, I suggest restarting your R session and make sure no packages are currently loaded to prevent issues.\nMost packages can and should be installed from the CRAN repository. These are a network of repositories that host the official, up-to-date and approved packages for R. This packages are pre-built, meaning you are unlikely to run into issues on installation. To install packages from CRAN, you typically do something like the following:\n\n## install one package\ninstall.packages(\"ggplot2\")\n\n## install multiple packages\ninstall.packages(\"dplyr\", \"tidyr\")\n\nSometimes you need a package or package version that is not currently available on CRAN. There are various justifiable reasons the packages might not be available on CRAN; however, one of the benefits of using CRAN packages is that they are all reviewed by a person before acceptance. This provides a safety mechanism for not only the quality of the package but potential security issues.\n\n\n\n\n\n\nNote\n\n\n\nIf you are installing a package from GitHub or other source, please review it for safety and quality before installation.\n\n\nThere are two primary way to install non-CRAN packages. The preferred method is to install pre-built packages from an alternative repository like r-universe. The readme file associated with the package will generally inform you if the package is available on a different repository and how to install it from that repository.\nAn example of this is shown below for the adc package:\n\ninstall.packages('adc', repos = c(txwri = 'https://txwri.r-universe.dev'))\n\nAn alternative option is to download and build the packages from the source, such as GitHub. For those on Windows, you will need to install the RTools toolchain. Then, we can use the remotes package to download, build and install a package from GitHub:\n\ninstall.packages(\"remotes\")\nremotes::install_github(\"mps9506/rATTAINS\")\n\nAfter you install a package, you need to load the package in order to use the functions. Confusingly, you use the library() function to accomplish this. Standard practice is to load libraries at the top of your script:\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)"
  },
  {
    "objectID": "intro.html#other-coding-conventions",
    "href": "intro.html#other-coding-conventions",
    "title": "\n1  Fundamentals\n",
    "section": "\n1.4 Other Coding Conventions",
    "text": "1.4 Other Coding Conventions\n\n\n\n\n\n\nIncomplete\n\n\n\nI still need to add discussion on strings, date formats, and the pipe function."
  },
  {
    "objectID": "intro.html#suggested-rstudio-settings",
    "href": "intro.html#suggested-rstudio-settings",
    "title": "\n1  Fundamentals\n",
    "section": "\n1.5 Suggested RStudio Settings",
    "text": "1.5 Suggested RStudio Settings\n\n\n\n\n\n\nIncomplete\n\n\n\nI still need to add discussion on\n\nsetting global and project options in RStudio\nusing ragg graphics device (this might be better in figure section)"
  },
  {
    "objectID": "data_exploration.html#import-data",
    "href": "data_exploration.html#import-data",
    "title": "\n2  Data Exploration\n",
    "section": "\n2.1 Import Data",
    "text": "2.1 Import Data\nOne of the first steps in any data analysis project is importing or reading data into R. For now, we will focus on reading in data from comma separated value (CSV), tab seperated value (TSV), and similar delimited text files. For most projects, it is useful to work off a singular data snapshot. So once you download your data and have it in your project, do not manipulate the data file1.1 Later on we will discuss loading web data sources directly into R, but for most of our work your best bet is to store raw data files inside the project\n\n\n\n\n\n\nDownload Data\n\n\n\nIf you want to work along, download the following example data export it directly into the data folder in your R project: Data\n\n\nUse the read_csv() function from the readr package to import the data and assign it to a variable. The console will print some information telling you what type of variable it made each column of the csv and if there are any problems.\n\ndf <- read_csv(file = \"data/easterwood.csv\")\n\nRows: 4045 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): name\ndbl  (3): station, dailymaximumdrybulbtemperature, dailyprecipitation\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nread_csv() guesses the column types and sometimes it can guess wrong, so the safest option is to tell it what to expect using the col_types argument. This argument takes a string where each character represents one column. Here we will tell it to expect a character, Date, character, number, number which is represented as cDcnn.\n\ndf <- read_csv(file = \"data/easterwood.csv\",\n               col_types = \"cDcnn\")\ndf\n\n# A tibble: 4,045 × 5\n   station     date       name                 dailymaximumdry… dailyprecipitat…\n   <chr>       <date>     <chr>                           <dbl>            <dbl>\n 1 74746003904 2010-07-31 COLLEGE STATION EAS…               99                0\n 2 74746003904 2010-08-01 COLLEGE STATION EAS…              102                0\n 3 74746003904 2010-08-02 COLLEGE STATION EAS…              101                0\n 4 74746003904 2010-08-03 COLLEGE STATION EAS…              100                0\n 5 74746003904 2010-08-04 COLLEGE STATION EAS…               99                0\n 6 74746003904 2010-08-05 COLLEGE STATION EAS…               99                0\n 7 74746003904 2010-08-06 COLLEGE STATION EAS…               99                0\n 8 74746003904 2010-08-07 COLLEGE STATION EAS…               99                0\n 9 74746003904 2010-08-08 COLLEGE STATION EAS…               99                0\n10 74746003904 2010-08-09 COLLEGE STATION EAS…              100                0\n# … with 4,035 more rows\n\n\nTSV and other delimited files are read in the same way but with read_delim() or read_tsv(). If your data is in .xlsx format, the readxl pacakge is required. In readxl there is a read_xlsx() function that works similar to the read_csv() function except you can specify the sheet and range of cells to read from. The col_types argument also needs to be spelled out as a character vector such as col_types = c(\"text\", \"date\", \"text\", \"numeric\", \"numeric\").\nIf you have multiple files with the same column names you can read them into the same data frame. Here we read in multiple files and are more explicit about defining the column types:\n\nfiles <- c(\"data/marabwq2020.csv\", \"data/marabwq2021.csv\", \"data/marcewq2020.csv\", \"data/marcewq2021.csv\")\ndf <- read_csv(file = files, \n               col_types = cols_only(\n                 StationCode = col_factor(),\n                 DateTimeStamp = col_datetime(format = \"%m/%e/%Y %H:%M\"),\n                 Temp = col_number(),\n                 F_Temp = col_character(),\n                 SpCond = col_number(),\n                 F_SpCond = col_character(),\n                 Sal = col_number(),\n                 F_Sal = col_character(),\n                 DO_mgl = col_number(),\n                 F_DO_mgl = col_character()\n               ))\ndf\n\n# A tibble: 140,352 × 10\n   StationCode DateTimeStamp        Temp F_Temp SpCond F_SpCond   Sal F_Sal\n   <fct>       <dttm>              <dbl> <chr>   <dbl> <chr>    <dbl> <chr>\n 1 marabwq     2020-01-01 00:00:00  15.5 <0>      38.2 <0>       24.3 <0>  \n 2 marabwq     2020-01-01 00:15:00  15.5 <0>      38.2 <0>       24.3 <0>  \n 3 marabwq     2020-01-01 00:30:00  15.5 <0>      38.2 <0>       24.3 <0>  \n 4 marabwq     2020-01-01 00:45:00  15.5 <0>      38.2 <0>       24.3 <0>  \n 5 marabwq     2020-01-01 01:00:00  15.5 <0>      38.2 <0>       24.3 <0>  \n 6 marabwq     2020-01-01 01:15:00  15.5 <0>      38.2 <0>       24.4 <0>  \n 7 marabwq     2020-01-01 01:30:00  15.5 <0>      38.3 <0>       24.4 <0>  \n 8 marabwq     2020-01-01 01:45:00  15.5 <0>      38.3 <0>       24.4 <0>  \n 9 marabwq     2020-01-01 02:00:00  15.5 <0>      38.3 <0>       24.4 <0>  \n10 marabwq     2020-01-01 02:15:00  15.5 <0>      38.3 <0>       24.4 <0>  \n# … with 140,342 more rows, and 2 more variables: DO_mgl <dbl>, F_DO_mgl <chr>\n\n\n\n\nThis dataset is from the NOAA NERRS Centralized Data Management Office and provides measures of water quality parameters in the Mission and Aransas (MAR) National Estuarine Research Reserve (NERR)(NOAA National Estuarine Research Reserve System (NERRS) 2022).\nIn this data we have a row for each date/time observation with associated columns for Station, Temperature, Specific Conductance, Salinity, and Dissolved Oxygen. The columns with the F_ prefix are qa/qc flags, <0> is accepted data."
  },
  {
    "objectID": "data_exploration.html#plot-and-clean-data",
    "href": "data_exploration.html#plot-and-clean-data",
    "title": "\n2  Data Exploration\n",
    "section": "\n2.2 Plot and Clean Data",
    "text": "2.2 Plot and Clean Data\nggplot2 is the graphics package for producing layered plots. The underlying philosophy of ggplot2 is to iteratively build your plots layer by layer which allows for some sophisticated plots. I won’t go into full details of using ggplot2 but lots of information is available in the ggplot2 book.\nThree key things you need to learn: data, aesthetics, geoms.\n\ndata: a data frame or tibble with the data you want to plot.\naesthetics mappings (or aes): specify how variables will be visually depicted (x, y, color, shape, size, etc.).\ngeoms: are the layers that define how each layer is rendered (points, lines, bars, etc.).\n\nUsing the data we imported from above we can quickly create a basic scatter plot. Note the use of the + symbol to iteratively add layers to our plot. First we specify the data, then the geometry, and the aesthetic for that geom:\n\np1 <- ggplot(data = df, aes(x = DateTimeStamp, y = Temp)) +\n  geom_point()\np1\n\nWarning: Removed 4975 rows containing missing values (geom_point).\n\n\n\n\n\nThe above figure demonstrates the importance of plotting your data, the -100 value is clearly an issue. One thing we can do is filter the data to try and track down the issue. The dplyr package provides a number of functions to explore your data. Here, the data is filtered on the Temp column to include data less than -99:\n\ndf |> \n  filter(Temp < -99)\n\n# A tibble: 1 × 10\n  StationCode DateTimeStamp        Temp F_Temp       SpCond F_SpCond   Sal F_Sal\n  <fct>       <dttm>              <dbl> <chr>         <dbl> <chr>    <dbl> <chr>\n1 marcewq     2021-08-23 20:45:00  -100 <-3> [GIM] … -1211. <-3> [G…   -99 <-3>…\n# … with 2 more variables: DO_mgl <dbl>, F_DO_mgl <chr>\n\n\nThe F_Temp qa flag is < -3 > indicating QA rejected the data point. So, let’s update the data to remove data that doesn’t have the < 0 > data flag:\n\ndf <- df |> \n  filter(F_Temp == \"<0>\")\ndf\n\n# A tibble: 111,090 × 10\n   StationCode DateTimeStamp        Temp F_Temp SpCond F_SpCond   Sal F_Sal\n   <fct>       <dttm>              <dbl> <chr>   <dbl> <chr>    <dbl> <chr>\n 1 marabwq     2020-01-01 00:00:00  15.5 <0>      38.2 <0>       24.3 <0>  \n 2 marabwq     2020-01-01 00:15:00  15.5 <0>      38.2 <0>       24.3 <0>  \n 3 marabwq     2020-01-01 00:30:00  15.5 <0>      38.2 <0>       24.3 <0>  \n 4 marabwq     2020-01-01 00:45:00  15.5 <0>      38.2 <0>       24.3 <0>  \n 5 marabwq     2020-01-01 01:00:00  15.5 <0>      38.2 <0>       24.3 <0>  \n 6 marabwq     2020-01-01 01:15:00  15.5 <0>      38.2 <0>       24.4 <0>  \n 7 marabwq     2020-01-01 01:30:00  15.5 <0>      38.3 <0>       24.4 <0>  \n 8 marabwq     2020-01-01 01:45:00  15.5 <0>      38.3 <0>       24.4 <0>  \n 9 marabwq     2020-01-01 02:00:00  15.5 <0>      38.3 <0>       24.4 <0>  \n10 marabwq     2020-01-01 02:15:00  15.5 <0>      38.3 <0>       24.4 <0>  \n# … with 111,080 more rows, and 2 more variables: DO_mgl <dbl>, F_DO_mgl <chr>\n\n\nThis removes about 29,263 observations. Try plotting again:\n\np1 <- ggplot(data = df, aes(x = DateTimeStamp, y = Temp)) +\n  geom_point()\np1\n\n\n\n\nWe can use different geoms to explore the data:\n\np2 <- ggplot(data = df, aes(x = Temp)) +\n  geom_histogram(binwidth = 1)\np2\n\n\n\n\n\nggplot(data = df, aes(y = Temp, x = StationCode)) +\n  geom_boxplot()\n\n\n\n\nTo explore relationships between two variables, use geom_point and geom_smooth with each variable mapped to x and y. In this example, ggplot2 prints a message indicating 834 rows of data had missing or NA values that could not be plotted. geom_mooth will plot the smooth line (using a loess or gam smooth) between two variables. If you want the linear regression drawn use the argument method = \"lm\".\n\np3 <- ggplot(data = df, aes(x = Temp, y = DO_mgl)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(se = FALSE)\np3\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 834 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 834 rows containing missing values (geom_point).\n\n\n\n\n\nThis is a good example to introduce the other important aesthetics in ggplot2. There is a clear negative relationship between Temperature and Dissolved Oxygen. Our data includes two sites, is there a difference between the two sites? We can map color to the site variable which will color each observation and each smooth a different color. Although shape and color can be used inside the aes() mapping function, you can assign them a value in the geom directly. Here we asign values for the shape and alpha properties in the point geom.\n\np3 <- ggplot(data = df, aes(x = Temp, y = DO_mgl, color = StationCode)) +\n  geom_point(shape = 21, alpha = 0.05) +\n  geom_smooth(se = FALSE)\np3\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 834 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 834 rows containing missing values (geom_point).\n\n\n\n\n\nThere was no reason to expect a difference and there isn’t. However the plot is muddy because there are so many overlying data points. We can also facet the graph by a variable. The following code also shows how you can just add another layer to your existing ggplot object:\n\np3 <- p3 +\n  facet_wrap(~StationCode)\n\np3\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 834 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 834 rows containing missing values (geom_point).\n\n\n\n\n\nAs you can see, ggplot2 allows you to rapidly iterate plots to explore data. When exploring the data, the formatting might not matter much, but if you want to export plots, we also need to take care of labels and general plot visual preferences.\n\np3 <- p3 +\n  labs(x = \"Temperature [°F]\", y = \"Dissolved Oxygen [mg/L]\") +\n  scale_colour_brewer(name = \"Stations\",\n                      palette = \"Dark2\",\n                      labels = c(\"Aransas Bay\", \"Copano East\"))\np3\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 834 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 834 rows containing missing values (geom_point).\n\n\n\n\n\n\n2.2.1 Themes\nggplot2 has a number of built-in themes. For most of our use cases (reports and papers), the grey background and white lines are not great choices. We recommend using the theme_bw() function at the bare minimum:\n\np3 +\n  theme_bw()\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 834 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 834 rows containing missing values (geom_point).\n\n\n\n\n\nThe twriTemplates package is available with a custom theme and some color palettes that are consistent with the Institute’s branding:\n\ninstall.packages(\"twriTemplates\", \n                 repos = c(txwri = 'https://txwri.r-universe.dev',\n                           CRAN = 'https://cloud.r-project.org'))\n\n\nlibrary(twriTemplates)\np3 +\n  theme_TWRI_print() +\n  scale_color_discrete_twri(name = \"Stations\",\n                            labels = c(\"Aransas Bay\", \"Copano East\"))\n\nWarning: Removed 834 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 834 rows containing missing values (geom_point).\n\n\n\n\n\nThe facet labels are still not very useful. There isn’t an easy way to change this using ggplot2. Instead we can create a new variable using the mutate() function in dplyr. There is more discussion about dplyr in the next section.\n\ndf <- df |> \n  mutate(StationName = case_when(\n    StationCode == \"marabwq\" ~ \"Aransas Bay\",\n    StationCode == \"marcewq\" ~ \"Copano Bay East\"\n  ))\n\nggplot(data = df,\n       aes(x = Temp, y = DO_mgl, color = StationName)) +\n  geom_point(shape = 21, alpha = 0.05) +\n  geom_smooth(se = FALSE) +\n  facet_wrap(~StationName) +\n  labs(x = \"Temperature [°F]\", y = \"Dissolved Oxygen [mg/L]\") +\n  scale_color_discrete_twri(name = \"Stations\") +\n  theme_TWRI_print()\n\nWarning: Removed 834 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 834 rows containing missing values (geom_point)."
  },
  {
    "objectID": "data_exploration.html#manipulate-and-summarize-data",
    "href": "data_exploration.html#manipulate-and-summarize-data",
    "title": "\n2  Data Exploration\n",
    "section": "\n2.3 Manipulate and Summarize Data",
    "text": "2.3 Manipulate and Summarize Data\ndplyr is a powerful package designed for working with dataframes and tibbles. Read the dplyr introduction to get a better sense of all the available functions. Essentially dplyr provides “verb” functions that correspond to how you want to manipulate the dataframe.\nCommonly used verbs include:\n\n\nfilter(): subset the rows based on value\n\nmutate(): create a new column\n\nselect(): subset columns based on names or types\n\ngroup_by(): group the data by variable values\n\nsummarise(): summarise the grouped or ungrouped data based on user provided functions\n\nThere are many other functions, but these cover many use cases for people just getting started. dplyr also provides functions for joining datasets based on common variables, changing the order of a dataframe, calculating cumulative statistics, calculating lag and leading values, and rolling or window rank functions among others. All of these can be very useful to become familiar with.\nIn this example I want to explore seasonal differences in the water quality parameters. mutate() is used to create new variables. ::: {.cell}\nsummary_plot <- df |> \n  # returns DateTimeStamp Value as the month only\n  # see ?strptime() for more info\n  mutate(Month = as_factor(format(DateTimeStamp, \"%B\")),\n         StationName = case_when(\n           StationCode == \"marabwq\" ~ \"Aransas Bay\",\n           StationCode == \"marcewq\" ~ \"Copano Bay East\")) |> \n  ## months have an inherent order, right now it is alphabetical\n  ## this adds the correct order so it plots nicely\n  mutate(Month = lvls_revalue(Month, month.name)) |> \n  ggplot() +\n  geom_boxplot(aes(x = Month, y = DO_mgl, fill = StationName)) +\n  scale_fill_discrete_twri(name = \"Stations\") +\n  theme_TWRI_print()\nsummary_plot\n\nWarning: Removed 834 rows containing non-finite values (stat_boxplot).\n\n\n\n\n:::\nNow we have a summary figure, but what about the data? We can use the summarize() or summarise() function (depending on your English preference) to calculate some summary statistics for each group.\n\nsummary_table <- df |> \n  mutate(Month = as_factor(format(DateTimeStamp, \"%B\")),\n         StationName = case_when(\n           StationCode == \"marabwq\" ~ \"Aransas Bay\",\n           StationCode == \"marcewq\" ~ \"Copano Bay East\")) |> \n  mutate(Month = lvls_revalue(Month, month.name)) |> \n  ## create groups to calculate summary stats on\n  group_by(StationName, Month) |> \n  ## calculate the summary stats\n  ## returns a column for each stat with a row for each group\n  summarize(Mean = mean(DO_mgl),\n            Max = max(DO_mgl),\n            Min = min(DO_mgl),\n            ## no standard error function in base R\n            ## can calculate manually here\n            SE = sd(DO_mgl)/sqrt(length(DO_mgl)))\n\n`summarise()` has grouped output by 'StationName'. You can override using the\n`.groups` argument.\n\nsummary_table\n\n# A tibble: 24 × 6\n# Groups:   StationName [2]\n   StationName Month      Mean   Max   Min      SE\n   <chr>       <fct>     <dbl> <dbl> <dbl>   <dbl>\n 1 Aransas Bay January    8.47  12.1   6.7 0.00870\n 2 Aransas Bay February   8.96  13.7   6.4 0.0171 \n 3 Aransas Bay March      7.57  10.4   5.6 0.00722\n 4 Aransas Bay April      7.12   8.5   5.9 0.00738\n 5 Aransas Bay May        6.75   9     4.8 0.00839\n 6 Aransas Bay June       5.98   8.1   2.2 0.0109 \n 7 Aransas Bay July       5.32   8.2   0.2 0.0180 \n 8 Aransas Bay August     5.45   7.4   0.2 0.0114 \n 9 Aransas Bay September  5.88   9.2   2.3 0.0111 \n10 Aransas Bay October    6.67   9.6   3.2 0.00959\n# … with 14 more rows"
  },
  {
    "objectID": "data_exploration.html#export-data",
    "href": "data_exploration.html#export-data",
    "title": "\n2  Data Exploration\n",
    "section": "\n2.4 Export Data",
    "text": "2.4 Export Data\n\n2.4.1 Figures\nYou have created a figure and a table in R. Now you probably need to put it in a report, website, or other self contained file for your boss to look at. For figures created with ggplot2, the ggsave() option is the easiest option. First I reccomend installing, the ragg package because if it available, ggsave() will use it. ragg provides higher performance and higher quality images than the graphics devices available in base R.\n\n# install ragg\ninstall.packages(\"ragg\")\nlibrary(ragg)\n\n## ggsave defaults to the last plot displayed\n## it is safer to specify which plot you want saved\nggsave(filename = \"figures/boxplot_mare.png\",\n       plot = summary_plot,\n       width = 8,\n       height = 6,\n       dpi = 300,\n       units = \"in\")\n\nIf you have a figure created from something other than a ggplot object, then we need to use the ragg graphics device to save it as a file (this also works with ggplot as demonstrated here). If you don’t have ragg installed, substitute png() for agg_png().\n\n# first function creates the device\nagg_png(filename = \"figures/boxplot_mare.png\",\n    width = 8,\n    height = 6,\n    res = 300,\n    units = \"in\")\n# copy the plot to the file\nsummary_plot\n# turns off the device\ndev.off()\n\n\n2.4.2 Tables\nreadr and writexl packages provide the functions required to export data frames to text delimited or Microsoft Excel files.\n\n## write a csv\nwrite_csv(x = summary_table,\n          file = \"export_data/monthly_summary_mare.csv\")\n\n## or an Excel file\n## uncomment the next line if writexl is not installed\n# install.packages(\"writexl\")\n\nlibrary(writexl)\n# one sheet\nwrite_xlsx(x = summary_table,\n           path = \"export_data/monthly_summary_mare.xlsx\")\n\n# or specify sheets\nwrite_xlsx(x = list(summarysheet = summary_table,\n                    rawdatasheet = df),\n           path = \"export_data/monthly_summary_mare.xlsx\")\n\n\n\n\n\nNOAA National Estuarine Research Reserve System (NERRS). 2022. System-Wide Monitoring Program. NOAA NERRS Centralized Data Management Office. http://cdmo.baruch.sc.edu/."
  },
  {
    "objectID": "streamflow.html#data-sources",
    "href": "streamflow.html#data-sources",
    "title": "3  Streamflow",
    "section": "3.1 Data Sources",
    "text": "3.1 Data Sources"
  },
  {
    "objectID": "streamflow.html#summarise",
    "href": "streamflow.html#summarise",
    "title": "3  Streamflow",
    "section": "3.2 Summarise",
    "text": "3.2 Summarise"
  },
  {
    "objectID": "streamflow.html#flow-duration-curve",
    "href": "streamflow.html#flow-duration-curve",
    "title": "3  Streamflow",
    "section": "3.3 Flow Duration Curve",
    "text": "3.3 Flow Duration Curve"
  },
  {
    "objectID": "stage-discharge.html#data",
    "href": "stage-discharge.html#data",
    "title": "\n4  Stage-Discharge Rating Curves\n",
    "section": "\n4.1 Data",
    "text": "4.1 Data\nThis example uses field measurements of stream stage and instantaneous discharge measured at the COMO NEON field site (NEON (National Ecological Observatory Network), n.d.). Raw data is available in the neon.rds file within the tutorial download.\n\nlibrary(tidyverse)\nlibrary(twriTemplates)\n\nexample_data <- read_rds(\"data/neon.rds\")\nstage_discharge <- example_data$dsc_fieldData\nglimpse(stage_discharge)\n\nRows: 134\nColumns: 36\n$ uid                     <chr> \"1068e9ad-37bd-42ac-96d2-b999545adefe\", \"06d63…\n$ recorduid               <chr> \"3b2d4830-50cd-40c8-a772-790d2386c33d\", \"5ed72…\n$ domainID                <chr> \"D13\", \"D13\", \"D13\", \"D13\", \"D13\", \"D13\", \"D13…\n$ siteID                  <chr> \"COMO\", \"COMO\", \"COMO\", \"COMO\", \"COMO\", \"COMO\"…\n$ namedLocation           <chr> \"COMO.AOS.discharge\", \"COMO.AOS.discharge\", \"C…\n$ collectedBy             <chr> \"LCLARK\", \"HSCHARTEL\", \"DMONAHAN\", \"HSCHARTEL\"…\n$ startDate               <dttm> 2015-08-11 15:30:00, 2015-08-24 16:16:00, 201…\n$ collectDate             <dttm> 2015-08-11 15:30:00, 2015-08-24 16:16:00, 201…\n$ streamStage             <dbl> 0.30, 0.34, 0.33, 0.31, 0.32, 0.35, 0.37, 0.74…\n$ streamStageUnits        <chr> \"meter\", \"meter\", \"meter\", \"meter\", \"meter\", \"…\n$ handheldDeviceID        <chr> \"122581001261\", \"122581001261\", \"122581001261\"…\n$ velocitySensorID        <chr> \"132660300436\", \"132660300436\", \"132660300436\"…\n$ filterParamTime         <dbl> 10, 10, 10, 10, 10, 10, 10, 10, 10, 20, 10, 10…\n$ stationEntryTest        <chr> \"Non-fixed\", \"Non-fixed\", \"Non-fixed\", \"Non-fi…\n$ flowCalculation         <chr> \"Mid-section\", \"Mid-section\", \"Mid-section\", \"…\n$ waterEdge               <chr> \"Right edge water\", \"Right edge water\", \"Right…\n$ totalDischarge          <dbl> 10.81, 5.71, 6.58, 3.49, 3.15, 9.39, 12.75, 77…\n$ totalDischargeUnits     <chr> \"litersPerSecond\", \"litersPerSecond\", \"litersP…\n$ samplingProtocolVersion <chr> \"NEON.DOC.001085vB\", \"NEON.DOC.001085vB\", \"NEO…\n$ averageVelocityUnits    <chr> \"meterPerSecond\", \"meterPerSecond\", \"meterPerS…\n$ waterDepthUnits         <chr> \"meter\", \"meter\", \"meter\", \"meter\", \"meter\", \"…\n$ tapeDistanceUnits       <chr> \"meter\", \"meter\", \"meter\", \"meter\", \"meter\", \"…\n$ flowCalcQF              <chr> \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"…\n$ dischargeUnitsQF        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ streamStageUnitsQF      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ averageVelocityUnitsQF  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ waterDepthUnitsQF       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ tapeDistanceUnitsQF     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ lowVelocityFinalQF      <dbl> 5, 20, 0, 10, 0, 17, 0, 45, 0, 11, 25, 11, 6, …\n$ finalDischarge          <dbl> 10.71, 5.62, 6.52, 3.44, 3.07, 9.29, 12.75, 78…\n$ totalDischargeCalcQF    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ profileName             <chr> \"COMO\", \"COMO\", \"COMO\", \"COMO\", \"COMO\", \"COMO\"…\n$ stageImpractical        <chr> \"OK\", \"OK\", \"OK\", \"OK\", \"OK\", \"OK\", \"OK\", \"OK\"…\n$ dataQF                  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ publicationDate         <chr> \"20211222T003739Z\", \"20211222T003739Z\", \"20211…\n$ release                 <chr> \"RELEASE-2022\", \"RELEASE-2022\", \"RELEASE-2022\"…"
  },
  {
    "objectID": "stage-discharge.html#plot-data",
    "href": "stage-discharge.html#plot-data",
    "title": "\n4  Stage-Discharge Rating Curves\n",
    "section": "\n4.2 Plot Data",
    "text": "4.2 Plot Data\nFirst step is to plot the data.\n\n# plot observations over time\nggplot(stage_discharge) +\n  geom_point(aes(collectDate, finalDischarge)) +\n  labs(x = \"Date\", y = \"Discharge [L/s]\") +\n  theme_TWRI_print()\n\nWarning: Removed 9 rows containing missing values (geom_point).\n\n\n\n\n# plot the stage and discharge relationship\nggplot(stage_discharge) +\n  geom_point(aes(streamStage, finalDischarge)) +\n  labs(x = \"Stage [m]\", y = \"Discharge [L/s]\") +\n  scale_x_log10() +\n  scale_y_log10() +\n  theme_TWRI_print()\n\nWarning: Removed 9 rows containing missing values (geom_point).\n\n\n\n\n\nLooks like there might be a shift in the rating curve at some point. Use dplyr to create a new year variable and the gghighlight package to explore it.\n\n# install.packages(\"gghighlight\")\nlibrary(gghighlight)\nstage_discharge |> \n  filter(!is.na(finalDischarge)) |> \n  mutate(year = format(collectDate, format = \"%Y\")) |> \n  ggplot() +\n  geom_point(aes(streamStage, finalDischarge, color = year)) +\n  facet_wrap(~year) +\n  gghighlight(max_highlight = 8,\n              use_direct_label = FALSE,\n              calculate_per_facet = FALSE)   +\n  labs(x = \"Stage [m]\", y = \"Discharge [L/s]\") +\n  theme_TWRI_print()"
  },
  {
    "objectID": "stage-discharge.html#fit-rating-curve",
    "href": "stage-discharge.html#fit-rating-curve",
    "title": "\n4  Stage-Discharge Rating Curves\n",
    "section": "\n4.3 Fit Rating Curve",
    "text": "4.3 Fit Rating Curve\nIt appears the rating curve shifted after 2016 and possibly after 2018. In practice we would fit different rating curves based on this information. For this example the rating curve will only be fit to 2019-2021 data. We will use this data to estimate the \\(K\\), \\(H_0\\) and \\(Z\\) parameters in the power function described at the top of the chapter using nonlinear least squares.\n\n# install.packages(\"nls.multstart\")\n# install.packages(\"nlstools\")\nlibrary(nls.multstart)\nlibrary(nlstools)\n\n# clean the data a little bit and filter\nstage_discharge <- stage_discharge |> \n  filter(!is.na(finalDischarge)) |>\n  mutate(year = format(startDate, \"%Y\")) |> \n  filter(year >= 2018) |> \n  # convert units to feet and cfs\n  mutate(streamStage = streamStage * 3.28084,\n         finalDischarge = finalDischarge * 0.0353147)\n\n# Set the equation\nf_Q <- formula(finalDischarge ~ K*(streamStage - H_0)^Z)\n\n# Some initial starting values\nstart_lower <- c(K = -10, Z = -10, H_0 = 0.02)\nstart_upper <- c(K = 10, Z = 10, H_0 = 1)\n\n# nonlinear least squares\nm1 <- nls_multstart(f_Q,\n          data = stage_discharge,\n          iter = 1000,\n          start_lower = start_lower,\n          start_upper = start_upper,\n          supp_errors = 'Y',\n          lower = c(K = -10, Z = -10, H_0 = 0),\n          control = minpack.lm::nls.lm.control(maxiter = 1000L))\n\nsummary(m1)\n\n\nFormula: finalDischarge ~ K * (streamStage - H_0)^Z\n\nParameters:\n    Estimate Std. Error t value Pr(>|t|)    \nK     5.0399     1.3421   3.755 0.000335 ***\nH_0   0.7874     0.1315   5.990 6.30e-08 ***\nZ     1.9097     0.2568   7.436 1.23e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.889 on 77 degrees of freedom\n\nNumber of iterations to convergence: 38 \nAchieved convergence tolerance: 1.49e-08\n\n\nNLS estimated parameters are: \\(K =\\) 5.0398733, \\(H_0 =\\) 0.7874016, and \\(Z =\\) 0.7874016. Before using these parameter, evaluated the goodness of fit using the model residuals (add citation or note with more resources here).\n\nShow the code# for easy multipanel plots, use the patchwork package\n#install.packages(\"patchwork\")\nlibrary(patchwork)\n\nstd_resids <- as_tibble(nlsResiduals(m1)$resi2)\n\nstage_discharge <- stage_discharge |> \n    mutate(fits = std_resids$`Fitted values`,\n         residuals = std_resids$`Standardized residuals`)\n\n\np1 <- ggplot(stage_discharge) +\n  geom_density(aes(residuals)) +\n  labs(x = \"Standardized Residuals\",\n       y = \"Count\",\n       subtitle = \"Distribution of standardized residuals\") +\n  theme_TWRI_print()\n\n\np2 <- ggplot(stage_discharge) +\n  geom_point(aes(streamStage, residuals), color = \"steelblue\", alpha = 0.4) +\n  labs(x = \"Stream Height [ft]\",\n       y = \"Standardized Residuals\",\n       subtitle = \"Residuals against stream height\") +\n  theme_TWRI_print()\n\np3 <- ggplot(stage_discharge) +\n  geom_point(aes(fits, finalDischarge), color = \"steelblue\", alpha = 0.4) +\n  labs(x = \"Model Fits\",\n       y = \"Measured Discharge [cfs]\",\n       subtitle = \"Measured against fitted\") +\n  theme_TWRI_print()\n\np4 <- ggplot(stage_discharge) +\n  stat_qq(aes(sample = residuals), color = \"steelblue\", alpha = 0.4) +\n  stat_qq_line(aes(sample = residuals)) +\n  labs(x = \"Theoretical\",\n       y = \"Standardized Residuals\",\n       subtitle = \"Sample Quantile against theoretical quantile\") +\n  theme_TWRI_print()\n\n\n# patchwork allows us to assemble plots using + and /\n\n(p1 + p2) / (p3 + p4)\n\n\n\n\nThe plots indicate increasing residual variance as stream height increases and heavy tails in Q-Q plot. Two options from here are to (1) fit a rating curve per year or (2) fit a piece-wise log-linear model that assumes a different relationship along different parts of the rating curve. Because of the large residual variance at the top of curve, I think fitting a curve per year will do the job. You may also choose to fit models by season (small streams in particular may see large changes in rating curves do to changes in stream bank vegetation)."
  },
  {
    "objectID": "stage-discharge.html#fit-multiple-curves",
    "href": "stage-discharge.html#fit-multiple-curves",
    "title": "\n4  Stage-Discharge Rating Curves\n",
    "section": "\n4.4 Fit Multiple Curves",
    "text": "4.4 Fit Multiple Curves\n\n\n\n\n\n\nNote\n\n\n\nThis section is slightly more advanced and requires some understanding of list structures in R and nested dataframes.\n\n\nThe purrr package facilitates running a function on a list of nested data. The idea here is to subset the dataframe by year, create a list with each item in the list being a subset of the dataframe, then running nls_multstart() on each item in that list. Sounds like a loop huh? We achieve this using for() or lapply() functions in base R. The nice thing about doing it with purrr is that we can keep everything together in a single dataframe. The first step is to create a nested dataframe:\n\nnested_data <- example_data$dsc_fieldData |> \n  filter(!is.na(finalDischarge)) |>\n  mutate(year = format(startDate, \"%Y\")) |> \n  filter(year >= 2018) |> \n  mutate(streamStage = streamStage * 3.28084,\n         finalDischarge = finalDischarge * 0.0353147) |> \n  ## group data by year\n  group_by(year) |> \n  ## nest the data by year\n  nest()\n\nnested_data\n\n# A tibble: 4 × 2\n# Groups:   year [4]\n  year  data              \n  <chr> <list>            \n1 2018  <tibble [20 × 36]>\n2 2019  <tibble [24 × 36]>\n3 2020  <tibble [15 × 36]>\n4 2021  <tibble [21 × 36]>\n\n\nNow we use the map() function in purrr to iterate the nls.multstart() function on each nested dataframe:\n\nnested_data <- nested_data |> \n  mutate(model_output = map(.x = data,\n                            ~nls_multstart(formula = f_Q,\n                                           data = .x,\n                                           iter = 1000,\n                                           start_lower = start_lower,\n                                           start_upper = start_upper,\n                                           supp_errors = 'Y',\n                                           lower = c(K = -10, Z = -10, H_0 = 0),\n                                           control = minpack.lm::nls.lm.control(maxiter = 1000L))))\nnested_data\n\n# A tibble: 4 × 3\n# Groups:   year [4]\n  year  data               model_output\n  <chr> <list>             <list>      \n1 2018  <tibble [20 × 36]> <nls>       \n2 2019  <tibble [24 × 36]> <nls>       \n3 2020  <tibble [15 × 36]> <nls>       \n4 2021  <tibble [21 × 36]> <nls>       \n\n\nWe created a new column called model_output that is a list of the output from the nls_multstart() function that was run on each item in the data column. Grab the residuals and fits from each model:\n\nShow the codenested_data <- nested_data |> \n  mutate(residuals = map(model_output,\n                         ~as_tibble(nlsResiduals(.x)$resi2))) |> \n  unnest(c(data,residuals))\n\np1 <- ggplot(nested_data) +\n  geom_point(aes(`Fitted values`, `Standardized residuals`)) +\n  facet_wrap(~year) +\n  labs(x = \"Fitted\",\n       y = \"Standardized Residuals\",\n       subtitle = \"Sample Quantile against theoretical quantile\") +\n  theme_TWRI_print()\n\np2 <- ggplot(nested_data) +\n  geom_density(aes(`Standardized residuals`)) +\n  facet_wrap(~year) +\n  labs(x = \"Standardized Residuals\",\n       y = \"Count\",\n       subtitle = \"Distribution of standardized residuals\") +\n  theme_TWRI_print()\n\np3 <- ggplot(nested_data) +\n  geom_point(aes(`Fitted values`, finalDischarge), color = \"steelblue\", alpha = 0.4) +\n  facet_wrap(~year) +\n  labs(x = \"Model Fits\",\n       y = \"Measured Discharge [cfs]\",\n       subtitle = \"Measured against fitted\") +\n  theme_TWRI_print()\n\np4 <- ggplot(nested_data) +\n  stat_qq(aes(sample = `Standardized residuals`), color = \"steelblue\", alpha = 0.4) +\n  stat_qq_line(aes(sample = `Standardized residuals`)) +\n  facet_wrap(~year) +\n  labs(x = \"Theoretical\",\n       y = \"Standardized Residuals\",\n       subtitle = \"Sample Quantile against theoretical quantile\") +\n  theme_TWRI_print()\n\np1 / p2 / p3 / p4\n\n\n\n\nThe results are a mixed bag. For the most part, the residuals are tighter to mean zero and the tails are not as heavy as the first example. We will assume the data is good enough to continue the example. In practice, I might explore the use of seasonal rating curves or piece-wise functions.\nNext lets make a nice plot showing the rating curve with the observed data.\n\nfits <- nested_data |> \n  nest(data = -c(year, model_output)) |> \n  # create a new dataframe by group\n  # this includes the full range of the predictor variable\n  # so we can draw a nice smooth line using predictions\n  mutate(newdata = map(data,\n                       ~{\n                         tibble(streamStage = seq(min(nested_data$streamStage),\n                                                  max(nested_data$streamStage),\n                                                  length.out = 100))\n                       })) |> \n  mutate(fits = map2(newdata, model_output,\n                     ~{predict(.y, .x)})) |> \n  unnest(c(newdata, fits))\n\nggplot() +\n  geom_point(data = nested_data,\n             aes(streamStage, finalDischarge, color = year)) +\n  geom_line(data = fits,\n            aes(streamStage, fits, color = year)) +\n  facet_wrap(~year) +\n  gghighlight(max_highlight = 8,\n              use_direct_label = FALSE,\n              calculate_per_facet = FALSE)   +\n  labs(x = \"Stage [ft]\", y = \"Discharge [cfs]\") +\n  theme_TWRI_print()\n\n\n\n\n\n\n\n\nNEON (National Ecological Observatory Network). n.d. Discharge Field Collection (Dp1.20048.001), RELEASE-2022. https://doi.org/10.48443/eaak-tt31."
  },
  {
    "objectID": "drainage-area-ratio.html#basic-approach",
    "href": "drainage-area-ratio.html#basic-approach",
    "title": "\n5  Drainage Area Ratio\n",
    "section": "\n5.1 Basic Approach",
    "text": "5.1 Basic Approach\nThe basic approach where the exponent \\(\\phi\\) equals one is shown below. We will use known data from two USGS gages to evaluate performance.\n\n## load packages\nlibrary(dataRetrieval)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(twriTemplates)\n\nFor this example, I am using two USGS sites. In application you would only use a single site.\n\n## Get drainage areas for two watersheds with dataRetrieval\nsiteInfo <- readNWISsite(c('08110100', '08109800'))\nsiteInfo$contrib_drain_area_va\n\n[1] 244 195\n\n\nDownload the mean daily streamflow from 08110100 (Davidson Creek):\n\nDavidson <- readNWISdv('08110100', \"00060\", \"2015-01-01\", \"2020-12-31\")\nDavidson <- renameNWISColumns(Davidson)\nhead(Davidson)\n\n  agency_cd  site_no       Date   Flow Flow_cd\n1      USGS 08110100 2015-01-01   4.82       A\n2      USGS 08110100 2015-01-02  17.20       A\n3      USGS 08110100 2015-01-03 258.00       A\n4      USGS 08110100 2015-01-04 288.00       A\n5      USGS 08110100 2015-01-05  91.80       A\n6      USGS 08110100 2015-01-06  40.90       A\n\n\nDownload mean daily streamflows at 08109800 (Yegua Creek):\n\nYegua <- readNWISdv('08109800', \"00060\", \"2015-01-01\", \"2020-12-31\")\nYegua <- renameNWISColumns(Yegua)\n\nNow create a dataframe to compare measured streamflows at Yegua Creek with DAR estimated flows using Davidson as the source creek:\n\n# calculate the area ratio\nDAR <- siteInfo$contrib_drain_area_va[2] / siteInfo$contrib_drain_area_va[1]\n\n# create columns for the area ratio and DAR*Davidson Flow\nDavidson <- Davidson |> \n  mutate(DAR = DAR,\n         est_Yegua_Q = DAR * Flow)\n\n# join the data by date\nestimates <- Yegua |> \n  left_join(Davidson |>  select(Date, Davidson_Flow = Flow, DAR, est_Yegua_Q),\n            by = c(\"Date\" = \"Date\"))\n\nggplot(estimates) +\n  geom_point(aes(Flow, est_Yegua_Q), \n             color = \"steelblue\",\n             alpha = 0.5) +\n  geom_abline(slope = 1) +\n  coord_equal() +\n  scale_x_continuous(trans = \"pseudo_log\",\n                     breaks = c(1,10,100,1000,10000)) +\n  scale_y_continuous(trans = \"pseudo_log\",\n                     breaks = c(1,10,100,1000,10000)) +\n  theme_TWRI_print() +\n  labs(x = \"Measured Flow [cfs]\",\n       y = \"DAR Estimate [cfs]\")"
  },
  {
    "objectID": "drainage-area-ratio.html#parameterized-exponent",
    "href": "drainage-area-ratio.html#parameterized-exponent",
    "title": "\n5  Drainage Area Ratio\n",
    "section": "\n5.2 Parameterized Exponent",
    "text": "5.2 Parameterized Exponent\n\n\n\n\n\n\nIncomplete\n\n\n\nStill needs to be written."
  },
  {
    "objectID": "drainage-area-ratio.html#other-data-considerations",
    "href": "drainage-area-ratio.html#other-data-considerations",
    "title": "\n5  Drainage Area Ratio\n",
    "section": "\n5.3 Other Data Considerations",
    "text": "5.3 Other Data Considerations\n\n\n\n\n\n\nIncomplete\n\n\n\nAdd discussion about naturalized flows."
  },
  {
    "objectID": "water-quality.html#sources-of-data",
    "href": "water-quality.html#sources-of-data",
    "title": "\n6  Water Quality Data\n",
    "section": "\n6.1 Sources of Data",
    "text": "6.1 Sources of Data\n\n6.1.1 Sampling Data\nThe official data source for state water quality data used in surface water quality assessments in Texas is the Surface Water Quality Monitoring Information System (SWQMIS). My suggestion for obtaining station or water body specific data is to ask our employees that have authorized access to SWQMIS to make a data pull for you. There are two other options. First, TCEQ has a spatial explorer that you can point and click to download data by stream segment https://www80.tceq.texas.gov/SwqmisPublic/index.htm. This will download a pipe (|) delimited text file with all the water quality monitoring data for the segment. Second, the CRP Data Tool provides a way to query the SWQMIS by date, parameter code, parameter group, basin, segment, and station. Data is also provided as a pipe delimited text file with the queried data. Currently, there are no capabilities to make queries from within R.\nThe Water Quality Portal (WQP) is the EPA and USGS national database (formally STORET) for storing and providing discrete water quality monitoring data collected by states, tribes, federal and other partners. The WQP is handy for large or automated data downloads for various projects. One drawback is that the WQP and SWQMIS do not have common variables across both databases. For example, data from the WQP does not include Project Type codes that indicate the purpose for the monitoring event (flow biased storm samples or unbiased ambient samples) that might impact data analysis. WQP and SWQMIS do not use the same parameter codes and sometimes important data such as analysis methods or units get omitted in the data uploads to WQP. In general, for our state funded projects, use the SWQMIS database.\n\n6.1.2 Standards\nThe most up to date EPA approved standards are available at EPA or at TCEQ. The applicable standard for a waterbody will vary based on assessed use and sometimes season. Do not assume the standard based on a nearby or upstream/downstream water body, double check the standards.\n\n6.1.3 Assessment Summaries\nTCEQ doesn’t publicly publish the data that goes into each water body assessment. However, summaries of assessed data by assessment unit are available. The full integrated report for the current cycle is available at https://www.tceq.texas.gov/waterquality/assessment. Within each integrated report is a link to the water body assessments by basin: https://www.tceq.texas.gov/waterquality/assessment/22twqi/22basinlist. These pdfs provide information about the use, criteria, data range, number of data assessed, number of excrescences, mean (or geomean) of data assessed and additional listing information."
  },
  {
    "objectID": "water-quality.html#summarizing-data",
    "href": "water-quality.html#summarizing-data",
    "title": "\n6  Water Quality Data\n",
    "section": "\n6.2 Summarizing Data",
    "text": "6.2 Summarizing Data"
  },
  {
    "objectID": "water-quality.html#data",
    "href": "water-quality.html#data",
    "title": "\n6  Water Quality Data\n",
    "section": "\n6.3 Data",
    "text": "6.3 Data\n\n\nGet the data: These examples use the swqmispublicdata.txt data in the example data\nSWQMIS column names have spaces and characters in them. This make it difficult to refer to those variables because object names cannot have spaces in R. The janitor package has a handy function to automatically format column/variable names in a dataframe and we will use it here.\n\n# install.packages(\"janitor\")\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(twriTemplates)\n\ndf <- read_delim(\"data/swqmispublicdata.txt\",\n                 delim = \"|\",\n                 # see awkward column names that have to be wrapped\n                 # in between \"`\" (escape) marks.\n                 col_types = cols_only(\n                   `Segment ID` = col_character(),\n                   `Station ID` = col_factor(),\n                   `Station Description` = col_character(),\n                   `End Date` = col_date(format = \"%m/%d/%Y\"),\n                   `Collecting Entity` = col_character(),\n                   `Monitoring Type` = col_character(),\n                   `Composite Category` = col_character(),\n                   `Parameter Name` = col_character(),\n                   `Parameter Code` = col_character(),\n                   `Value` = col_number(),\n                   `RFA/Tag ID` = col_character()\n                 )) |> \n  clean_names()\n  \ndf\n\n# A tibble: 5,648 × 11\n   segment_id station_id station_description         end_date   collecting_enti…\n   <chr>      <fct>      <chr>                       <date>     <chr>           \n 1 1502       12517      TRES PALACIOS CREEK AT FM … 1987-09-22 TCEQ REGIONAL O…\n 2 1502       12517      TRES PALACIOS CREEK AT FM … 1987-09-22 TCEQ REGIONAL O…\n 3 1502       12517      TRES PALACIOS CREEK AT FM … 1987-09-22 TCEQ REGIONAL O…\n 4 1502       12517      TRES PALACIOS CREEK AT FM … 1987-09-22 TCEQ REGIONAL O…\n 5 1502       12517      TRES PALACIOS CREEK AT FM … 1987-09-22 TCEQ REGIONAL O…\n 6 1502       12517      TRES PALACIOS CREEK AT FM … 1987-09-22 TCEQ REGIONAL O…\n 7 1502       12517      TRES PALACIOS CREEK AT FM … 1987-09-22 TCEQ REGIONAL O…\n 8 1502       12517      TRES PALACIOS CREEK AT FM … 1987-09-22 TCEQ REGIONAL O…\n 9 1502       12517      TRES PALACIOS CREEK AT FM … 1987-09-22 TCEQ REGIONAL O…\n10 1502       12517      TRES PALACIOS CREEK AT FM … 1987-09-22 TCEQ REGIONAL O…\n# … with 5,638 more rows, and 6 more variables: monitoring_type <chr>,\n#   composite_category <chr>, parameter_name <chr>, parameter_code <chr>,\n#   value <dbl>, rfa_tag_id <chr>\n\n\n\n6.3.1 Summary Stats\nThe most common summary statistics are mean or geometric means. How we report this info depends on the project. For example, some projects might group all the stations within a single assessment unit together and report a mean. In other projects, station-specific means might be relevant. It is usually useful to explore both. An example of calculating station-specific geometric means is below:\n\n#install.packages(DescTools)\nlibrary(DescTools)\ndf |> \n  # parameter code for E. coli\n  filter(parameter_code == \"31699\") |> \n  group_by(station_id) |> \n  summarise(geomean = Gmean(value),\n            min = min(value),\n            max = max(value),\n            n = n(),\n            first_date = min(end_date),\n            last_date = max(end_date))\n\n# A tibble: 2 × 7\n  station_id geomean   min   max     n first_date last_date \n  <fct>        <dbl> <dbl> <dbl> <int> <date>     <date>    \n1 12517         118.   6.3 24192    88 2000-12-20 2022-05-17\n2 15325         210.   1    2400    35 2019-06-25 2022-05-17\n\n\n\n6.3.2 Exceedance Probabilities\nAnother relevant measure is the number of exceedances of the criterion. This is relevant because we can use a binomial test to evaluate the probability that a water body is above an acceptable exceedance rate (typically 0.1 or 0.2 depending on the parameter). Smith et al. (2001) is highly recommended reading about why and how the binomal test is applied to assess water quality conditions.\nThe following example evaluates the exceedance rate for total phosphorus. For total phosphorus, the screening level in Texas is currently 0.69 mg/L which is evaluated using the binomal test and a 20% exceedance rate.\n(https://www.tceq.texas.gov/downloads/water-quality/assessment/integrated-report-2022/2022-guidance.pdf)\n\nexceedance_df <- df |> \n  ## filter to total phos\n  filter(parameter_code == \"00665\") |>\n  filter(end_date >= as.Date(\"2013-12-01\") &\n           end_date <= as.Date(\"2020-11-30\")) |> \n  filter(monitoring_type == \"Routine - Monitoring not intentionally targeted toward any environmental condition or event\") |> \n  ## create a new variable 1 = exceedance, 0 = non-exceedance\n  mutate(\n    exceedance = case_when(\n      value < 0.69 ~ 0,\n      value >= 0.69 ~ 1\n    )) |> \n  summarize(total = n(),\n            n_exceedance = sum(exceedance),\n            probability = n_exceedance/total)\n\nexceedance_df\n\n# A tibble: 1 × 3\n  total n_exceedance probability\n  <int>        <dbl>       <dbl>\n1    18            2       0.111\n\n\nThis information nicely matches the assessment summary information provided for AU 1502_01 in the 2022 Texas Integrated Report.\nIf we want to run the binomial test on this data:\n\nbinom.test(x = exceedance_df$n_exceedance,\n           n = exceedance_df$total,\n           p = 0.2, alternative = \"g\")\n\n\n    Exact binomial test\n\ndata:  exceedance_df$n_exceedance and exceedance_df$total\nnumber of successes = 2, number of trials = 18, p-value = 0.9009\nalternative hypothesis: true probability of success is greater than 0.2\n95 percent confidence interval:\n 0.02011068 1.00000000\nsample estimates:\nprobability of success \n             0.1111111 \n\n\nThe null hypothesis is that the probability of TP exceeding 0.69 is less than or equal to 0.2, which the binomial test failed to reject (p>0.05). Thus, the assessment unit meets the water quality screening level."
  },
  {
    "objectID": "water-quality.html#figures",
    "href": "water-quality.html#figures",
    "title": "\n6  Water Quality Data\n",
    "section": "\n6.4 Figures",
    "text": "6.4 Figures\nTypically, we develop at least the following water quality figures:\n\nscatterplots of measured values over time by waterbody and station\nboxplots or histograms displaying the distribution of measured values (upstream to downstream is particularly useful)\ndepending on the amount of data, a “rolling” statistic like exceedance probability, mean or geometric mean\n\n\n# install.packages(\"ggtext\")\nlibrary(ggtext)\n\necoli <- df |> \n  filter(parameter_code == \"31699\")\n\n## the E. coli critera is 126 MPN/100mL\ncritera <- 126\n\nggplot(ecoli) +\n  ## add a dotted line to show the water quality criteria\n  geom_hline(yintercept = critera, linetype = 2) +\n  ## measured points colored by the station\n  geom_point(aes(end_date, value, color = station_id)) +\n  ## this will add a label to the dotted line\n  annotate(\"label\", \n           x = max(ecoli$end_date)+60, \n           y = critera,\n           label = \"126 MPN/100 mL\",\n           hjust = 0,\n           size = 4,\n           family = \"OpenSansCondensed_TWRI\") +\n  ## need to make some space at the end of the plot for\n  ## the label (adds 5% and 20% space to start and end)\n  scale_x_date(expand = expansion(mult = c(0.05, 0.20))) +\n  ## use log transformed spacing on the y-axis\n  scale_y_log10() +\n  scale_color_discrete_twri(name = \"Stations\") +\n  labs(x = \"Date\", y = \"*E. coli* [MPN/100 mL]\") +\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown())\n\n\n\n\nBoxplots are used to display distribution of values. The twriTemplates package has a function to include a boxplot legend that explains the components of a boxplot.\n\n# install.packages(\"patchwork\")\nlibrary(patchwork)\n\nbplot <- ggplot(ecoli) +\n  ## add a dotted line to show the water quality criteria\n  geom_hline(yintercept = critera, linetype = 2) +\n  ## measured points colored by the station\n  geom_boxplot(aes(station_id, value, fill = station_id)) +\n  scale_y_log10() +\n  scale_fill_discrete_twri(name = \"Stations\") +\n  labs(x = \"Station\", y = \"*E. coli* [MPN/100 mL]\") +\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown())\n\nlegend <- ggplot_box_legend(point_label = \" values outside the\\ninterquartile range\")\n\n# the ggplot_box_legend function is in the twriTemplates package\nbplot + legend + plot_layout(design = \"112\")\n\n\n\n\n\n6.4.1 Rolling statistics\nRolling statistics are a useful way of displaying how a statistical measure (mean, geomean, etc.) changes over time. These are typically done with regularly measured data with equal intervals and is easily calculated using dplyr window functions. However, water quality data is unequal and we are generally interested in the statistic over a specific time window regardless of the number of samples in that window (so we can’t just look back \\(n\\) rows and calculate the statistic). The runner packages solves this by letting us set the window by amount of time.1 runner() utilizes the date column and looks back to all the data within the time frame specified in the k argument, then uses that data to calculate the function in argument f.1 Some of our older scripts used the tbrf package to do this. runner is now reccomended because it is much faster and more flexible.\n\n# install.packages(\"runner\")\nlibrary(runner)\n\n# first calculate the rolling statistic\n\nrollingmean <- df %>%\n    # parameter code for E. coli\n  filter(parameter_code == \"31699\") %>% \n  group_by(station_id) %>%\n  arrange(end_date) %>%\n  mutate(Gmean = runner(x = value,\n                        k = \"7 years\",\n                        idx = end_date,\n                        f = function(x) DescTools::Gmean(x)))\n\n\nggplot(rollingmean) +\n  geom_hline(yintercept = critera, linetype = 2) +\n  geom_point(aes(end_date, value, color = station_id)) +\n  geom_step(aes(end_date, Gmean, color = station_id)) +\n  scale_y_log10() +\n  scale_color_discrete_twri(name = \"Stations\") +\n  labs(x = \"Date\", y = \"*E. coli* [MPN/100 mL]\") +\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown())\n\n\n\n\nWe can also add confidence intervals to this plot. This is a good way to communicate how certain we are that the geometric mean is actually above or below the water quality criterion.\n\nrollingmean <- rollingmean %>%\n  mutate(Gmean_lwr = runner(x = value,\n                        k = \"7 years\",\n                        idx = end_date,\n                        f = function(x) {\n                          DescTools::Gmean(x, \n                                           conf.level = 0.95)[2]\n                          }),\n         Gmean_upr = runner(x = value,\n                        k = \"7 years\",\n                        idx = end_date,\n                        f = function(x) {\n                          DescTools::Gmean(x, \n                                           conf.level = 0.95)[3]\n                          }))\n\n\nggplot(rollingmean) +\n  geom_hline(aes(yintercept = critera, linetype = \"Criteria\"), alpha = 0.5) +\n  geom_point(aes(end_date, value, color = \"Measured *E. coli* value\")) +\n  geom_step(aes(end_date, Gmean, linetype = \"7-yr rolling geomean\")) +\n  geom_step(aes(end_date, Gmean_upr, linetype = \"95% confidence interval\")) +\n  geom_step(aes(end_date, Gmean_lwr, linetype = \"95% confidence interval\")) +\n  scale_y_log10(limits = c(1,10000)) +\n  facet_wrap(~station_id, scales = \"free_x\") +\n  scale_color_manual(values = \"steelblue\") +\n  labs(x = \"Date\", y = \"*E. coli* [MPN/100 mL]\") +\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown(),\n        # turn off legend title\n        legend.title = element_blank(),\n        legend.text = element_markdown(),\n        # turn off the horizontal gridlines\n        # because they look similar to the \n        # 95% confidence intervals\n        panel.grid = element_blank())"
  },
  {
    "objectID": "water-quality.html#notes-on-figures",
    "href": "water-quality.html#notes-on-figures",
    "title": "\n6  Water Quality Data\n",
    "section": "\n6.5 Notes on figures",
    "text": "6.5 Notes on figures\n\n6.5.1 Log Transformations\nBacteria data is typically log distributed. Analysis of bacteria data almost always involve a log transformation to meet distributional assumptions in many statistical tests or regression models. Although in some fields, it is common to display these results in the log-transformed values, our standard practice is to back-transform the data and display the data with a log-transformed axis or scale.\nA brief example is shown below:\n\nex_data <- tibble(dates = seq.Date(as.Date(\"2005-01-01\"), as.Date(\"2020-12-31\"), length.out = 100),\n             x = rlnorm(100, log(126), log(5)))\n\nggplot(ex_data) +\n    geom_point(aes(dates, log(x)), color = \"#0054a4\") +\n  labs(x = \"Date\", \n       y =\"log *E. coli* [MPN/100 mL]\",\n       title = \"Log-Transformed Values\") +\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown())-> p1\n\nggplot(ex_data) +\n  geom_point(aes(dates, x), color = \"#0054a4\") +\n  scale_y_log10() +\n  labs(x = \"Date\", \n       y = \"*E. coli* [MPN/100mL]\",\n       title = \"Log-10 Y-Axis Scale\") +\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown()) -> p2\n\np1 + p2\n\n\n\n\nThe locations of the points on both plots are the same, but the scale on the left shows the log-transformed values. Conversely, the scale on the left shows the actual values but with scale log-transformed. For general audience it is easier to discern the magnitude difference between \\(10\\) and \\(10,0000\\) E. coli than between \\(2.3026\\) and \\(9.21034\\) \\(log(E. coli)\\). If your audience are microbiologists or some field where reporting log-values are standard and the audience members intuit differences in log vaalues, don’t follow this advice.\n\n6.5.2 Error Bars\nError bars are conventionally used to display the uncertainty in estimates, not the variability or distribution of measured data. Box-plots and histograms are the appropriate graph to show measurement variability. Where error bars are used, indicate if the bars represent the standard error of the mean or confidence intervals. Do not use error bars for standard deviations or maximum/minimum measurements.\n\ndf <- readr::read_csv(\"data/easterwood.csv\",\n                col_types = \"cDcnn\")\n\ndf %>%\n  mutate(month = lubridate::month(date, label = TRUE),\n         year = lubridate::year(date)) %>%\n  ggplot() +\n  geom_boxplot(aes(month,dailymaximumdrybulbtemperature)) +\n  theme_TWRI_print() +\n  labs(x = \"Month\",\n       y = \"Maximum daily temperature [°F]\",\n       title = \"Boxplots Show Distribution\",\n       subtitle = \"Distribution of Daily High Temperature\") -> p1\ndf %>%\n  mutate(month = lubridate::month(date, label = TRUE),\n         year = lubridate::year(date)) %>%\n  group_by(month, year) %>%\n  summarise(mean_monthly =  mean(dailymaximumdrybulbtemperature),\n            .groups = \"drop_last\") %>%\n  summarise(mean_temp = mean(mean_monthly),\n            mean_se = 2*DescTools::MeanSE(mean_monthly)) %>%\n  ggplot() +\n  geom_pointrange(aes(x = month, y = mean_temp, ymin = mean_temp-mean_se, ymax = mean_temp+mean_se)) +\n  theme_TWRI_print() +\n  labs(x = \"Month\",\n       y = \"Maximum daily temperature [°F]\",\n       subtitle = \"Estimated Mean Monthly High +/- 95% CI\",\n       title = \"Error Bars Show Uncertaintity\") -> p2\ndf %>%\n  mutate(month = lubridate::month(date, label = TRUE),\n         year = lubridate::year(date)) %>%\n  group_by(month) %>%\n  summarise(mean_temp = mean(dailymaximumdrybulbtemperature),\n            bad = 2*sd(dailymaximumdrybulbtemperature)) %>%\n  ggplot() +\n  geom_pointrange(aes(x = month, y = mean_temp, ymin = mean_temp-bad, ymax = mean_temp+bad)) +\n  theme_TWRI_print() +\n  labs(x = \"Month\",\n       y = \"Maximum daily temperature [°F]\",\n       subtitle = \"Estimated monthly mean high temperatures +/- 2 sd.\",\n       title = \"Don't Do This!\") -> p3\n(p1 + p2) / (p3 + plot_spacer())\n\n\n\n\n\n\n\n\nSmith, E.P., Ye, K., Hughes, C., and Shabman, L. 2001. Statistical Assessment of Violations of Water Quality Standards under Section 303(d) of the Clean Water Act. Environmental Science & Technology 35 (3): 606–12. https://doi.org/10.1021/es001159e."
  },
  {
    "objectID": "load-duration.html",
    "href": "load-duration.html",
    "title": "7  Load Duration Curves",
    "section": "",
    "text": "Incomplete\n\n\n\nThis has not been written yet."
  },
  {
    "objectID": "maps.html",
    "href": "maps.html",
    "title": "8  Spatial Data",
    "section": "",
    "text": "Incomplete\n\n\n\nThis has not been written yet."
  },
  {
    "objectID": "watershed-delineation.html",
    "href": "watershed-delineation.html",
    "title": "9  Delineate Watersheds",
    "section": "",
    "text": "Incomplete\n\n\n\nThis has not been written yet."
  },
  {
    "objectID": "raster-summary.html",
    "href": "raster-summary.html",
    "title": "10  Extract Raster Summaries",
    "section": "",
    "text": "Incomplete\n\n\n\nThis has not been written yet."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "NEON (National Ecological Observatory Network). n.d. Discharge Field\nCollection (Dp1.20048.001), RELEASE-2022. https://doi.org/10.48443/eaak-tt31.\n\n\nNOAA National Estuarine Research Reserve System (NERRS). 2022.\nSystem-Wide Monitoring Program. NOAA NERRS Centralized Data Management\nOffice. http://cdmo.baruch.sc.edu/.\n\n\nSmith, E.P., Ye, K., Hughes, C., and Shabman, L. 2001. Statistical\nAssessment of Violations of Water Quality Standards under Section 303(d)\nof the Clean Water Act. Environmental Science & Technology 35 (3):\n606–12. https://doi.org/10.1021/es001159e."
  }
]