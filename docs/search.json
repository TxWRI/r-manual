[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "twri r-manual",
    "section": "",
    "text": "Preface\nThis online resource provides guidance for reproducible data analysis using R and RStudio. The intent is not to provide copy and pastable code but to provide genaral guidance and examples that are adaptable to your own project.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#requirements",
    "href": "index.html#requirements",
    "title": "twri r-manual",
    "section": "Requirements",
    "text": "Requirements\n\nR version 4.2 or higher. This can be installed from https://cran.r-project.org/ or using the Ivanti Installation Manager if you are on an AgriLife computer.\nRStudio Desktop is the integrated development environment used to write and execute R code. Available for free from RStudio or Posit. Admin privileges are required to install, so install from Ivanti on AgriLife computers.\nThe manual assumes some basic understanding of using R and RStudio, reach out to colleagues if you are having difficulty.\nMost of the functions and code make heavy use of the tidyverse ecosystem of packages and functions. In particular tibble, dplyr, ggplot2. You are encouraged to become familiar with these particular packages. If you are just starting out with R, the R for Data Science book (free!) is a recommended reading.\nProject oriented workflows are an absolute requirement. Create projects in RStudio that align with each project you work on. Store your source and raw data within the project. Export your processed data and figures within the project. The complexity of the project will vary according to the project needs and the analysts comfort in various R packages.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html#project-oriented-workflow",
    "href": "intro.html#project-oriented-workflow",
    "title": "1  Fundamentals",
    "section": "\n1.1 Project-oriented workflow",
    "text": "1.1 Project-oriented workflow\n\n\n\n\n\n\nNote\n\n\n\nThis subsection is largely based on Jenny Bryan’s What They Forgot (WTF) to Teach You About R.\n\n\n1 - Organize your work into a project. This means within the file system store all your data, code, figures, notes, and related material within the same folder and subfolders.\n2 - RStudio Projects enforces this approach. When you create a new project in RStudio, it creates a folder with some metadata and user options for that specific project (stored in the .Rproj file inside the folder it created for the project).\n3 - RStudio Projects establish a working directory and use relative file paths by default. Usually this is what you want so when you share a project or move it from one computer to the next, it. just. works. This is also why it is critical to store your data and scripts within the project.\nA typical project might have a file and folder structure like this:\n\n\n\n Project Folder\n|\n|-- Data\n|   |\n|   |-- Raw Data\n|   |-- More Raw Data\n|\n|-- Scripts\n|   |\n|   |-- Analysis.R file\n|   |-- Figures.R file\n|\n|-- Figures\n|   |\n|   |-- Plot\n|   |-- Another figure\n|\n|-- Exported Data\n|   |\n|   |-- Results\n|\n|--- Reports\n|   |\n|   |-- Report\n|\n|- Readme file (usually .Rmd, .md, or .txt file)\n\n\n\n\n\n1.1.1 Your RStudio Project\nStart a new project! Open RStudio, in the upper left click “File” -&gt; “New Project.” We generally want to start a project in a New Directory, so click that. One the next window click New Project. Now you can choose the subdirectory name of your project (folder name) followed by where you want that subdirectory to be stored. Click “Create Project” and RStudio create the subdirectory and puts a .Rproj file with specific project info in there for you.\n\n\nExample RStudio workspace\n\nThe RStudio workspace includes 3 major components. In the upper left, the script area shows the content of open R scripts (or any text based file that you open will show up here). You can edit, save, and run lines of code from this window.\nAt the bottom left, is the R console. This is where R operates. The code you wrote in the script gets loaded into the console and R does whatever is in the script. Output, messages, and warnings from your R code will probably show up here.\nAt the bottom right, are a couple of tabs. This is where graphical outputs are displayed. There are also tabs for files, packages, and help. The file tab lets you navigate, create, delete, and open files and folders. It defaults to your projects working directory. The packages tab is for exploring the packages you have installed, more on that below. Technically you can load and unload packages from here by clicking boxes next to each package. Don’t do that. The help tab is just that, it lets you search functions in each package and displays the documentation for packages and functions. Learn to use this tab, it will help you just like it says!\n\n1.1.2 Running Code\nYou should generally write your code in the script window and execute it from there. This will save you from retyping code again and again.\nIf you have your cursor on an expression in your R script, use the keyboard shortcut: Ctrl+Enter to execute that expression. The cursor will automatically move to the next statement and the code will run in the console. If you want to execute the entire script at once, use the keyboard shortcut: Ctrl+Shift+S.\n\n1.1.3 Basic coding\nBoxes with the grey background and blue vertical bar indicate chunks of R code. If there is an output when that code chunk is run by R, the output (text output, tables or figures) will follow directly below the chunk. For example, here is a code chunk:\n\n10*100\n\nAnd this is the output:\n\n\n[1] 1000\n\n\n\n\n\n\n\n\nNote\n\n\n\nMuch of this subsection is from R for Data Science which you are encouraged to explore.\n\n\nThis: &lt;-, is called an assignment operator in R. We use it to create objects by assigning a value to a variable name. We name objects so we can easily refer to whatever you assigned later on in your script:\n\nx &lt;- 10\ny &lt;- 100\n\nx * y\n\n[1] 1000\n\n\nYou don’t have to assign numbers:\n\nx &lt;- \"Hello\"\n\nprint(x)\n\n[1] \"Hello\"\n\n\nAssignment operators go either direction, you might find it useful to use the left to right assinment operator in some situations:\n\n\"Hello\" -&gt; x\n\nprint(x)\n\n[1] \"Hello\"\n\n\nHowever, for the most part, standard practice is to assign right to left so you can easily find the variable name receiving the value. Whatever you choose, use the same direction throughout your project.\nAs your scripts get more complicated, it is important to use descriptive object names. Object names can only contain letters, numbers, _, and ., so we recommend using “snake_case” to create object names:\nstreamflow\nstreamflow_site_a\nObject names are case sensitive, streamflow_site_a is not the same as streamflow_site_A.\nThe # symbol is ignored by R and used to include human readable comments in your script. Use comments liberally in your code.\n\n## I can write what I want\n## and R does not evaluate\n## this\na &lt;- 1\na",
    "crumbs": [
      "R Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentals</span>"
    ]
  },
  {
    "objectID": "intro.html#more-about-objects",
    "href": "intro.html#more-about-objects",
    "title": "1  Fundamentals",
    "section": "\n1.2 More About Objects",
    "text": "1.2 More About Objects\nBefore diving to much into R programming it is worth becoming familiar with some of the basic types of object in R. Function arguments might only accept specific types of objects and return specific types of objects. Being able to identify object types is fundamental for troubleshooting errors. This is not an exhaustive list but commonly objects are one of:\n\nA single variable that can be character, double, integer, raw, logical, or complex eg: x &lt;- 1.\n\nVectors are a one-dimensional set of homogeneous data types. Typically constructed using the c() function: x &lt;- c(1,2,3).\n\nLists are similar to a vector but group together R objects. Lists can include different types of objects and are constructed using the list() function: list_1 &lt;- list(c(1,2,3,4,5), c(\"A\", \"B\", \"C\"), list(TRUE, FALSE)).\n\nData Frames are two-dimensional lists or more easily thought of as tabular data similar to a spreadsheet. The data frame represents a list of variables which have the same number of rows. Columns can be of differing types as long as they have the same number of rows. The data frame and its cousin the tibble are extensively used in data analysis workflows. Data frames are constructed with the data.frame() function: df &lt;- data.frame(x = c(1,2,3), y = c(\"a\", \"b\", \"c\")).\n\nThere are a number of other object types that I won’t go into detail on. Matrices and arrays are multi-dimensional objects similar to matrices from linear algebra.\nMost R objects have something called a class attribute. You can use the class() function to inspect any object:\n\nclass(\"Hello\")\n\n[1] \"character\"\n\ndf &lt;- data.frame(x = c(1,2,3), y = c(\"a\", \"b\", \"c\"))\nclass(df)\n\n[1] \"data.frame\"\n\n\n\n1.2.1 Factors\nFactors are a way of storing categorical information in R. Factors often appear to be character objects, but R stores the information as levels and numbers which facilitates statistical modeling off categorical variables. Even if you won’t be modeling categorical variables, factors are valuable for storing the order to display information if that variable has some inherent order to it (something like drought category for example). Factors are created with the factor() function. When you use factor, R will recode the data as integers and store an additional attribute with the data called levels that stores the labels associated with an integer:\n\ndifficulty &lt;- factor(c(\"Hard\", \"Easy\", \"Easy\", \"Medium\", \"Medium\"))\nclass(difficulty)\n\n[1] \"factor\"\n\n\nR orders these alphabetically by default. These makes sense if there is no other inherent ordering, but in this case we would probably want to order it something like: Easy, Medium, Hard. The use of this makes more sense once you start plotting or summarizing data into tables:\n\n## shows the class and levels\nattributes(difficulty)\n\n$levels\n[1] \"Easy\"   \"Hard\"   \"Medium\"\n\n$class\n[1] \"factor\"\n\n## to reorder the levels:\nlevels(difficulty) &lt;- c(\"Easy\", \"Medium\", \"Hard\")\n\n## or tell R the order when you make the factor\nfactor(c(\"Hard\", \"Easy\", \"Easy\", \"Medium\", \"Medium\"), \n       levels = c(\"Easy\", \"Medium\", \"Hard\"))\n\n[1] Hard   Easy   Easy   Medium Medium\nLevels: Easy Medium Hard\n\n\n\n1.2.2 Dates\nThere are specific classes for date and time objects in R. Use the Date class to represent dates in R using the as.Date() function. The character representation of Dates always defaults to \"yyyy-mm-dd\" format:\n\nas.Date(\"2021-01-01\")\n\n[1] \"2021-01-01\"\n\n\nIf your date string is in a different format, you need to specify it in the format argument. The help documentation in the strptime() function provide details on how to specify the format, an example is shown for dates entered as \"mm/dd/yyyy\":\n\nas.Date(\"01/01/2021\",\n        format = \"%m/%d/%Y\")\n\n[1] \"2021-01-01\"\n\n\nWe can subtract two dates and get a difftime object that tells us the difference in days:\n\nn &lt;- Sys.Date() - as.Date(\"2021-01-01\")\nn\n\nTime difference of 1018 days\n\n##  or specify units\ndifftime(Sys.Date(), as.Date(\"2021-01-01\"), units = \"weeks\")\n\nTime difference of 145.4286 weeks\n\n\nIf you have Date-Time strings, the POSIXct class is your friend. The default format is shown below and strptime() provides the details on how to specify different formats. The time recorded in your data probably depends on a timezone and might shift based on daylight savings time (or might not depending on the device collecting the data). Use the time zone argument to specify the time. If you are setting up a device to collect data that will be read into R, I highly recommend setting up the device to collect in UTC offset.1\n1 See: https://en.wikipedia.org/wiki/List_of_UTC_offsets\nas.POSIXct(\"2021-06-01 12:00:00\", tz = \"UTC\")\n\n[1] \"2021-06-01 12:00:00 UTC\"\n\n## example of UTC offset format\nas.POSIXct(\"2021-06-01 06:00:00 -0600\", \n           format = \"%Y-%m-%d %H:%M:%S %z\",\n           tz = \"UTC\")\n\n[1] \"2021-06-01 12:00:00 UTC\"",
    "crumbs": [
      "R Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentals</span>"
    ]
  },
  {
    "objectID": "intro.html#functions",
    "href": "intro.html#functions",
    "title": "1  Fundamentals",
    "section": "\n1.3 Functions",
    "text": "1.3 Functions\nFunctions are essentially tools that take input arguments and output some kind of value. Functions are the basis for most everything you do in R. For example, seq() is a function to generate a regular sequence of numbers. You can get to the help documentation by entering ?seq() in the console. It takes the arguments from, to, by, length.out, along.with. Use = for argument values:\n\nseq(from = 0, to = 10, by = 2)\n\n[1]  0  2  4  6  8 10\n\n\nWriting your own functions is one of the reasons for using R. Here is a simplistic function that generates a message in the console screen depending on the condition of the first argument.\n\nprint_hello &lt;- function(x) {\n  if (x &lt; 1) message(\"Hello!\")\n  else message(\"Bye\")\n}\n\nprint_hello(x = -1)\n\nHello!\n\nprint_hello(x = 1)\n\nBye\n\n\nWhy write a function in the first place? Sometimes you might need to repeatedly run the same set of functions on different data or subsets of data. You will find yourself copy and pasting code and changing some values within. If the output is dependent on some values you forgot to change when you cut and paste, instant problems! Functions let you skip that copy and paste action, and just update the arguments. Here is an example of some code to calculate the confidence interval around the mean for a vector of numbers:\n\nmin &lt;- 0\nmax &lt;- 10\nn &lt;- 1000\nci &lt;- 0.95\nx &lt;- runif(n = n, min = min, max = max)\nse &lt;- sd(x)/sqrt(length(x))\nalpha &lt;- 1 - ci\nmean(x) + se * qnorm(c(alpha/2, 1-alpha/2))\n\n[1] 4.907045 5.260658\n\n\nIf we need to recalculate the confidence interval for different values or combinations of values of x, n, and ci we would have to cut and paste the chunk each time with the potential for data entry errors if the wrong values are entered. Instead, create a function and change the arguments as needed.\n\nci &lt;- function(min, max, n, ci) {\n  x &lt;- runif(n = n, min = min, max = max)\n  se &lt;- sd(x)/sqrt(length(x))\n  alpha &lt;- 1 - ci\n  mean(x) + se * qnorm(c(alpha/2, 1-alpha/2))\n}\n\nci(min = 0, max = 10, n = 1000, ci = 0.95)\n\n[1] 4.741090 5.092174\n\nci(min = 10, max = 100, n = 1000, ci = 0.90)\n\n[1] 53.37521 56.08682\n\nci(min = 10, max = 1000, n = 1000, ci = 0.80)\n\n[1] 508.1175 531.4035",
    "crumbs": [
      "R Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentals</span>"
    ]
  },
  {
    "objectID": "intro.html#packages",
    "href": "intro.html#packages",
    "title": "1  Fundamentals",
    "section": "\n1.4 Packages",
    "text": "1.4 Packages\nPackages might be considered the toolboxes of R. They are generally a collection of functions and classes the expand the capabilities of the base R functions. Many packages have dependencies from other packages. This mean when you install one package, you may end up installing multiple other packages automatically that are required for the package that you chose to work. Normally this works without hiccup. However, before installing packages, I suggest restarting your R session and make sure no packages are currently loaded to prevent issues.\nMost packages can and should be installed from the CRAN repository. These are a network of repositories that host the official, up-to-date and approved packages for R. This packages are pre-built, meaning you are unlikely to run into issues on installation. To install packages from CRAN, you typically do something like the following:\n\n## install one package\ninstall.packages(\"ggplot2\")\n\n## install multiple packages\ninstall.packages(\"dplyr\", \"tidyr\")\n\nSometimes you need a package or package version that is not currently available on CRAN. There are various justifiable reasons the packages might not be available on CRAN; however, one of the benefits of using CRAN packages is that they are all reviewed by a person before acceptance. This provides a safety mechanism for not only the quality of the package but potential security issues.\n\n\n\n\n\n\nNote\n\n\n\nIf you are installing a package from GitHub or other source, please review it for safety and quality before installation.\n\n\nThere are two primary way to install non-CRAN packages. The preferred method is to install pre-built packages from an alternative repository like r-universe. The readme file associated with the package will generally inform you if the package is available on a different repository and how to install it from that repository.\nAn example of this is shown below for the adc package:\n\ninstall.packages('adc', repos = c(txwri = 'https://txwri.r-universe.dev'))\n\nAn alternative option is to download and build the packages from the source, such as GitHub. For those on Windows, you will need to install the RTools toolchain. Then, we can use the remotes package to download, build and install a package from GitHub:\n\ninstall.packages(\"remotes\")\nremotes::install_github(\"mps9506/rATTAINS\")\n\nAfter you install a package, you need to load the package in order to use the functions. Confusingly, you use the library() function to accomplish this. Standard practice is to load libraries at the top of your script:\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)",
    "crumbs": [
      "R Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentals</span>"
    ]
  },
  {
    "objectID": "intro.html#other-coding-conventions",
    "href": "intro.html#other-coding-conventions",
    "title": "1  Fundamentals",
    "section": "\n1.5 Other Coding Conventions",
    "text": "1.5 Other Coding Conventions\n\n1.5.1 Pipe Function\nMany of the examples in this manual use something called the pipe function (either %&gt;% or |&gt; in newer versions of R). The pipe takes the output of a function and makes it the input of the next function without writing it to an object in the environment. The primary advantage of this is when you have many data processing steps in a row and you don’t want to write or overwrite an object at every step. In this manual we will use the |&gt; function.\nHere is a short example where we take the mean of a sample of numbers\n\n## generate a numeric vector of randomly log normally distributed\n## values whose mean log is 100\nx &lt;- rlnorm(100, meanlog = 100)\n\n## calculate the mean of the log\nmean(log(x))\n\n[1] 99.99192\n\n\nThis is a little messy. If we had multiple steps this could make the code difficult to interprt. The pipe function separates this into steps. The code below pipes x into the first argument of log(), the output of log() is piped into the first argument of mean(). This delineates each step in the data workflow.\n\n## this is the same as above\nx |&gt; \n  log() |&gt; \n  mean()\n\n[1] 99.99192\n\n\nIf it isn’t the first argument that needs the output from the previous function, we use the placeholder _. A good example is the function to fit a linear regresion, lm() which requires the regression formula as the first argument and data for the second argument. We generate vector of sample random predictor data (x) from a uniform distribution using runif(). A vector b of coefficents normally distributed around mean 10 is generated using rnorm(). The response is generated using x * b. The linear regression should return a coefficient for x that is near 10.\n\n## generate a pretend response variable\nx &lt;- runif(100, min = 10, max = 100)\nb &lt;- rnorm(n = 100, mean = 10, sd = 2)\ndata.frame(x = x,\n           y = x * b) |&gt; \n  ## fit a regression model, notice the \"_\" placeholder\n  lm(y ~ x, data = _) |&gt;\n  ## return the regression summary\n  summary()\n\n\nCall:\nlm(formula = y ~ x, data = data.frame(x = x, y = x * b))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-453.24  -36.78    7.69   67.76  368.61 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -24.045     26.825  -0.896    0.372    \nx             10.346      0.446  23.199   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 121.5 on 98 degrees of freedom\nMultiple R-squared:  0.846, Adjusted R-squared:  0.8444 \nF-statistic: 538.2 on 1 and 98 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "R Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentals</span>"
    ]
  },
  {
    "objectID": "intro.html#suggested-rstudio-settings",
    "href": "intro.html#suggested-rstudio-settings",
    "title": "1  Fundamentals",
    "section": "\n1.6 Suggested RStudio Settings",
    "text": "1.6 Suggested RStudio Settings\n\n\n\n\n\n\nIncomplete\n\n\n\nI still need to add discussion on\n\nsetting global and project options in RStudio\nusing ragg graphics device (this might be better in figure section)",
    "crumbs": [
      "R Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentals</span>"
    ]
  },
  {
    "objectID": "data_exploration.html#import-data",
    "href": "data_exploration.html#import-data",
    "title": "2  Data Exploration",
    "section": "\n2.1 Import Data",
    "text": "2.1 Import Data\nOne of the first steps in any data analysis project is importing or reading data into R. For now, we will focus on reading in data from comma separated value (CSV), tab seperated value (TSV), and similar delimited text files. For most projects, it is useful to work off a singular data snapshot. So once you download your data and have it in your project, do not manipulate the data file1.\n1 Later on we will discuss loading web data sources directly into R, but for most of our work your best bet is to store raw data files inside the project\n\n\n\n\n\nExample Data\n\n\n\nThe twriTemplates package includes the example data used here. The path_to_file() function returns the file path to example data csv and text files in the package. The output from path_to_file() can be used directly in the functions used to read or import csv and text files.\n\n\nUse the read_csv() function from the readr package to import the data and assign it to a variable2. The console will print some information telling you what type of variable it made each column of the csv and if there are any problems.\n2 Try downloading and extracting this example data to your R project data folder. Instead of using the path_to_file() function, type to path to the data folder and the filename for your first argument in read_csv(). It should look something like: read_csv(\"data/easterwood.csv\"\n## this is a shortcut function in the twriTemplates package\n## that returns the file path to example data used in this documentation\nexample_file_path &lt;- path_to_file(\"easterwood.csv\")\n\n## return a dataframe from the read_csv function with the file path\n## as the first argument\ndf &lt;- read_csv(file = example_file_path)\n\nRows: 4045 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): name\ndbl  (3): station, dailymaximumdrybulbtemperature, dailyprecipitation\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nread_csv() guesses the column types and sometimes it can guess wrong, so the safest option is to tell it what to expect using the col_types argument. This argument takes a string where each character represents one column. Here we will tell it to expect a character, Date, character, number, number which is represented as cDcnn.\n\ndf &lt;- read_csv(file = example_file_path,\n               col_types = \"cDcnn\")\ndf\n\n# A tibble: 4,045 × 5\n   station     date       name         dailymaximumdrybulbt…¹ dailyprecipitation\n   &lt;chr&gt;       &lt;date&gt;     &lt;chr&gt;                         &lt;dbl&gt;              &lt;dbl&gt;\n 1 74746003904 2010-07-31 COLLEGE STA…                     99                  0\n 2 74746003904 2010-08-01 COLLEGE STA…                    102                  0\n 3 74746003904 2010-08-02 COLLEGE STA…                    101                  0\n 4 74746003904 2010-08-03 COLLEGE STA…                    100                  0\n 5 74746003904 2010-08-04 COLLEGE STA…                     99                  0\n 6 74746003904 2010-08-05 COLLEGE STA…                     99                  0\n 7 74746003904 2010-08-06 COLLEGE STA…                     99                  0\n 8 74746003904 2010-08-07 COLLEGE STA…                     99                  0\n 9 74746003904 2010-08-08 COLLEGE STA…                     99                  0\n10 74746003904 2010-08-09 COLLEGE STA…                    100                  0\n# ℹ 4,035 more rows\n# ℹ abbreviated name: ¹​dailymaximumdrybulbtemperature\n\n\nTSV and other delimited files are read in the same way but with read_delim() or read_tsv(). If your data is in .xlsx format, the readxl pacakge is required. In readxl there is a read_xlsx() function that works similar to the read_csv() function except you can specify the sheet and range of cells to read from. The col_types argument also needs to be spelled out as a character vector such as col_types = c(\"text\", \"date\", \"text\", \"numeric\", \"numeric\").\nIf you have multiple files with the same column names you can read them into the same data frame. Here we read in multiple files and are more explicit about defining the column types:\n\n## returns two different file paths\nexample_file_paths &lt;- path_to_file(c(\"marabwq2021.csv\", \"marcewq2021.csv\"))\n\n## returns one dataframe from two files\ndf &lt;- read_csv(file = example_file_paths, \n               col_types = cols_only(\n                 StationCode = col_factor(),\n                 DateTimeStamp = col_datetime(format = \"%m/%e/%Y %H:%M\"),\n                 Temp = col_number(),\n                 F_Temp = col_character(),\n                 SpCond = col_number(),\n                 F_SpCond = col_character(),\n                 Sal = col_number(),\n                 F_Sal = col_character(),\n                 DO_mgl = col_number(),\n                 F_DO_mgl = col_character()\n               ))\ndf\n\n# A tibble: 70,080 × 10\n   StationCode DateTimeStamp        Temp F_Temp SpCond F_SpCond   Sal F_Sal\n   &lt;fct&gt;       &lt;dttm&gt;              &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;\n 1 marabwq     2021-01-01 00:00:00  16.1 &lt;0&gt;      47.9 &lt;0&gt;       31.2 &lt;0&gt;  \n 2 marabwq     2021-01-01 00:15:00  16   &lt;0&gt;      47.9 &lt;0&gt;       31.3 &lt;0&gt;  \n 3 marabwq     2021-01-01 00:30:00  16   &lt;0&gt;      47.9 &lt;0&gt;       31.3 &lt;0&gt;  \n 4 marabwq     2021-01-01 00:45:00  16   &lt;0&gt;      47.9 &lt;0&gt;       31.3 &lt;0&gt;  \n 5 marabwq     2021-01-01 01:00:00  15.9 &lt;0&gt;      47.9 &lt;0&gt;       31.2 &lt;0&gt;  \n 6 marabwq     2021-01-01 01:15:00  15.9 &lt;0&gt;      47.8 &lt;0&gt;       31.2 &lt;0&gt;  \n 7 marabwq     2021-01-01 01:30:00  15.9 &lt;0&gt;      47.8 &lt;0&gt;       31.2 &lt;0&gt;  \n 8 marabwq     2021-01-01 01:45:00  15.8 &lt;0&gt;      47.8 &lt;0&gt;       31.2 &lt;0&gt;  \n 9 marabwq     2021-01-01 02:00:00  15.8 &lt;0&gt;      47.8 &lt;0&gt;       31.2 &lt;0&gt;  \n10 marabwq     2021-01-01 02:15:00  15.7 &lt;0&gt;      47.8 &lt;0&gt;       31.2 &lt;0&gt;  \n# ℹ 70,070 more rows\n# ℹ 2 more variables: DO_mgl &lt;dbl&gt;, F_DO_mgl &lt;chr&gt;\n\n\n\n\nThis dataset is from the NOAA NERRS Centralized Data Management Office and provides measures of water quality parameters in the Mission and Aransas (MAR) National Estuarine Research Reserve (NERR)(NOAA National Estuarine Research Reserve System (NERRS) 2022).\nIn this data we have a row for each date/time observation with associated columns for Station, Temperature, Specific Conductance, Salinity, and Dissolved Oxygen. The columns with the F_ prefix are qa/qc flags, &lt;0&gt; is accepted data.",
    "crumbs": [
      "R Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "data_exploration.html#sec-plotclean",
    "href": "data_exploration.html#sec-plotclean",
    "title": "2  Data Exploration",
    "section": "\n2.2 Plot and Clean Data",
    "text": "2.2 Plot and Clean Data\nggplot2 is the graphics package for producing layered plots. The underlying philosophy of ggplot2 is to iteratively build your plots layer by layer which allows for some sophisticated plots. I won’t go into full details of using ggplot2 but lots of information is available in the ggplot2 book.\nThree key things you need to learn: data, aesthetics, geoms.\n\ndata: a data frame or tibble with the data you want to plot.\naesthetics mappings (or aes): specify how variables will be visually depicted (x, y, color, shape, size, etc.).\ngeoms: are the layers that define how each layer is rendered (points, lines, bars, etc.).\n\nUsing the data we imported from above we can quickly create a basic scatter plot. Note the use of the + symbol to iteratively add layers to our plot. First we specify the data, then the geometry, and the aesthetic for that geom:\n\np1 &lt;- ggplot(data = df, aes(x = DateTimeStamp, y = Temp)) +\n  geom_point()\np1\n\nWarning: Removed 4959 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nThe above figure demonstrates the importance of plotting your data, the -100 value is clearly an issue. One thing we can do is filter the data to try and track down the issue. The dplyr package provides a number of functions to explore your data. Here, the data is filtered on the Temp column to include data less than -99:\n\ndf |&gt; \n  filter(Temp &lt; -99)\n\n# A tibble: 1 × 10\n  StationCode DateTimeStamp        Temp F_Temp       SpCond F_SpCond   Sal F_Sal\n  &lt;fct&gt;       &lt;dttm&gt;              &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;\n1 marcewq     2021-08-23 20:45:00  -100 &lt;-3&gt; [GIM] … -1211. &lt;-3&gt; [G…   -99 &lt;-3&gt;…\n# ℹ 2 more variables: DO_mgl &lt;dbl&gt;, F_DO_mgl &lt;chr&gt;\n\n\nThe F_Temp qa flag is &lt; -3 &gt; indicating QA rejected the data point. Let’s update the data to remove data that doesn’t have the &lt; 0 &gt; data flag:\n\ndf &lt;- df |&gt; \n  filter(F_Temp == \"&lt;0&gt;\")\ndf\n\n# A tibble: 58,386 × 10\n   StationCode DateTimeStamp        Temp F_Temp SpCond F_SpCond   Sal F_Sal\n   &lt;fct&gt;       &lt;dttm&gt;              &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;\n 1 marabwq     2021-01-01 00:00:00  16.1 &lt;0&gt;      47.9 &lt;0&gt;       31.2 &lt;0&gt;  \n 2 marabwq     2021-01-01 00:15:00  16   &lt;0&gt;      47.9 &lt;0&gt;       31.3 &lt;0&gt;  \n 3 marabwq     2021-01-01 00:30:00  16   &lt;0&gt;      47.9 &lt;0&gt;       31.3 &lt;0&gt;  \n 4 marabwq     2021-01-01 00:45:00  16   &lt;0&gt;      47.9 &lt;0&gt;       31.3 &lt;0&gt;  \n 5 marabwq     2021-01-01 01:00:00  15.9 &lt;0&gt;      47.9 &lt;0&gt;       31.2 &lt;0&gt;  \n 6 marabwq     2021-01-01 01:15:00  15.9 &lt;0&gt;      47.8 &lt;0&gt;       31.2 &lt;0&gt;  \n 7 marabwq     2021-01-01 01:30:00  15.9 &lt;0&gt;      47.8 &lt;0&gt;       31.2 &lt;0&gt;  \n 8 marabwq     2021-01-01 01:45:00  15.8 &lt;0&gt;      47.8 &lt;0&gt;       31.2 &lt;0&gt;  \n 9 marabwq     2021-01-01 02:00:00  15.8 &lt;0&gt;      47.8 &lt;0&gt;       31.2 &lt;0&gt;  \n10 marabwq     2021-01-01 02:15:00  15.7 &lt;0&gt;      47.8 &lt;0&gt;       31.2 &lt;0&gt;  \n# ℹ 58,376 more rows\n# ℹ 2 more variables: DO_mgl &lt;dbl&gt;, F_DO_mgl &lt;chr&gt;\n\n\nThis removes about 29,263 observations. Try plotting again:\n\np1 &lt;- ggplot(data = df, aes(x = DateTimeStamp, y = Temp)) +\n  geom_point()\np1\n\n\n\n\n\n\n\n\n\nWe can use different geoms to explore the data:\n\np2 &lt;- ggplot(data = df, aes(x = Temp)) +\n  geom_histogram(binwidth = 1)\np2\n\n\n\n\n\n\n\n\n\n\nggplot(data = df, aes(y = Temp, x = StationCode)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nTo explore relationships between two variables, use geom_point and geom_smooth with each variable mapped to x and y. In this example, ggplot2 prints a message indicating 834 rows of data had missing or NA values that could not be plotted. geom_smooth() will plot the smooth line (using a loess or gam smooth) between two variables. If you want the linear regression drawn use the argument method = \"lm\".\n\np3 &lt;- ggplot(data = df, aes(x = Temp, y = DO_mgl)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(se = FALSE)\np3\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 834 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 834 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nThis is a good example to introduce the other important aesthetics in ggplot2. There is a clear negative relationship between Temperature and Dissolved Oxygen. Our data includes two sites, is there a difference between the two sites? We can map color to the site variable which will color each observation and each smooth a different color. Although shape and color can be used inside the aes() mapping function, you can assign them a value in the geom directly. Here we asign values for the shape and alpha properties in the point geom.\n\np3 &lt;- ggplot(data = df, aes(x = Temp, y = DO_mgl, color = StationCode)) +\n  geom_point(shape = 21, alpha = 0.05) +\n  geom_smooth(se = FALSE)\np3\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 834 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 834 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nThere was no reason to expect a difference and there isn’t. However the plot is muddy because there are so many overlying data points. We can also facet the graph by a variable. The following code also shows how you can just add another layer to your existing ggplot object:\n\np3 &lt;- p3 +\n  facet_wrap(~StationCode)\n\np3\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 834 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 834 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nAs you can see, ggplot2 allows you to rapidly iterate plots to explore data. When exploring the data, the formatting might not matter much, but if you want to export plots, we also need to take care of labels and general plot visual preferences.\n\np3 &lt;- p3 +\n  labs(x = \"Temperature [°F]\", y = \"Dissolved Oxygen [mg/L]\") +\n  scale_colour_brewer(name = \"Stations\",\n                      palette = \"Dark2\",\n                      labels = c(\"Aransas Bay\", \"Copano East\"))\np3\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 834 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 834 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n2.2.1 Themes\nggplot2 has a number of built-in themes. For most of our use cases (reports and papers), the grey background and white lines are not great choices. We recommend using the theme_bw() function at the bare minimum:\n\np3 +\n  theme_bw()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 834 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 834 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nThe twriTemplates package is available with a custom theme and some color palettes that are consistent with the Institute’s branding:\n\ninstall.packages(\"twriTemplates\", \n                 repos = c(txwri = 'https://txwri.r-universe.dev',\n                           CRAN = 'https://cloud.r-project.org'))\n\n\nlibrary(twriTemplates)\np3 +\n  theme_TWRI_print() +\n  scale_color_discrete_twri(name = \"Stations\",\n                            labels = c(\"Aransas Bay\", \"Copano East\"))\n\nWarning: Removed 834 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 834 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nThe facet labels are still not very useful. There isn’t an easy way to change this using ggplot2. Instead we can create a new variable using the mutate() function in dplyr. There is more discussion about dplyr in the next section.\n\ndf &lt;- df |&gt; \n  mutate(StationName = case_when(\n    StationCode == \"marabwq\" ~ \"Aransas Bay\",\n    StationCode == \"marcewq\" ~ \"Copano Bay East\"\n  ))\n\nggplot(data = df,\n       aes(x = Temp, y = DO_mgl, color = StationName)) +\n  geom_point(shape = 21, alpha = 0.05) +\n  geom_smooth(se = FALSE) +\n  facet_wrap(~StationName) +\n  labs(x = \"Temperature [°F]\", y = \"Dissolved Oxygen [mg/L]\") +\n  scale_color_discrete_twri(name = \"Stations\") +\n  theme_TWRI_print()\n\nWarning: Removed 834 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 834 rows containing missing values (`geom_point()`).",
    "crumbs": [
      "R Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "data_exploration.html#manipulate-and-summarize-data",
    "href": "data_exploration.html#manipulate-and-summarize-data",
    "title": "2  Data Exploration",
    "section": "\n2.3 Manipulate and Summarize Data",
    "text": "2.3 Manipulate and Summarize Data\ndplyr is a powerful package designed for working with dataframes and tibbles. Read the dplyr introduction to get a better sense of all the available functions. Essentially dplyr provides “verb” functions that correspond to how you want to manipulate the dataframe.\nCommonly used verbs include:\n\n\nfilter(): subset the rows based on value\n\nmutate(): create a new column\n\nselect(): subset columns based on names or types\n\ngroup_by(): group the data by variable values\n\nsummarise(): summarise the grouped or ungrouped data based on user provided functions\n\nThere are many other functions, but these cover many use cases for people just getting started. dplyr also provides functions for joining datasets based on common variables, changing the order of a dataframe, calculating cumulative statistics, calculating lag and leading values, and rolling or window rank functions among others. All of these can be very useful to become familiar with.\nIn this example I want to explore seasonal differences in the water quality parameters. mutate() is used to create new variables.\n\nsummary_plot &lt;- df |&gt; \n  # returns DateTimeStamp Value as the month only\n  # see ?strptime() for more info\n  mutate(Month = as_factor(format(DateTimeStamp, \"%B\")),\n         StationName = case_when(\n           StationCode == \"marabwq\" ~ \"Aransas Bay\",\n           StationCode == \"marcewq\" ~ \"Copano Bay East\")) |&gt; \n  ## months have an inherent order, right now it is alphabetical\n  ## this adds the correct order so it plots nicely\n  mutate(Month = lvls_revalue(Month, month.name)) |&gt; \n  ggplot() +\n  geom_boxplot(aes(x = Month, y = DO_mgl, fill = StationName)) +\n  scale_fill_discrete_twri(name = \"Stations\") +\n  theme_TWRI_print()\nsummary_plot\n\nWarning: Removed 834 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nNow we have a summary figure, but what about the data? We can use the summarize() or summarise() function (depending on your English preference) to calculate some summary statistics for each group.\n\nsummary_table &lt;- df |&gt; \n  mutate(Month = as_factor(format(DateTimeStamp, \"%B\")),\n         StationName = case_when(\n           StationCode == \"marabwq\" ~ \"Aransas Bay\",\n           StationCode == \"marcewq\" ~ \"Copano Bay East\")) |&gt; \n  mutate(Month = lvls_revalue(Month, month.name)) |&gt; \n  ## create groups to calculate summary stats on\n  group_by(StationName, Month) |&gt; \n  ## calculate the summary stats\n  ## returns a column for each stat with a row for each group\n  summarize(Mean = mean(DO_mgl),\n            Max = max(DO_mgl),\n            Min = min(DO_mgl),\n            ## no standard error function in base R\n            ## can calculate manually here\n            SE = sd(DO_mgl)/sqrt(length(DO_mgl)))\n\n`summarise()` has grouped output by 'StationName'. You can override using the\n`.groups` argument.\n\nsummary_table\n\n# A tibble: 24 × 6\n# Groups:   StationName [2]\n   StationName Month      Mean   Max   Min      SE\n   &lt;chr&gt;       &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Aransas Bay January    8.56  12.1   6.7 0.0149 \n 2 Aransas Bay February   9.36  13.7   6.4 0.0314 \n 3 Aransas Bay March      7.57   8.7   5.6 0.00920\n 4 Aransas Bay April      7.19   8.2   5.9 0.00952\n 5 Aransas Bay May        7.10   9     5.7 0.0143 \n 6 Aransas Bay June       5.86   7.8   2.2 0.0190 \n 7 Aransas Bay July       4.74   8.2   0.2 0.0308 \n 8 Aransas Bay August     5.08   7.1   0.2 0.0185 \n 9 Aransas Bay September  5.69   7.8   2.3 0.0162 \n10 Aransas Bay October    6.67   9.6   3.2 0.0167 \n# ℹ 14 more rows",
    "crumbs": [
      "R Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "data_exploration.html#export-data",
    "href": "data_exploration.html#export-data",
    "title": "2  Data Exploration",
    "section": "\n2.4 Export Data",
    "text": "2.4 Export Data\n\n2.4.1 Figures\nYou have created a figure and a table in R. Now you probably need to put it in a report, website, or other self contained file for your boss to look at. For figures created with ggplot2, the ggsave() option is the easiest option. First I recommend installing, the ragg package because if it available, ggsave() will use it. ragg provides higher performance and higher quality images than the graphics devices available in base R.\n\n# install ragg\ninstall.packages(\"ragg\")\nlibrary(ragg)\n\n## ggsave defaults to the last plot displayed\n## it is safer to specify which plot you want saved\nggsave(filename = \"figures/boxplot_mare.png\",\n       plot = summary_plot,\n       width = 8,\n       height = 6,\n       dpi = 300,\n       units = \"in\")\n\nIf you have a figure created from something other than a ggplot object, then we need to use the ragg graphics device to save it as a file (this also works with ggplot as demonstrated here). If you don’t have ragg installed, substitute png() for agg_png().\n\n# first function creates the device\nagg_png(filename = \"figures/boxplot_mare.png\",\n    width = 8,\n    height = 6,\n    res = 300,\n    units = \"in\")\n# copy the plot to the file\nsummary_plot\n# turns off the device\ndev.off()\n\n\n2.4.2 Tables\nreadr and writexl packages provide the functions required to export data frames to text delimited or Microsoft Excel files.\n\n## write a csv\nwrite_csv(x = summary_table,\n          file = \"export_data/monthly_summary_mare.csv\")\n\n## or an Excel file\n## uncomment the next line if writexl is not installed\n# install.packages(\"writexl\")\n\nlibrary(writexl)\n# one sheet\nwrite_xlsx(x = summary_table,\n           path = \"export_data/monthly_summary_mare.xlsx\")\n\n# or specify sheets\nwrite_xlsx(x = list(summarysheet = summary_table,\n                    rawdatasheet = df),\n           path = \"export_data/monthly_summary_mare.xlsx\")",
    "crumbs": [
      "R Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "data_exploration.html#workflow",
    "href": "data_exploration.html#workflow",
    "title": "2  Data Exploration",
    "section": "\n2.5 Workflow",
    "text": "2.5 Workflow\n\n\n\n\n\n\nIncomplete\n\n\n\nPlan to add information on proper workflow. One script to download, clean, and save data that analysis is conducted on.\nOne script to run the analysis and export data/tables/figures\nMetadata, readme, etc.\n\n\n\n\n\n\nNOAA National Estuarine Research Reserve System (NERRS). 2022. System-Wide Monitoring Program. NOAA NERRS Centralized Data Management Office. http://cdmo.baruch.sc.edu/.",
    "crumbs": [
      "R Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Exploration</span>"
    ]
  },
  {
    "objectID": "streamflow.html#data-sources",
    "href": "streamflow.html#data-sources",
    "title": "3  Streamflow",
    "section": "3.1 Data Sources",
    "text": "3.1 Data Sources",
    "crumbs": [
      "Hydrology",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Streamflow</span>"
    ]
  },
  {
    "objectID": "streamflow.html#summarise",
    "href": "streamflow.html#summarise",
    "title": "3  Streamflow",
    "section": "3.2 Summarise",
    "text": "3.2 Summarise",
    "crumbs": [
      "Hydrology",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Streamflow</span>"
    ]
  },
  {
    "objectID": "streamflow.html#flow-duration-curve",
    "href": "streamflow.html#flow-duration-curve",
    "title": "3  Streamflow",
    "section": "3.3 Flow Duration Curve",
    "text": "3.3 Flow Duration Curve",
    "crumbs": [
      "Hydrology",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Streamflow</span>"
    ]
  },
  {
    "objectID": "stage-discharge.html#data",
    "href": "stage-discharge.html#data",
    "title": "4  Stage-Discharge Rating Curves",
    "section": "\n4.1 Data",
    "text": "4.1 Data\nThis example uses field measurements of stream stage and instantaneous discharge measured at the COMO NEON field site (NEON (National Ecological Observatory Network), n.d.). Raw data is available in the neon.rds file within the tutorial download.\n\nlibrary(tidyverse)\nlibrary(twriTemplates)\n\n## neon_stage_discharge is an example dataset in the twriTemplates package\nstage_discharge &lt;- neon_stage_discharge\nglimpse(stage_discharge)\n\nRows: 134\nColumns: 36\n$ uid                     &lt;chr&gt; \"1068e9ad-37bd-42ac-96d2-b999545adefe\", \"06d63…\n$ recorduid               &lt;chr&gt; \"3b2d4830-50cd-40c8-a772-790d2386c33d\", \"5ed72…\n$ domainID                &lt;chr&gt; \"D13\", \"D13\", \"D13\", \"D13\", \"D13\", \"D13\", \"D13…\n$ siteID                  &lt;chr&gt; \"COMO\", \"COMO\", \"COMO\", \"COMO\", \"COMO\", \"COMO\"…\n$ namedLocation           &lt;chr&gt; \"COMO.AOS.discharge\", \"COMO.AOS.discharge\", \"C…\n$ collectedBy             &lt;chr&gt; \"LCLARK\", \"HSCHARTEL\", \"DMONAHAN\", \"HSCHARTEL\"…\n$ startDate               &lt;dttm&gt; 2015-08-11 15:30:00, 2015-08-24 16:16:00, 201…\n$ collectDate             &lt;dttm&gt; 2015-08-11 15:30:00, 2015-08-24 16:16:00, 201…\n$ streamStage             &lt;dbl&gt; 0.30, 0.34, 0.33, 0.31, 0.32, 0.35, 0.37, 0.74…\n$ streamStageUnits        &lt;chr&gt; \"meter\", \"meter\", \"meter\", \"meter\", \"meter\", \"…\n$ handheldDeviceID        &lt;chr&gt; \"122581001261\", \"122581001261\", \"122581001261\"…\n$ velocitySensorID        &lt;chr&gt; \"132660300436\", \"132660300436\", \"132660300436\"…\n$ filterParamTime         &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 20, 10, 10…\n$ stationEntryTest        &lt;chr&gt; \"Non-fixed\", \"Non-fixed\", \"Non-fixed\", \"Non-fi…\n$ flowCalculation         &lt;chr&gt; \"Mid-section\", \"Mid-section\", \"Mid-section\", \"…\n$ waterEdge               &lt;chr&gt; \"Right edge water\", \"Right edge water\", \"Right…\n$ totalDischarge          &lt;dbl&gt; 10.81, 5.71, 6.58, 3.49, 3.15, 9.39, 12.75, 77…\n$ totalDischargeUnits     &lt;chr&gt; \"litersPerSecond\", \"litersPerSecond\", \"litersP…\n$ samplingProtocolVersion &lt;chr&gt; \"NEON.DOC.001085vB\", \"NEON.DOC.001085vB\", \"NEO…\n$ averageVelocityUnits    &lt;chr&gt; \"meterPerSecond\", \"meterPerSecond\", \"meterPerS…\n$ waterDepthUnits         &lt;chr&gt; \"meter\", \"meter\", \"meter\", \"meter\", \"meter\", \"…\n$ tapeDistanceUnits       &lt;chr&gt; \"meter\", \"meter\", \"meter\", \"meter\", \"meter\", \"…\n$ flowCalcQF              &lt;chr&gt; \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"…\n$ dischargeUnitsQF        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ streamStageUnitsQF      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ averageVelocityUnitsQF  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ waterDepthUnitsQF       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ tapeDistanceUnitsQF     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ lowVelocityFinalQF      &lt;dbl&gt; 5, 20, 0, 10, 0, 17, 0, 45, 0, 11, 25, 11, 6, …\n$ finalDischarge          &lt;dbl&gt; 10.71, 5.62, 6.52, 3.44, 3.07, 9.29, 12.75, 78…\n$ totalDischargeCalcQF    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ profileName             &lt;chr&gt; \"COMO\", \"COMO\", \"COMO\", \"COMO\", \"COMO\", \"COMO\"…\n$ stageImpractical        &lt;chr&gt; \"OK\", \"OK\", \"OK\", \"OK\", \"OK\", \"OK\", \"OK\", \"OK\"…\n$ dataQF                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ publicationDate         &lt;chr&gt; \"20211222T003739Z\", \"20211222T003739Z\", \"20211…\n$ release                 &lt;chr&gt; \"RELEASE-2022\", \"RELEASE-2022\", \"RELEASE-2022\"…",
    "crumbs": [
      "Hydrology",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Stage-Discharge Rating Curves</span>"
    ]
  },
  {
    "objectID": "stage-discharge.html#plot-data",
    "href": "stage-discharge.html#plot-data",
    "title": "4  Stage-Discharge Rating Curves",
    "section": "\n4.2 Plot Data",
    "text": "4.2 Plot Data\nFirst step is to plot the data.\n\n# plot observations over time\nggplot(stage_discharge) +\n  geom_point(aes(collectDate, finalDischarge)) +\n  labs(x = \"Date\", y = \"Discharge [L/s]\") +\n  theme_TWRI_print()\n\nWarning: Removed 9 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n# plot the stage and discharge relationship\nggplot(stage_discharge) +\n  geom_point(aes(streamStage, finalDischarge)) +\n  labs(x = \"Stage [m]\", y = \"Discharge [L/s]\") +\n  scale_x_log10() +\n  scale_y_log10() +\n  theme_TWRI_print()\n\nWarning: Removed 9 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nLooks like there might be a shift in the rating curve at some point. Use dplyr to create a new year variable and the gghighlight package to explore it.\n\n# install.packages(\"gghighlight\")\nlibrary(gghighlight)\nstage_discharge |&gt; \n  filter(!is.na(finalDischarge)) |&gt; \n  mutate(year = format(collectDate, format = \"%Y\")) |&gt; \n  ggplot() +\n  geom_point(aes(streamStage, finalDischarge, color = year)) +\n  facet_wrap(~year) +\n  gghighlight(max_highlight = 8,\n              use_direct_label = FALSE,\n              calculate_per_facet = FALSE)   +\n  labs(x = \"Stage [m]\", y = \"Discharge [L/s]\") +\n  theme_TWRI_print()",
    "crumbs": [
      "Hydrology",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Stage-Discharge Rating Curves</span>"
    ]
  },
  {
    "objectID": "stage-discharge.html#fit-rating-curve",
    "href": "stage-discharge.html#fit-rating-curve",
    "title": "4  Stage-Discharge Rating Curves",
    "section": "\n4.3 Fit Rating Curve",
    "text": "4.3 Fit Rating Curve\nIt appears the rating curve shifted after 2016 and possibly after 2018. In practice we would fit different rating curves based on this information. For this example the rating curve will only be fit to 2019-2021 data. We will use this data to estimate the \\(K\\), \\(H_0\\) and \\(Z\\) parameters in the power function described at the top of the chapter using nonlinear least squares.\n\n# install.packages(\"nls.multstart\")\n# install.packages(\"nlstools\")\nlibrary(nls.multstart)\nlibrary(nlstools)\n\n# clean the data a little bit and filter\nstage_discharge &lt;- stage_discharge |&gt; \n  filter(!is.na(finalDischarge)) |&gt;\n  mutate(year = format(startDate, \"%Y\")) |&gt; \n  filter(year &gt;= 2018) |&gt; \n  # convert units to feet and cfs\n  mutate(streamStage = streamStage * 3.28084,\n         finalDischarge = finalDischarge * 0.0353147)\n\n# Set the equation\nf_Q &lt;- formula(finalDischarge ~ K*(streamStage - H_0)^Z)\n\n# Some initial starting values\nstart_lower &lt;- c(K = -10, Z = -10, H_0 = 0.02)\nstart_upper &lt;- c(K = 10, Z = 10, H_0 = 1)\n\n# nonlinear least squares\nm1 &lt;- nls_multstart(f_Q,\n          data = stage_discharge,\n          iter = 1000,\n          start_lower = start_lower,\n          start_upper = start_upper,\n          supp_errors = 'Y',\n          lower = c(K = -10, Z = -10, H_0 = 0),\n          control = minpack.lm::nls.lm.control(maxiter = 1000L))\n\nsummary(m1)\n\n\nFormula: finalDischarge ~ K * (streamStage - H_0)^Z\n\nParameters:\n    Estimate Std. Error t value Pr(&gt;|t|)    \nK     5.0399     1.3421   3.755 0.000335 ***\nH_0   0.7874     0.1315   5.990 6.30e-08 ***\nZ     1.9097     0.2568   7.436 1.23e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.889 on 77 degrees of freedom\n\nNumber of iterations to convergence: 41 \nAchieved convergence tolerance: 1.49e-08\n\n\nNLS estimated parameters are: \\(K =\\) 5.0398732, \\(H_0 =\\) 0.7874016, and \\(Z =\\) 0.7874016. Before using these parameter, evaluated the goodness of fit using the model residuals (add citation or note with more resources here).\n\nShow the code# for easy multipanel plots, use the patchwork package\n#install.packages(\"patchwork\")\nlibrary(patchwork)\n\nstd_resids &lt;- as_tibble(nlsResiduals(m1)$resi2)\n\nstage_discharge &lt;- stage_discharge |&gt; \n    mutate(fits = std_resids$`Fitted values`,\n         residuals = std_resids$`Standardized residuals`)\n\n\np1 &lt;- ggplot(stage_discharge) +\n  geom_density(aes(residuals)) +\n  labs(x = \"Standardized Residuals\",\n       y = \"Count\",\n       subtitle = \"Distribution of standardized residuals\") +\n  theme_TWRI_print()\n\n\np2 &lt;- ggplot(stage_discharge) +\n  geom_point(aes(streamStage, residuals), color = \"steelblue\", alpha = 0.4) +\n  labs(x = \"Stream Height [ft]\",\n       y = \"Standardized Residuals\",\n       subtitle = \"Residuals against stream height\") +\n  theme_TWRI_print()\n\np3 &lt;- ggplot(stage_discharge) +\n  geom_point(aes(fits, finalDischarge), color = \"steelblue\", alpha = 0.4) +\n  labs(x = \"Model Fits\",\n       y = \"Measured Discharge [cfs]\",\n       subtitle = \"Measured against fitted\") +\n  theme_TWRI_print()\n\np4 &lt;- ggplot(stage_discharge) +\n  stat_qq(aes(sample = residuals), color = \"steelblue\", alpha = 0.4) +\n  stat_qq_line(aes(sample = residuals)) +\n  labs(x = \"Theoretical\",\n       y = \"Standardized Residuals\",\n       subtitle = \"Sample Quantile against theoretical quantile\") +\n  theme_TWRI_print()\n\n\n# patchwork allows us to assemble plots using + and /\n\n(p1 + p2) / (p3 + p4)\n\n\n\n\n\n\n\n\n\nThe plots indicate increasing residual variance as stream height increases and heavy tails in Q-Q plot. Two options from here are to (1) fit a rating curve per year or (2) fit a piece-wise log-linear model that assumes a different relationship along different parts of the rating curve. Because of the large residual variance at the top of curve, I think fitting a curve per year will do the job. You may also choose to fit models by season (small streams in particular may see large changes in rating curves do to changes in stream bank vegetation).",
    "crumbs": [
      "Hydrology",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Stage-Discharge Rating Curves</span>"
    ]
  },
  {
    "objectID": "stage-discharge.html#fit-multiple-curves",
    "href": "stage-discharge.html#fit-multiple-curves",
    "title": "4  Stage-Discharge Rating Curves",
    "section": "\n4.4 Fit Multiple Curves",
    "text": "4.4 Fit Multiple Curves\n\n\n\n\n\n\nNote\n\n\n\nThis section is slightly more advanced and requires some understanding of list structures in R and nested dataframes.\n\n\nThe purrr package facilitates running a function on a list of nested data. The idea here is to subset the dataframe by year, create a list with each item in the list being a subset of the dataframe, then running nls_multstart() on each item in that list. Sounds like a loop huh? We achieve this using for() or lapply() functions in base R. The nice thing about doing it with purrr is that we can keep everything together in a single dataframe. The first step is to create a nested dataframe:\n\nnested_data &lt;- neon_stage_discharge |&gt; \n  filter(!is.na(finalDischarge)) |&gt;\n  mutate(year = format(startDate, \"%Y\")) |&gt; \n  filter(year &gt;= 2018) |&gt; \n  mutate(streamStage = streamStage * 3.28084,\n         finalDischarge = finalDischarge * 0.0353147) |&gt; \n  ## group data by year\n  group_by(year) |&gt; \n  ## nest the data by year\n  nest()\n\nnested_data\n\n# A tibble: 4 × 2\n# Groups:   year [4]\n  year  data              \n  &lt;chr&gt; &lt;list&gt;            \n1 2018  &lt;tibble [20 × 36]&gt;\n2 2019  &lt;tibble [24 × 36]&gt;\n3 2020  &lt;tibble [15 × 36]&gt;\n4 2021  &lt;tibble [21 × 36]&gt;\n\n\nNow we use the map() function in purrr to iterate the nls.multstart() function on each nested dataframe:\n\nnested_data &lt;- nested_data |&gt; \n  mutate(model_output = map(.x = data,\n                            ~nls_multstart(formula = f_Q,\n                                           data = .x,\n                                           iter = 1000,\n                                           start_lower = start_lower,\n                                           start_upper = start_upper,\n                                           supp_errors = 'Y',\n                                           lower = c(K = -10, Z = -10, H_0 = 0),\n                                           control = minpack.lm::nls.lm.control(maxiter = 1000L))))\nnested_data\n\n# A tibble: 4 × 3\n# Groups:   year [4]\n  year  data               model_output\n  &lt;chr&gt; &lt;list&gt;             &lt;list&gt;      \n1 2018  &lt;tibble [20 × 36]&gt; &lt;nls&gt;       \n2 2019  &lt;tibble [24 × 36]&gt; &lt;nls&gt;       \n3 2020  &lt;tibble [15 × 36]&gt; &lt;nls&gt;       \n4 2021  &lt;tibble [21 × 36]&gt; &lt;nls&gt;       \n\n\nWe created a new column called model_output that is a list of the output from the nls_multstart() function that was run on each item in the data column. Grab the residuals and fits from each model:\n\nShow the codenested_data &lt;- nested_data |&gt; \n  mutate(residuals = map(model_output,\n                         ~as_tibble(nlsResiduals(.x)$resi2))) |&gt; \n  unnest(c(data,residuals))\n\np1 &lt;- ggplot(nested_data) +\n  geom_point(aes(`Fitted values`, `Standardized residuals`)) +\n  facet_wrap(~year) +\n  labs(x = \"Fitted\",\n       y = \"Standardized Residuals\",\n       subtitle = \"Sample Quantile against theoretical quantile\") +\n  theme_TWRI_print()\n\np2 &lt;- ggplot(nested_data) +\n  geom_density(aes(`Standardized residuals`)) +\n  facet_wrap(~year) +\n  labs(x = \"Standardized Residuals\",\n       y = \"Count\",\n       subtitle = \"Distribution of standardized residuals\") +\n  theme_TWRI_print()\n\np3 &lt;- ggplot(nested_data) +\n  geom_point(aes(`Fitted values`, finalDischarge), color = \"steelblue\", alpha = 0.4) +\n  facet_wrap(~year) +\n  labs(x = \"Model Fits\",\n       y = \"Measured Discharge [cfs]\",\n       subtitle = \"Measured against fitted\") +\n  theme_TWRI_print()\n\np4 &lt;- ggplot(nested_data) +\n  stat_qq(aes(sample = `Standardized residuals`), color = \"steelblue\", alpha = 0.4) +\n  stat_qq_line(aes(sample = `Standardized residuals`)) +\n  facet_wrap(~year) +\n  labs(x = \"Theoretical\",\n       y = \"Standardized Residuals\",\n       subtitle = \"Sample Quantile against theoretical quantile\") +\n  theme_TWRI_print()\n\np1 / p2 / p3 / p4\n\n\n\n\n\n\n\n\n\nThe results are a mixed bag. For the most part, the residuals are tighter to mean zero and the tails are not as heavy as the first example. We will assume the data is good enough to continue the example. In practice, I might explore the use of seasonal rating curves or piece-wise functions.\nNext lets make a nice plot showing the rating curve with the observed data.\n\nfits &lt;- nested_data |&gt; \n  nest(data = -c(year, model_output)) |&gt; \n  # create a new dataframe by group\n  # this includes the full range of the predictor variable\n  # so we can draw a nice smooth line using predictions\n  mutate(newdata = map(data,\n                       ~{\n                         tibble(streamStage = seq(min(nested_data$streamStage),\n                                                  max(nested_data$streamStage),\n                                                  length.out = 100))\n                       })) |&gt; \n  mutate(fits = map2(newdata, model_output,\n                     ~{predict(.y, .x)})) |&gt; \n  unnest(c(newdata, fits))\n\nggplot() +\n  geom_point(data = nested_data,\n             aes(streamStage, finalDischarge, color = year)) +\n  geom_line(data = fits,\n            aes(streamStage, fits, color = year)) +\n  facet_wrap(~year) +\n  gghighlight(max_highlight = 8,\n              use_direct_label = FALSE,\n              calculate_per_facet = FALSE)   +\n  labs(x = \"Stage [ft]\", y = \"Discharge [cfs]\") +\n  theme_TWRI_print()\n\n\n\n\n\n\n\n\n\n\n\n\n\nNEON (National Ecological Observatory Network). n.d. Discharge Field Collection (DP1.20048.001), RELEASE-2022. https://doi.org/10.48443/eaak-tt31.",
    "crumbs": [
      "Hydrology",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Stage-Discharge Rating Curves</span>"
    ]
  },
  {
    "objectID": "drainage-area-ratio.html#basic-approach",
    "href": "drainage-area-ratio.html#basic-approach",
    "title": "5  Drainage Area Ratio",
    "section": "\n5.1 Basic Approach",
    "text": "5.1 Basic Approach\nThe basic approach where the exponent \\(\\phi\\) equals one is shown below. We will use known data from two USGS gages to evaluate performance.\n\n## load packages\nlibrary(dataRetrieval)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(twriTemplates)\n\nFor this example, I am using two USGS sites. In application you would only use a single site.\n\n## Get drainage areas for two watersheds with dataRetrieval\nsiteInfo &lt;- readNWISsite(c('08110100', '08109800'))\nsiteInfo$contrib_drain_area_va\n\n[1] 244 195\n\n\nDownload the mean daily streamflow from 08110100 (Davidson Creek):\n\nDavidson &lt;- readNWISdv('08110100', \"00060\", \"2015-01-01\", \"2020-12-31\")\nDavidson &lt;- renameNWISColumns(Davidson)\nhead(Davidson)\n\n  agency_cd  site_no       Date   Flow Flow_cd\n1      USGS 08110100 2015-01-01   4.82       A\n2      USGS 08110100 2015-01-02  17.20       A\n3      USGS 08110100 2015-01-03 258.00       A\n4      USGS 08110100 2015-01-04 288.00       A\n5      USGS 08110100 2015-01-05  91.80       A\n6      USGS 08110100 2015-01-06  40.90       A\n\n\nDownload mean daily streamflows at 08109800 (Yegua Creek):\n\nYegua &lt;- readNWISdv('08109800', \"00060\", \"2015-01-01\", \"2020-12-31\")\nYegua &lt;- renameNWISColumns(Yegua)\n\nNow create a dataframe to compare measured streamflows at Yegua Creek with DAR estimated flows using Davidson as the source creek:\n\n# calculate the area ratio\nDAR &lt;- siteInfo$contrib_drain_area_va[2] / siteInfo$contrib_drain_area_va[1]\n\n# create columns for the area ratio and DAR*Davidson Flow\nDavidson &lt;- Davidson |&gt; \n  mutate(DAR = DAR,\n         est_Yegua_Q = DAR * Flow)\n\n# join the data by date\nestimates &lt;- Yegua |&gt; \n  left_join(Davidson |&gt;  select(Date, Davidson_Flow = Flow, DAR, est_Yegua_Q),\n            by = c(\"Date\" = \"Date\"))\n\nggplot(estimates) +\n  geom_point(aes(Flow, est_Yegua_Q), \n             color = \"steelblue\",\n             alpha = 0.5) +\n  geom_abline(slope = 1) +\n  coord_equal() +\n  scale_x_continuous(trans = \"pseudo_log\",\n                     breaks = c(1,10,100,1000,10000)) +\n  scale_y_continuous(trans = \"pseudo_log\",\n                     breaks = c(1,10,100,1000,10000)) +\n  theme_TWRI_print() +\n  labs(x = \"Measured Flow [cfs]\",\n       y = \"DAR Estimate [cfs]\")",
    "crumbs": [
      "Hydrology",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Drainage Area Ratio</span>"
    ]
  },
  {
    "objectID": "drainage-area-ratio.html#parameterized-exponent",
    "href": "drainage-area-ratio.html#parameterized-exponent",
    "title": "5  Drainage Area Ratio",
    "section": "\n5.2 Parameterized Exponent",
    "text": "5.2 Parameterized Exponent\n\n\n\n\n\n\nIncomplete\n\n\n\nStill needs to be written.",
    "crumbs": [
      "Hydrology",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Drainage Area Ratio</span>"
    ]
  },
  {
    "objectID": "drainage-area-ratio.html#other-data-considerations",
    "href": "drainage-area-ratio.html#other-data-considerations",
    "title": "5  Drainage Area Ratio",
    "section": "\n5.3 Other Data Considerations",
    "text": "5.3 Other Data Considerations\n\n\n\n\n\n\nIncomplete\n\n\n\nAdd discussion about naturalized flows.",
    "crumbs": [
      "Hydrology",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Drainage Area Ratio</span>"
    ]
  },
  {
    "objectID": "water-quality.html#sources-of-data",
    "href": "water-quality.html#sources-of-data",
    "title": "6  Water Quality Data",
    "section": "\n6.1 Sources of Data",
    "text": "6.1 Sources of Data\n\n6.1.1 Sampling Data\nThe official data source for state water quality data used in surface water quality assessments in Texas is the Surface Water Quality Monitoring Information System (SWQMIS). My suggestion for obtaining station or water body specific data is to ask our employees that have authorized access to SWQMIS to make a data pull for you. There are two other options. First, TCEQ has a spatial explorer that you can point and click to download data by stream segment https://www80.tceq.texas.gov/SwqmisPublic/index.htm. This will download a pipe (|) delimited text file with all the water quality monitoring data for the segment. Second, the CRP Data Tool provides a way to query the SWQMIS by date, parameter code, parameter group, basin, segment, and station. Data is also provided as a pipe delimited text file with the queried data. Currently, there are no capabilities to make queries from within R.\nThe Water Quality Portal (WQP) is the EPA and USGS national database (formally STORET) for storing and providing discrete water quality monitoring data collected by states, tribes, federal and other partners. The WQP is handy for large or automated data downloads for various projects. One drawback is that the WQP and SWQMIS do not have common variables across both databases. For example, data from the WQP does not include Project Type codes that indicate the purpose for the monitoring event (flow biased storm samples or unbiased ambient samples) that might impact data analysis. WQP and SWQMIS do not use the same parameter codes and sometimes important data such as analysis methods or units get omitted in the data uploads to WQP. In general, for our state funded projects, use the SWQMIS database.\n\n6.1.2 Standards\nThe most up to date EPA approved standards are available at EPA or at TCEQ. The applicable standard for a waterbody will vary based on assessed use and sometimes season. Do not assume the standard based on a nearby or upstream/downstream water body, double check the standards.\n\n6.1.3 Assessment Summaries\nTCEQ doesn’t publicly publish the data that goes into each water body assessment. However, summaries of assessed data by assessment unit are available. The full integrated report for the current cycle is available at https://www.tceq.texas.gov/waterquality/assessment. Within each integrated report is a link to the water body assessments by basin: https://www.tceq.texas.gov/waterquality/assessment/22twqi/22basinlist. These pdfs provide information about the use, criteria, data range, number of data assessed, number of excrescences, mean (or geomean) of data assessed and additional listing information.",
    "crumbs": [
      "Water Quality",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Water Quality Data</span>"
    ]
  },
  {
    "objectID": "water-quality.html#summarizing-data",
    "href": "water-quality.html#summarizing-data",
    "title": "6  Water Quality Data",
    "section": "\n6.2 Summarizing Data",
    "text": "6.2 Summarizing Data",
    "crumbs": [
      "Water Quality",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Water Quality Data</span>"
    ]
  },
  {
    "objectID": "water-quality.html#data",
    "href": "water-quality.html#data",
    "title": "6  Water Quality Data",
    "section": "\n6.3 Data",
    "text": "6.3 Data\n\n\nGet the data: These examples use the swqmispublicdata.txt data in the example data\nSWQMIS column names have spaces and characters in them. This make it difficult to refer to those variables because object names cannot have spaces in R. The janitor package has a handy function to automatically format column/variable names in a dataframe and we will use it here.\n\n# install.packages(\"janitor\")\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(twriTemplates)\n\ndf &lt;- read_delim(\"data/swqmispublicdata.txt\",\n                 delim = \"|\",\n                 # see awkward column names that have to be wrapped\n                 # in between \"`\" (escape) marks.\n                 col_types = cols_only(\n                   `Segment ID` = col_character(),\n                   `Station ID` = col_factor(),\n                   `Station Description` = col_character(),\n                   `End Date` = col_date(format = \"%m/%d/%Y\"),\n                   `Collecting Entity` = col_character(),\n                   `Monitoring Type` = col_character(),\n                   `Composite Category` = col_character(),\n                   `Parameter Name` = col_character(),\n                   `Parameter Code` = col_character(),\n                   `Value` = col_number(),\n                   `RFA/Tag ID` = col_character()\n                 )) |&gt; \n  clean_names()\n  \ndf\n\n# A tibble: 5,648 × 11\n   segment_id station_id station_description        end_date   collecting_entity\n   &lt;chr&gt;      &lt;fct&gt;      &lt;chr&gt;                      &lt;date&gt;     &lt;chr&gt;            \n 1 1502       12517      TRES PALACIOS CREEK AT FM… 1987-09-22 TCEQ REGIONAL OF…\n 2 1502       12517      TRES PALACIOS CREEK AT FM… 1987-09-22 TCEQ REGIONAL OF…\n 3 1502       12517      TRES PALACIOS CREEK AT FM… 1987-09-22 TCEQ REGIONAL OF…\n 4 1502       12517      TRES PALACIOS CREEK AT FM… 1987-09-22 TCEQ REGIONAL OF…\n 5 1502       12517      TRES PALACIOS CREEK AT FM… 1987-09-22 TCEQ REGIONAL OF…\n 6 1502       12517      TRES PALACIOS CREEK AT FM… 1987-09-22 TCEQ REGIONAL OF…\n 7 1502       12517      TRES PALACIOS CREEK AT FM… 1987-09-22 TCEQ REGIONAL OF…\n 8 1502       12517      TRES PALACIOS CREEK AT FM… 1987-09-22 TCEQ REGIONAL OF…\n 9 1502       12517      TRES PALACIOS CREEK AT FM… 1987-09-22 TCEQ REGIONAL OF…\n10 1502       12517      TRES PALACIOS CREEK AT FM… 1987-09-22 TCEQ REGIONAL OF…\n# ℹ 5,638 more rows\n# ℹ 6 more variables: monitoring_type &lt;chr&gt;, composite_category &lt;chr&gt;,\n#   parameter_name &lt;chr&gt;, parameter_code &lt;chr&gt;, value &lt;dbl&gt;, rfa_tag_id &lt;chr&gt;\n\n\n\n6.3.1 Summary Stats\nThe most common summary statistics are mean or geometric means. How we report this info depends on the project. For example, some projects might group all the stations within a single assessment unit together and report a mean. In other projects, station-specific means might be relevant. It is usually useful to explore both. An example of calculating station-specific geometric means is below:\n\n#install.packages(DescTools)\nlibrary(DescTools)\ndf |&gt; \n  # parameter code for E. coli\n  filter(parameter_code == \"31699\") |&gt; \n  group_by(station_id) |&gt; \n  summarise(geomean = Gmean(value),\n            min = min(value),\n            max = max(value),\n            n = n(),\n            first_date = min(end_date),\n            last_date = max(end_date))\n\n# A tibble: 2 × 7\n  station_id geomean   min   max     n first_date last_date \n  &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;date&gt;     &lt;date&gt;    \n1 12517         118.   6.3 24192    88 2000-12-20 2022-05-17\n2 15325         210.   1    2400    35 2019-06-25 2022-05-17\n\n\n\n6.3.2 Exceedance Probabilities\nAnother relevant measure is the number of exceedances of the criterion. This is relevant because we can use a binomial test to evaluate the probability that a water body is above an acceptable exceedance rate (typically 0.1 or 0.2 depending on the parameter). Smith et al. (2001) is highly recommended reading about why and how the binomal test is applied to assess water quality conditions.\nThe following example evaluates the exceedance rate for total phosphorus. For total phosphorus, the screening level in Texas is currently 0.69 mg/L which is evaluated using the binomal test and a 20% exceedance rate.\n(https://www.tceq.texas.gov/downloads/water-quality/assessment/integrated-report-2022/2022-guidance.pdf)\n\nexceedance_df &lt;- df |&gt; \n  ## filter to total phos\n  filter(parameter_code == \"00665\") |&gt;\n  filter(end_date &gt;= as.Date(\"2013-12-01\") &\n           end_date &lt;= as.Date(\"2020-11-30\")) |&gt; \n  filter(monitoring_type == \"Routine - Monitoring not intentionally targeted toward any environmental condition or event\") |&gt; \n  ## create a new variable 1 = exceedance, 0 = non-exceedance\n  mutate(\n    exceedance = case_when(\n      value &lt; 0.69 ~ 0,\n      value &gt;= 0.69 ~ 1\n    )) |&gt; \n  summarize(total = n(),\n            n_exceedance = sum(exceedance),\n            probability = n_exceedance/total)\n\nexceedance_df\n\n# A tibble: 1 × 3\n  total n_exceedance probability\n  &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1    18            2       0.111\n\n\nThis information nicely matches the assessment summary information provided for AU 1502_01 in the 2022 Texas Integrated Report.\nIf we want to run the binomial test on this data:\n\nbinom.test(x = exceedance_df$n_exceedance,\n           n = exceedance_df$total,\n           p = 0.2, alternative = \"g\")\n\n\n    Exact binomial test\n\ndata:  exceedance_df$n_exceedance and exceedance_df$total\nnumber of successes = 2, number of trials = 18, p-value = 0.9009\nalternative hypothesis: true probability of success is greater than 0.2\n95 percent confidence interval:\n 0.02011068 1.00000000\nsample estimates:\nprobability of success \n             0.1111111 \n\n\nThe null hypothesis is that the probability of TP exceeding 0.69 is less than or equal to 0.2, which the binomial test failed to reject (p&gt;0.05). Thus, the assessment unit meets the water quality screening level.",
    "crumbs": [
      "Water Quality",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Water Quality Data</span>"
    ]
  },
  {
    "objectID": "water-quality.html#figures",
    "href": "water-quality.html#figures",
    "title": "6  Water Quality Data",
    "section": "\n6.4 Figures",
    "text": "6.4 Figures\nTypically, we develop at least the following water quality figures:\n\nscatterplots of measured values over time by waterbody and station\nboxplots or histograms displaying the distribution of measured values (upstream to downstream is particularly useful)\ndepending on the amount of data, a “rolling” statistic like exceedance probability, mean or geometric mean\n\n\n# install.packages(\"ggtext\")\nlibrary(ggtext)\n\necoli &lt;- df |&gt; \n  filter(parameter_code == \"31699\")\n\n## the E. coli critera is 126 MPN/100mL\ncritera &lt;- 126\n\nggplot(ecoli) +\n  ## add a dotted line to show the water quality criteria\n  geom_hline(yintercept = critera, linetype = 2) +\n  ## measured points colored by the station\n  geom_point(aes(end_date, value, color = station_id)) +\n  ## this will add a label to the dotted line\n  annotate(\"label\", \n           x = max(ecoli$end_date)+60, \n           y = critera,\n           label = \"126 MPN/100 mL\",\n           hjust = 0,\n           size = 4,\n           family = \"OpenSansCondensed_TWRI\") +\n  ## need to make some space at the end of the plot for\n  ## the label (adds 5% and 20% space to start and end)\n  scale_x_date(expand = expansion(mult = c(0.05, 0.20))) +\n  ## use log transformed spacing on the y-axis\n  scale_y_log10() +\n  scale_color_discrete_twri(name = \"Stations\") +\n  labs(x = \"Date\", y = \"*E. coli* [MPN/100 mL]\") +\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown())\n\n\n\n\n\n\n\n\n\nBoxplots are used to display distribution of values. The twriTemplates package has a function to include a boxplot legend that explains the components of a boxplot.\n\n# install.packages(\"patchwork\")\nlibrary(patchwork)\n\nbplot &lt;- ggplot(ecoli) +\n  ## add a dotted line to show the water quality criteria\n  geom_hline(yintercept = critera, linetype = 2) +\n  ## measured points colored by the station\n  geom_boxplot(aes(station_id, value, fill = station_id)) +\n  scale_y_log10() +\n  scale_fill_discrete_twri(name = \"Stations\") +\n  labs(x = \"Station\", y = \"*E. coli* [MPN/100 mL]\") +\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown())\n\nlegend &lt;- ggplot_box_legend(point_label = \" values outside the\\ninterquartile range\")\n\n# the ggplot_box_legend function is in the twriTemplates package\nbplot + legend + plot_layout(design = \"112\")\n\n\n\n\n\n\n\n\n\n\n6.4.1 Rolling statistics\nRolling statistics are a useful way of displaying how a statistical measure (mean, geomean, etc.) changes over time. These are typically done with regularly measured data with equal intervals and is easily calculated using dplyr window functions. However, water quality data is unequal and we are generally interested in the statistic over a specific time window regardless of the number of samples in that window (so we can’t just look back \\(n\\) rows and calculate the statistic). The runner packages solves this by letting us set the window by amount of time.1 runner() utilizes the date column and looks back to all the data within the time frame specified in the k argument, then uses that data to calculate the function in argument f.\n1 Some of our older scripts used the tbrf package to do this. runner is now reccomended because it is much faster and more flexible.\n# install.packages(\"runner\")\nlibrary(runner)\n\n# first calculate the rolling statistic\n\nrollingmean &lt;- df %&gt;%\n    # parameter code for E. coli\n  filter(parameter_code == \"31699\") %&gt;% \n  group_by(station_id) %&gt;%\n  arrange(end_date) %&gt;%\n  mutate(Gmean = runner(x = value,\n                        k = \"7 years\",\n                        idx = end_date,\n                        f = function(x) DescTools::Gmean(x)))\n\n\nggplot(rollingmean) +\n  geom_hline(yintercept = critera, linetype = 2) +\n  geom_point(aes(end_date, value, color = station_id)) +\n  geom_step(aes(end_date, Gmean, color = station_id)) +\n  scale_y_log10() +\n  scale_color_discrete_twri(name = \"Stations\") +\n  labs(x = \"Date\", y = \"*E. coli* [MPN/100 mL]\") +\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown())\n\n\n\n\n\n\n\n\n\nWe can also add confidence intervals to this plot. This is a good way to communicate how certain we are that the geometric mean is actually above or below the water quality criterion.\n\nrollingmean &lt;- rollingmean %&gt;%\n  mutate(Gmean_lwr = runner(x = value,\n                        k = \"7 years\",\n                        idx = end_date,\n                        f = function(x) {\n                          DescTools::Gmean(x, \n                                           conf.level = 0.95)[2]\n                          }),\n         Gmean_upr = runner(x = value,\n                        k = \"7 years\",\n                        idx = end_date,\n                        f = function(x) {\n                          DescTools::Gmean(x, \n                                           conf.level = 0.95)[3]\n                          }))\n\n\nggplot(rollingmean) +\n  geom_hline(aes(yintercept = critera, linetype = \"Criteria\"), alpha = 0.5) +\n  geom_point(aes(end_date, value, color = \"Measured *E. coli* value\")) +\n  geom_step(aes(end_date, Gmean, linetype = \"7-yr rolling geomean\")) +\n  geom_step(aes(end_date, Gmean_upr, linetype = \"95% confidence interval\")) +\n  geom_step(aes(end_date, Gmean_lwr, linetype = \"95% confidence interval\")) +\n  scale_y_log10(limits = c(1,10000)) +\n  facet_wrap(~station_id, scales = \"free_x\") +\n  scale_color_manual(values = \"steelblue\") +\n  labs(x = \"Date\", y = \"*E. coli* [MPN/100 mL]\") +\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown(),\n        # turn off legend title\n        legend.title = element_blank(),\n        legend.text = element_markdown(),\n        # turn off the horizontal gridlines\n        # because they look similar to the \n        # 95% confidence intervals\n        panel.grid = element_blank())",
    "crumbs": [
      "Water Quality",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Water Quality Data</span>"
    ]
  },
  {
    "objectID": "water-quality.html#notes-on-figures",
    "href": "water-quality.html#notes-on-figures",
    "title": "6  Water Quality Data",
    "section": "\n6.5 Notes on figures",
    "text": "6.5 Notes on figures\n\n6.5.1 Log Transformations\nBacteria data is typically log distributed. Analysis of bacteria data almost always involve a log transformation to meet distributional assumptions in many statistical tests or regression models. Although in some fields, it is common to display these results in the log-transformed values, our standard practice is to back-transform the data and display the data with a log-transformed axis or scale.\nA brief example is shown below:\n\nex_data &lt;- tibble(dates = seq.Date(as.Date(\"2005-01-01\"), as.Date(\"2020-12-31\"), length.out = 100),\n             x = rlnorm(100, log(126), log(5)))\n\nggplot(ex_data) +\n    geom_point(aes(dates, log(x)), color = \"#0054a4\") +\n  labs(x = \"Date\", \n       y =\"log *E. coli* [MPN/100 mL]\",\n       title = \"Log-Transformed Values\") +\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown())-&gt; p1\n\nggplot(ex_data) +\n  geom_point(aes(dates, x), color = \"#0054a4\") +\n  scale_y_log10() +\n  labs(x = \"Date\", \n       y = \"*E. coli* [MPN/100mL]\",\n       title = \"Log-10 Y-Axis Scale\") +\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown()) -&gt; p2\n\np1 + p2\n\n\n\n\n\n\n\n\n\nThe locations of the points on both plots are the same, but the scale on the left shows the log-transformed values. Conversely, the scale on the left shows the actual values but with scale log-transformed. For general audience it is easier to discern the magnitude difference between \\(10\\) and \\(10,0000\\) E. coli than between \\(2.3026\\) and \\(9.21034\\) \\(log(E. coli)\\). If your audience are microbiologists or some field where reporting log-values are standard and the audience members intuit differences in log vaalues, don’t follow this advice.\n\n6.5.2 Error Bars\nError bars are conventionally used to display the uncertainty in estimates, not the variability or distribution of measured data. Box-plots and histograms are the appropriate graph to show measurement variability. Where error bars are used, indicate if the bars represent the standard error of the mean or confidence intervals. Do not use error bars for standard deviations or maximum/minimum measurements.\n\ndf &lt;- readr::read_csv(\"data/easterwood.csv\",\n                col_types = \"cDcnn\")\n\ndf %&gt;%\n  mutate(month = lubridate::month(date, label = TRUE),\n         year = lubridate::year(date)) %&gt;%\n  ggplot() +\n  geom_boxplot(aes(month,dailymaximumdrybulbtemperature)) +\n  theme_TWRI_print() +\n  labs(x = \"Month\",\n       y = \"Maximum daily temperature [°F]\",\n       title = \"Boxplots Show Distribution\",\n       subtitle = \"Distribution of Daily High Temperature\") -&gt; p1\ndf %&gt;%\n  mutate(month = lubridate::month(date, label = TRUE),\n         year = lubridate::year(date)) %&gt;%\n  group_by(month, year) %&gt;%\n  summarise(mean_monthly =  mean(dailymaximumdrybulbtemperature),\n            .groups = \"drop_last\") %&gt;%\n  summarise(mean_temp = mean(mean_monthly),\n            mean_se = 2*DescTools::MeanSE(mean_monthly)) %&gt;%\n  ggplot() +\n  geom_pointrange(aes(x = month, y = mean_temp, ymin = mean_temp-mean_se, ymax = mean_temp+mean_se)) +\n  theme_TWRI_print() +\n  labs(x = \"Month\",\n       y = \"Maximum daily temperature [°F]\",\n       subtitle = \"Estimated Mean Monthly High +/- 95% CI\",\n       title = \"Error Bars Show Uncertaintity\") -&gt; p2\ndf %&gt;%\n  mutate(month = lubridate::month(date, label = TRUE),\n         year = lubridate::year(date)) %&gt;%\n  group_by(month) %&gt;%\n  summarise(mean_temp = mean(dailymaximumdrybulbtemperature),\n            bad = 2*sd(dailymaximumdrybulbtemperature)) %&gt;%\n  ggplot() +\n  geom_pointrange(aes(x = month, y = mean_temp, ymin = mean_temp-bad, ymax = mean_temp+bad)) +\n  theme_TWRI_print() +\n  labs(x = \"Month\",\n       y = \"Maximum daily temperature [°F]\",\n       subtitle = \"Estimated monthly mean high temperatures +/- 2 sd.\",\n       title = \"Don't Do This!\") -&gt; p3\n(p1 + p2) / (p3 + plot_spacer())\n\n\n\n\n\n\n\n\n\n\n\n\n\nSmith, E.P., Ye, K., Hughes, C., and Shabman, L. 2001. Statistical Assessment of Violations of Water Quality Standards under Section 303(d) of the Clean Water Act. Environmental Science & Technology 35 (3): 606–12. https://doi.org/10.1021/es001159e.",
    "crumbs": [
      "Water Quality",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Water Quality Data</span>"
    ]
  },
  {
    "objectID": "load-duration.html#data",
    "href": "load-duration.html#data",
    "title": "7  Load Duration Curves",
    "section": "\n7.1 Data",
    "text": "7.1 Data\nThis example uses E. coli bacteria concentrations collected at SWQMIS station 12517 on Tres Palacios Creek. Streamflow data comes from the co-located USGS streamgage 08162600.\n\n7.1.1 Water Quality\n\n\nGet the data: These examples use the swqmispublicdata.txt data in the example data\nWe will import water quality data the same way that was covered in Chapter 6.\n\n# install.packages(\"janitor\")\nlibrary(tidyverse)\nlibrary(ggtext)\nlibrary(janitor)\nlibrary(dataRetrieval)\nlibrary(DescTools)\nlibrary(twriTemplates)\n\ndf &lt;- read_delim(\"data/swqmispublicdata.txt\",\n                 delim = \"|\",\n                 # see awkward column names that have to be wrapped\n                 # in between \"`\" (escape) marks.\n                 col_types = cols_only(\n                   `Segment ID` = col_character(),\n                   `Station ID` = col_factor(),\n                   `Station Description` = col_character(),\n                   `End Date` = col_date(format = \"%m/%d/%Y\"),\n                   `Collecting Entity` = col_character(),\n                   `Monitoring Type` = col_character(),\n                   `Composite Category` = col_character(),\n                   `Parameter Name` = col_character(),\n                   `Parameter Code` = col_character(),\n                   `Value` = col_number(),\n                   `RFA/Tag ID` = col_character()\n                 )) |&gt; \n  clean_names() |&gt; \n  filter(station_id == \"12517\") |&gt; \n  filter(parameter_code == \"31699\")\n  \n\ndf |&gt; glimpse()\n\nRows: 88\nColumns: 11\n$ segment_id          &lt;chr&gt; \"1502\", \"1502\", \"1502\", \"1502\", \"1502\", \"1502\", \"1…\n$ station_id          &lt;fct&gt; 12517, 12517, 12517, 12517, 12517, 12517, 12517, 1…\n$ station_description &lt;chr&gt; \"TRES PALACIOS CREEK AT FM 456\", \"TRES PALACIOS CR…\n$ end_date            &lt;date&gt; 2001-03-14, 2000-12-20, 2001-06-21, 2002-01-10, 2…\n$ collecting_entity   &lt;chr&gt; \"TCEQ REGIONAL OFFICE\", \"TCEQ REGIONAL OFFICE\", \"T…\n$ monitoring_type     &lt;chr&gt; \"Routine - Monitoring not intentionally targeted t…\n$ composite_category  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ parameter_name      &lt;chr&gt; \"E. COLI, COLILERT, IDEXX METHOD, MPN/100ML\", \"E. …\n$ parameter_code      &lt;chr&gt; \"31699\", \"31699\", \"31699\", \"31699\", \"31699\", \"3169…\n$ value               &lt;dbl&gt; 10.0, 85.0, 183.0, 537.0, 52.0, 74.0, 24192.0, 169…\n$ rfa_tag_id          &lt;chr&gt; \"R194283\", \"R194025\", \"R195800\", \"R200079\", \"R2013…\n\n\n\n7.1.2 Hydrology\nStreamflow data is obtained from the USGS NWIS using the dataRetrieval package. It is important to note that streamflow data might not be static. USGS might update the data at some point due to new rating curves, data quality checks, etc. We want to work off a snapshot of the data.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn a project workflow, this would be in a data download script that saves your downloaded data to a csv. Then in your analysis script, read the csv data. You can rerun the analysis script without having to re-download the data and possibly running into changes in the source data. Frequent repeated downloads might also cause server issues or your IP getting temporarily blocked or rate-limited by the NWIS server.\n\n\n\n# This should be in a separate data download script!\nQ_df &lt;- readNWISdv(siteNumbers = \"08162600\",\n           startDate = \"2000-01-01\",\n           endDate = \"2020-12-31\",\n           parameterCd = \"00060\",\n           statCd = \"00003\")\nQ_df &lt;- renameNWISColumns(Q_df) |&gt; \n  clean_names()\n\n# save the data and work off the saved data\n# write_csv(Q_df, \"data/streamflow_0816200.csv\")\n# Q_df &lt;- read_csv(\"data/streamflow_0816200.csv\")\n\nQ_df |&gt; glimpse()\n\nRows: 7,671\nColumns: 5\n$ agency_cd &lt;chr&gt; \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USG…\n$ site_no   &lt;chr&gt; \"08162600\", \"08162600\", \"08162600\", \"08162600\", \"08162600\", …\n$ date      &lt;date&gt; 2000-01-01, 2000-01-02, 2000-01-03, 2000-01-04, 2000-01-05,…\n$ flow      &lt;dbl&gt; 0.84, 3.00, 3.40, 2.60, 1.60, 3.20, 11.00, 17.00, 22.00, 18.…\n$ flow_cd   &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", …",
    "crumbs": [
      "Water Quality",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Load Duration Curves</span>"
    ]
  },
  {
    "objectID": "load-duration.html#manual-method",
    "href": "load-duration.html#manual-method",
    "title": "7  Load Duration Curves",
    "section": "\n7.2 Manual Method",
    "text": "7.2 Manual Method\nFirst we will go through a manual method for developing the FDC and LDC. I go through this becuae it is worth understanding the specific steps for developing the FDC and LDC. the ldc is available to streamline these steps, but I highly reccomend understanding the fundamentals first.\n\n7.2.1 Flow Duration Curve\nThe primary step in calculating the FDC is calculating the exceedance probability for each streamflow value. Without delving into statistics, there are a surprising number of ways to accomplish this. If you have lots of streamflow values, the method you chose does not matter much (Vogel and Fennessey 1994). For manual calculations, we typically calculate the exceedance probability as the rank value of a given flow divided by the the number of streamflow values plus 1 (Morrison and Bonta 2008):\n\\[\np_i = \\frac{i}{n+1}\n\\] where \\(p_i\\) is the exceedance probability, \\(i\\) is the rank number of a given streamflow and \\(n\\) is the number of observations. \\(p_i\\) is also called the Weibull plotting position which is the mean of the cumulative distribution function of the \\(i\\)th observation (Gumbel 1958).\nThere are two methods for calculating this in dplyr. The first involves the direct calculation by ranking flows in descending order and dividing by length plus one. The second involves using the ppoints() function in R which returns ordered probability points for a given vector. by setting a=0 we specify that the function returns the Weibull plotting positions.\n\nQ_df &lt;- Q_df |&gt; \n  select(date, flow) |&gt; \n  arrange(flow) |&gt; \n  mutate(\n    ## direct calculation if you prefer\n    flow_exceedance = rank(desc(flow), ties.method = \"last\")/(length(flow)+1),\n    ## or weibull pp function, you don't need to do both!\n    flow_exceedance_1 = 1-ppoints(flow, a = 0))\n\nggplot(Q_df) +\n  geom_line(aes(flow_exceedance, flow)) +\n  scale_y_log10() +\n  theme_TWRI_print() +\n  labs(y = \"Mean Daily Flow [cfs]\", x = \"Proportion of Days Flow Exceeded\")\n\n\n\n\n\n\n\n\n\nThe resulting FDC shows us the percent of time over the entire period of record that mean daily streamflows were exceeded. For example the above figure shows the max streamflow was around 10,000 cfs and the minimum was less than 1. Also 80% of the time, streamflows exceeded about 10 cfs.\n\n7.2.2 Load Duration Curve\nNow the FDC can be converted to an LDC by multiplying the mean daily streamflow volume by the allowable bacteria concentration. The general steps in your head should be:\n\nconvert mean daily discharge (cfs) to daily volume for water (cubic feet, mL, whatever);\nmultiply the measured pollutant concentration by the daily volume\n\nThis results in total mass or counts of pollutant per day.\n\nQ_df &lt;- Q_df |&gt; \n  # We don't need both flow exceedance columns\n  select(-c(flow_exceedance_1)) |&gt; \n  # MPN/100mL * cubic feet/sec * mL/cubic feet * sec/day = mpn/day\n  mutate(ldc = (126/100) * flow * 28316.8 * 86400)\n\nggplot(Q_df) +\n  geom_line(aes(flow_exceedance, ldc)) +\n  scale_y_log10() +\n  theme_TWRI_print() +\n  labs(y = \"*E. coli* [MPN/day]\", x = \"Proportion of Days Load Exceeded\") +\n  theme(axis.title.y = element_markdown())\n\n\n\n\n\n\n\n\n\nThe LDC looks exactly the same as the FDC, the units on the y-axis change to pollutant load per day. The next step is to add the measured concentrations to the figure. We need to join the bacteria data to the flow data, then calculate the measured loads. In order to plot this and label the legends properly, we need to manually set some of the aesthtic values in ggplot2().\n\necoli_df &lt;- df |&gt; \n  select(station_id, end_date, parameter_code, value)\n\nQ_df &lt;- Q_df |&gt; \n  left_join(ecoli_df, by = c(\"date\" = \"end_date\")) |&gt; \n  mutate(measured_load = (value/100) * flow * 28316.8 * 86400)\n\nggplot(Q_df) +\n  geom_line(aes(flow_exceedance, ldc,\n                linetype = \"Allowable Load at Geomean Criterion (126 MPN/100 mL)\")) +\n  geom_point(aes(flow_exceedance, measured_load,\n                 shape = \"Measurement Value (MPN/day)\",\n                 color = \"Measurement Value (MPN/day)\")) +\n  scale_y_log10() +\n  scale_shape_manual(name = \"values\", values = c(21)) +\n  scale_color_manual(name = \"values\", values = c(\"dodgerblue4\")) +\n  theme_TWRI_print() +\n  labs(y = \"*E. coli* [MPN/day]\", x = \"Proportion of Days Load Exceeded\") +\n  theme(axis.title.y = element_markdown(),\n        legend.direction = \"vertical\",\n        legend.title = element_blank())\n\n\n\n\n\n\n\n\n\nWe can already see general trends in the LDC and bacteria data. There is clearly a higher variance and probably a higher difference in median measured load and allowable load at high flows. As flows decrease (on the right hand side of the graph), a higher proportion of measured loads appear to be below the allowable load line.\nThere are several ways to quantify this. The easiest to explain to the general audience is to (1) split the flows into different regimes, (2) take the geomean of the loads within the flow regime, and (3) take the difference between geomean measured load and the median allowable load. Alternatively, we could fit a log-regression, LOADEST or generalized additive model to the data to estimate load across all exceedance percentiles. The former approach is shown below:\n\n# create a summary table\nload_summary &lt;- Q_df |&gt; \n  # classify flow conditions based on exceedance\n  mutate(flow_condition = case_when(\n    flow_exceedance &gt;= 0 & flow_exceedance &lt; 0.1 ~ \"Highest Flows\",\n    flow_exceedance &gt;= 0.1 & flow_exceedance &lt; 0.4 ~ \"Moist Conditions\",\n    flow_exceedance &gt;= 0.4 & flow_exceedance &lt; 0.6 ~ \"Mid-range Conditions\",\n    flow_exceedance &gt;= 0.6 & flow_exceedance &lt; 0.9 ~ \"Dry Conditions\",\n    flow_exceedance &gt;= 0.9 & flow_exceedance &lt;= 1 ~ \"Lowest Flows\"\n  )) |&gt; \n  group_by(flow_condition) |&gt; \n  summarize(median_flow = quantile(flow, 0.5, type = 5, \n                                   names = FALSE, na.rm = TRUE),\n            median_p = round(quantile(flow_exceedance, \n                                      .5, type = 5, names = FALSE, \n                                      na.rm = TRUE), 2),\n            geomean_ecoli = Gmean(value, na.rm = TRUE),\n            allowable_load = median_flow * 126/100 * 28316.8 * 86400,\n            geomean_load = median_flow * geomean_ecoli/100 * 28316.8 * 86400,\n            reduction_needed = case_when(\n              allowable_load &lt; geomean_load ~ geomean_load - allowable_load,\n              allowable_load &gt;= geomean_load ~ 0),\n            percent_reduction_needed = reduction_needed/geomean_load *100) |&gt; \n  arrange(median_p) |&gt; \n  mutate(flow_condition = as_factor(flow_condition))\n  \n\nload_summary\n\n# A tibble: 5 × 8\n  flow_condition  median_flow median_p geomean_ecoli allowable_load geomean_load\n  &lt;fct&gt;                 &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n1 Highest Flows         541       0.05         459.         1.67e12      6.07e12\n2 Moist Conditio…        40.7     0.25         205.         1.25e11      2.04e11\n3 Mid-range Cond…        19.7     0.5           95.8        6.07e10      4.62e10\n4 Dry Conditions         12.6     0.75          68.3        3.88e10      2.10e10\n5 Lowest Flows            6.9     0.95         103.         2.13e10      1.74e10\n# ℹ 2 more variables: reduction_needed &lt;dbl&gt;, percent_reduction_needed &lt;dbl&gt;\n\n\nNow we have some summary data that includes median values for each flow regime and estimates of required reductions. The next step is to add some info to the ggplot.\n\n# set the y-axis value for the flow-regime labels\nlabel_max &lt;- max(Q_df$measured_load, na.rm = TRUE) + \n  (0.5 * max(Q_df$measured_load, na.rm = TRUE))\n\nggplot(Q_df) +\n  ## add some lines to indicate flow regimes\n  geom_vline(xintercept = c(.10, .40, .60, .90), color = \"#cccccc\") +\n  ## add ldc line\n  geom_line(aes(flow_exceedance, ldc,\n                linetype = \"Allowable Load at Geomean Criterion (126 MPN/100 mL)\")) +\n  ## add measured loads\n  geom_point(aes(flow_exceedance, measured_load,\n                 shape = \"Measurement Value (MPN/day)\",\n                 color = \"Measurement Value (MPN/day)\")) +\n  ## add summarized measured loads\n  geom_point(data = load_summary, aes(median_p, geomean_load,\n                                      shape = \"Exisiting Geomean Load (MPN/day)\",\n                                      color = \"Exisiting Geomean Load (MPN/day)\")) +\n  ## log10 y-axis\n  scale_y_log10() +\n  ## shrink the ends of the x-axis a little bit\n  scale_x_continuous(expand = c(0.005,0.005)) +\n  ## manually set the shapes for the point aesthetics\n  scale_shape_manual(name = \"values\", values = c(12, 21)) +\n  ## manually set the shapes for the color aesthetic\n  scale_color_manual(name = \"values\", values = c(\"red\", \"dodgerblue4\")) +\n  ## I like this tick marks that indicate a log transformed scale\n  annotation_logticks(sides = \"l\", color = \"#cccccc\") +\n  ## add some labels to the flow-regimes\n  annotate(\"text\", x = .05, y = label_max, \n           label = \"High\\nflows\", hjust = 0.5, size = 3, \n           family = \"OpenSansCondensed_TWRI\", lineheight = 1) +\n  annotate(\"text\", x = .25, y = label_max, \n           label = \"Moist\\nconditions\", hjust = 0.5, size = 3, \n           family = \"OpenSansCondensed_TWRI\", lineheight = 1) +\n  annotate(\"text\", x = .50, y = label_max, \n           label = \"Mid-range\\nflows\", hjust = 0.5, size = 3, \n           family = \"OpenSansCondensed_TWRI\", lineheight = 1) +\n  annotate(\"text\", x = .75, y = label_max, \n           label = \"Dry\\nconditions\", hjust = 0.5, size = 3, \n           family = \"OpenSansCondensed_TWRI\", lineheight = 1) +\n  annotate(\"text\", x = .95, y = label_max, \n           label = \"Low\\nflows\", hjust = 0.5, size = 3, \n           family = \"OpenSansCondensed_TWRI\", lineheight = 1) +\n  ## labels\n  labs(y = \"*E. coli* [MPN/day]\", x = \"Proportion of Days Load Exceeded\") +\n  ## general theme\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown(),\n        legend.direction = \"vertical\",\n        legend.title = element_blank())",
    "crumbs": [
      "Water Quality",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Load Duration Curves</span>"
    ]
  },
  {
    "objectID": "load-duration.html#ldc-package",
    "href": "load-duration.html#ldc-package",
    "title": "7  Load Duration Curves",
    "section": "\n7.3 ldc Package",
    "text": "7.3 ldc Package\nSome of the steps described above cna be skipped by using the ldc package. The package includes functions for period of record LDCs and annualized LDCs which are not covered here. One important functionality introduced with the package is the use of units.\n\n# install.packages(\"ldc\", \n#                  repos = c(txwri = 'https://txwri.r-universe.dev', \n#                            CRAN = 'https://cloud.r-project.org'))\n\nlibrary(ldc)\nlibrary(units)\n\n## ldc uses the unit package to facilitate unit conversions\n## we need to make the cfu unit first, since it isn't included \n## in the units package\ninstall_unit(\"MPN\")\n\n\n## get a new clean dataframe with column of Date, flows, and E. coli\nQ_df &lt;- readNWISdv(siteNumbers = \"08162600\",\n           startDate = \"2000-01-01\",\n           endDate = \"2020-12-31\",\n           parameterCd = \"00060\",\n           statCd = \"00003\") |&gt; \n  renameNWISColumns(Q_df) |&gt; \n  clean_names() |&gt; \n  left_join(ecoli_df, by = c(\"date\" = \"end_date\")) |&gt; \n  ## attach a unit to streamflow\n  mutate(flow = set_units(flow, \"ft^3/s\"),\n         value = set_units(value, \"MPN/100mL\"))\n\nQ_df |&gt; glimpse()\n\nRows: 7,671\nColumns: 8\n$ agency_cd      &lt;chr&gt; \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\",…\n$ site_no        &lt;chr&gt; \"08162600\", \"08162600\", \"08162600\", \"08162600\", \"081626…\n$ date           &lt;date&gt; 2000-01-01, 2000-01-02, 2000-01-03, 2000-01-04, 2000-0…\n$ flow           [ft^3/s] 0.84 [ft^3/s], 3.00 [ft^3/s], 3.40 [ft^3/s], 2.60 [f…\n$ flow_cd        &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", …\n$ station_id     &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ parameter_code &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ value          [MPN/100mL] NA [MPN/100mL], NA [MPN/100mL], NA [MPN/100mL], N…\n\n\nWith the dataframe setup, the calc_ldc() function will generate a exceedance probabilities and flow regime categories:\n\n## specify the allowable concentration\nallowable_concentration &lt;- 126\n## set the units\nunits(allowable_concentration) &lt;- \"MPN/100mL\"\n\ndf_ldc &lt;- calc_ldc(Q_df, \n                   Q = flow, \n                   C = value, \n                   allowable_concentration = allowable_concentration,\n                   breaks = c(1, 0.9, 0.6, 0.4, 0.1, 0),\n                   labels = c(\"Highest Flows\", \n                              \"Moist Conditions\",\n                              \"Mid-range Flows\",\n                              \"Dry Conditions\",\n                              \"Low Flows\"))\ndf_ldc\n\n# A tibble: 7,671 × 13\n   agency_cd site_no  date          flow flow_cd station_id parameter_code value\n   &lt;chr&gt;     &lt;chr&gt;    &lt;date&gt;     [ft^3/… &lt;chr&gt;   &lt;fct&gt;      &lt;chr&gt;          [MPN…\n 1 USGS      08162600 2000-08-18    0.22 A       &lt;NA&gt;       &lt;NA&gt;              NA\n 2 USGS      08162600 2000-03-07    0.42 A       &lt;NA&gt;       &lt;NA&gt;              NA\n 3 USGS      08162600 2000-03-06    0.6  A       &lt;NA&gt;       &lt;NA&gt;              NA\n 4 USGS      08162600 2000-08-22    0.75 A       &lt;NA&gt;       &lt;NA&gt;              NA\n 5 USGS      08162600 2000-03-08    0.78 A       &lt;NA&gt;       &lt;NA&gt;              NA\n 6 USGS      08162600 2000-08-17    0.78 A       &lt;NA&gt;       &lt;NA&gt;              NA\n 7 USGS      08162600 2000-09-11    0.8  A       &lt;NA&gt;       &lt;NA&gt;              NA\n 8 USGS      08162600 2000-01-01    0.84 A       &lt;NA&gt;       &lt;NA&gt;              NA\n 9 USGS      08162600 2000-08-21    0.93 A       &lt;NA&gt;       &lt;NA&gt;              NA\n10 USGS      08162600 2000-03-05    1    A       &lt;NA&gt;       &lt;NA&gt;              NA\n# ℹ 7,661 more rows\n# ℹ 5 more variables: Daily_Flow_Volume [100mL/d], Daily_Load [MPN/d],\n#   Allowable_Daily_Load [MPN/d], P_Exceedance &lt;dbl&gt;, Flow_Category &lt;fct&gt;\n\n\nWith the LDC information calculated, the summary table can be generated with summ_ldc():\n\ndf_sum &lt;- summ_ldc(df_ldc, \n                   Q = flow, \n                   C = value, \n                   Exceedance = P_Exceedance,\n                   groups = Flow_Category,\n                   method = \"geomean\")\ndf_sum\n\n# A tibble: 5 × 6\n  Flow_Category    Median_Flow Median_P   Geomean_C Median_Daily_Flow_Volume\n  &lt;fct&gt;               [ft^3/s]    &lt;dbl&gt; [MPN/100mL]                [100mL/d]\n1 Highest Flows          541     0.0501       459.              13235973701.\n2 Moist Conditions        40.7   0.25         205.                995756247.\n3 Mid-range Flows         19.7   0.5           95.8               481975382.\n4 Dry Conditions          12.6   0.75          68.3               308268519.\n5 Low Flows                6.9   0.950        103.                168813713.\n# ℹ 1 more variable: Median_Flow_Load [MPN/d]\n\n\nWith the summary table we can finally plot the LDC:\n\ndraw_ldc(df_ldc,\n         df_sum,\n         label_nudge_y = log10(1000)) +\n  labs(y = \"*E. coli* [MPN/day]\") +\n  scale_y_log10() +\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown(),\n        legend.title = element_blank(),\n        legend.direction = \"vertical\")\n\n\n\n\n\n\n\n\n\nWe get the same outputs as the manual method with far fewer functions and less change for copy/paste errors. Another advantage of using ldc is the ability to change units on the fly without digging into a formula.\nFor example, the summary table shows median daily flow volume as 100ml/day. That isn’t an inuitive unit. Let’s report in million gallons per day instead by using the set_units() function:\n\ndf_sum &lt;- df_sum |&gt; \n  mutate(Median_Daily_Flow_Volume = set_units(Median_Daily_Flow_Volume,\n                                              \"1E6gallons/day\"))\n\ndf_sum\n\n# A tibble: 5 × 6\n  Flow_Category    Median_Flow Median_P   Geomean_C Median_Daily_Flow_Volume\n  &lt;fct&gt;               [ft^3/s]    &lt;dbl&gt; [MPN/100mL]           [1E6gallons/d]\n1 Highest Flows          541     0.0501       459.                    350.  \n2 Moist Conditions        40.7   0.25         205.                     26.3 \n3 Mid-range Flows         19.7   0.5           95.8                    12.7 \n4 Dry Conditions          12.6   0.75          68.3                     8.14\n5 Low Flows                6.9   0.950        103.                      4.46\n# ℹ 1 more variable: Median_Flow_Load [MPN/d]\n\n\nOften we report bacteria loads as million or billion counts per day:\n\ndf_sum &lt;- df_sum |&gt; \n  mutate(Median_Flow_Load = set_units(Median_Flow_Load,\n                                      \"1E9MPN/day\"))\n\ndf_sum\n\n# A tibble: 5 × 6\n  Flow_Category    Median_Flow Median_P   Geomean_C Median_Daily_Flow_Volume\n  &lt;fct&gt;               [ft^3/s]    &lt;dbl&gt; [MPN/100mL]           [1E6gallons/d]\n1 Highest Flows          541     0.0501       459.                    350.  \n2 Moist Conditions        40.7   0.25         205.                     26.3 \n3 Mid-range Flows         19.7   0.5           95.8                    12.7 \n4 Dry Conditions          12.6   0.75          68.3                     8.14\n5 Low Flows                6.9   0.950        103.                      4.46\n# ℹ 1 more variable: Median_Flow_Load [1E9MPN/d]\n\n\nUpdate units in df_ldc also:\n\ndf_ldc &lt;- df_ldc |&gt; \n  mutate(Daily_Load = set_units(Daily_Load, \n                                \"1E9MPN/day\"),\n         Allowable_Daily_Load = set_units(Allowable_Daily_Load, \n                                          \"1E9MPN/day\"))\ndf_ldc\n\n# A tibble: 7,671 × 13\n   agency_cd site_no  date          flow flow_cd station_id parameter_code value\n   &lt;chr&gt;     &lt;chr&gt;    &lt;date&gt;     [ft^3/… &lt;chr&gt;   &lt;fct&gt;      &lt;chr&gt;          [MPN…\n 1 USGS      08162600 2000-08-18    0.22 A       &lt;NA&gt;       &lt;NA&gt;              NA\n 2 USGS      08162600 2000-03-07    0.42 A       &lt;NA&gt;       &lt;NA&gt;              NA\n 3 USGS      08162600 2000-03-06    0.6  A       &lt;NA&gt;       &lt;NA&gt;              NA\n 4 USGS      08162600 2000-08-22    0.75 A       &lt;NA&gt;       &lt;NA&gt;              NA\n 5 USGS      08162600 2000-03-08    0.78 A       &lt;NA&gt;       &lt;NA&gt;              NA\n 6 USGS      08162600 2000-08-17    0.78 A       &lt;NA&gt;       &lt;NA&gt;              NA\n 7 USGS      08162600 2000-09-11    0.8  A       &lt;NA&gt;       &lt;NA&gt;              NA\n 8 USGS      08162600 2000-01-01    0.84 A       &lt;NA&gt;       &lt;NA&gt;              NA\n 9 USGS      08162600 2000-08-21    0.93 A       &lt;NA&gt;       &lt;NA&gt;              NA\n10 USGS      08162600 2000-03-05    1    A       &lt;NA&gt;       &lt;NA&gt;              NA\n# ℹ 7,661 more rows\n# ℹ 5 more variables: Daily_Flow_Volume [100mL/d], Daily_Load [1E9MPN/d],\n#   Allowable_Daily_Load [1E9MPN/d], P_Exceedance &lt;dbl&gt;, Flow_Category &lt;fct&gt;\n\n\nNow the updated units can be plotted on the LDC again:\n\ndraw_ldc(df_ldc,\n         df_sum,\n         label_nudge_y = log10(1000)) +\n  labs(y = \"*E. coli* [Billion MPN/day]\") +\n  ## make our labels more reader friendly\n  scale_y_log10(labels = scales::comma) +\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown(),\n        legend.title = element_blank(),\n        legend.direction = \"vertical\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nGumbel, E.J. 1958. Statistics of Extreme. New York: Columbia University Press.\n\n\nMorrison, M.A., and Bonta, J.V. 2008. Development of Duration-Curve Based Methods for Quantifying Variability and Change in Watershed Hydrology and Water Quality. EPA/600/R-08/065. U.S. Environmental Protection Agency. https://nepis.epa.gov/Exe/ZyPURL.cgi?Dockey=P1000VR4.txt.\n\n\nU.S. Environmental Protection Agency. 2007. An Approach for Using Load Duration Curves in the Development of TMDLs. EPA 841-B-07-006. U.S. Environmental Protection Agency. https://nepis.epa.gov/Exe/ZyPURL.cgi?Dockey=P1008ZQA.txt.\n\n\nVogel, R.M., and Fennessey, N.M. 1994. Flow‐Duration Curves. I: New Interpretation and Confidence Intervals. Journal of Water Resources Planning and Management 120 (4): 485–504. https://doi.org/10.1061/(ASCE)0733-9496(1994)120:4(485).",
    "crumbs": [
      "Water Quality",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Load Duration Curves</span>"
    ]
  },
  {
    "objectID": "water-quality-statistics.html#hypothesis-tests",
    "href": "water-quality-statistics.html#hypothesis-tests",
    "title": "8  Water Quality Statistics",
    "section": "\n8.1 Hypothesis Tests",
    "text": "8.1 Hypothesis Tests\n\n\n\nTable 8.1: Guide to classification of hypothesis tests. Adapted from Helsel et al. (2020).\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\nData\nParametric\nNonparametric\nPermutation\n\n\n\n\nTwo independent groups\n\n\nt-test\n\n\nRank-sum test\n\n\nTwo-sample permutation test\n\n\n\n\nMatched pairs\n\n\nPaired t-test\n\n\nSigned-rank test\n\n\nPaired permuatation test\n\n\n\n\nThree of more independent groups\n\n\nAnalysis of variance (ANOVA)\n\n\nKruskal-Walis test\n\n\nOne-way permutation test\n\n\n\n\nTwo-factor group comparisons\n\n\nTwo-factor ANOVA\n\n\nBrunner-Dette-Munk test\n\n\nTwo-factor permutation test\n\n\n\n\nCorrelation between two independent variables\n\n\nPearson’s r\n\n\nSpearman’s p or Kendall’s r\n\n\nPermutation test for Pearson’s r\n\n\n\n\n\n\n\n\n\n\n\n\n8.1.1 Compare two independent groups\nIn the following example, we will generate example data using random data drawn from the normal distribution using the rnorm() function. Figure 8.1 shows two samples with \\(n\\)=10, the first sample was drawn from a normal distribution with mean (\\(\\mu\\))=0.5 and standard deviation (\\(\\sigma\\))= 0.25. The second sample is drawn from a normal distribution with \\(\\mu\\)=1.0 and \\(\\sigma\\) = 0.5.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(twriTemplates)\n\n## sets seed for reproducible example with random data\nset.seed(1000)\n\n## sample size\nn = 10\n\n## generate example data\nexample_data &lt;- tibble(\n  sample_1 = rnorm(n = n, mean = 0.5, sd = 0.5),\n  sample_2 = rnorm(n = n, mean = 0.7, sd = 0.5)\n)\n\nexample_data |&gt; \n  pivot_longer(cols = everything(),\n               names_to = \"Sample\", values_to = \"Value\") |&gt; \n  ggplot() +\n  geom_boxplot(aes(x = Sample, y = Value)) +\n  geom_point(aes(x = Sample, y = Value), position = \"jitter\") +\n  theme_TWRI_print()\n\n\n\n\n\n\nFigure 8.1: Box-plot and values of randomly generated sample drawn from the normal distribution.\n\n\n\n\nA test for the difference in the means is conducted using the t.test() function to run the two-sample t-test:\n\nresults &lt;- t.test(example_data$sample_1, example_data$sample_2)\nresults\n\n\n    Welch Two Sample t-test\n\ndata:  example_data$sample_1 and example_data$sample_2\nt = -0.88698, df = 17.779, p-value = 0.3869\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.4946719  0.2011626\nsample estimates:\nmean of x mean of y \n0.3354548 0.4822094 \n\n\nFor the t-test, the null hypothesis (\\(H_0\\)) is that the difference in means is equal to zero, the alternative hypothesis (\\(H_1\\)) is that the difference in means is not equal to zero. By default t.test() prints some information about your test results, including the t-statistic, degrees of freedom for the t-statistic calculation, p-value, and confidence intervals.\n\n\n\n\n\n\nNote\n\n\n\nBy assigning the output of t.test() to the results object we can also obtaining these results individually, which can be handy for plotting or exporting results to other files. See the output of str(results) for a list of values you can reach.\n\n\nIn this example, we do not have the evidence to reject \\(H_0\\) at an \\(\\alpha\\) = 0.05 (t-stat = -0.887, \\(p\\) = 0.387).\nSince this example uses randomly drawn data, we can examine what happens when sample size is increased to \\(n\\) = 100:\n\n## sample size\nn = 100\n\n## generate example data\nexample_data &lt;- tibble(\n  sample_1 = rnorm(n = n, mean = 0.5, sd = 0.5),\n  sample_2 = rnorm(n = n, mean = 0.7, sd = 0.5)\n)\n\nt.test(example_data$sample_1, example_data$sample_2)\n\n\n    Welch Two Sample t-test\n\ndata:  example_data$sample_1 and example_data$sample_2\nt = -2.246, df = 195.46, p-value = 0.02583\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.28312141 -0.01837698\nsample estimates:\nmean of x mean of y \n0.5475622 0.6983114 \n\n\nNow we have evidence to reject \\(H_0\\) due to the larger sample size which increased the statistical power for detecting a smaller effect size at a cost of increasing the risk of detecting an effect that is not actually there or is not environmentally relevant and of course increased monitoring costs if this were an actual water quality monitoring project.\n\n\n\n\n\n\nNote\n\n\n\nHelsel et al. (2020) (Chapter 13) and Schramm (2021) have a important discussions on statistical power, sample sizes, and study designs.\n\n\nThe t-test assumes underlying data is normally distributed. However, hydrology and water quality data is often skewed and log-normally distributed. While, a simple log-transformation in the data can correct this, it is suggested to use a non-parametric or permutation test instead.\nThe Wilcoxon Rank Sum (also called Mann-Whitney) tests can be considered a non-parametric versions of the two-sample t-test. This example uses the bacteria data first shown in Chapter 6. Figure 8.2 shows the density plot of the bacteria values in the dataset. The heavily skewed data observed in fecal indicator bacteria are well suited for non-parametric statistical analysis.\n\nlibrary(readr)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\ndf &lt;- read_delim(\"data/swqmispublicdata.txt\",\n                 delim = \"|\",\n                 # see awkward column names that have to be wrapped\n                 # in between \"`\" (escape) marks.\n                 col_types = cols_only(\n                   `Segment ID` = col_character(),\n                   `Station ID` = col_factor(),\n                   `Station Description` = col_character(),\n                   `End Date` = col_date(format = \"%m/%d/%Y\"),\n                   `Collecting Entity` = col_character(),\n                   `Monitoring Type` = col_character(),\n                   `Composite Category` = col_character(),\n                   `Parameter Name` = col_character(),\n                   `Parameter Code` = col_character(),\n                   `Value` = col_number(),\n                   `RFA/Tag ID` = col_character()\n                 )) |&gt; \n  clean_names() |&gt; \n  filter(parameter_code == \"31699\") |&gt; \n  select(station_id, value)\n\nggplot(df) +\n  geom_density(aes(value, fill = station_id), alpha = 0.5) +\n  theme_TWRI_print()\n\n\n\n\n\n\nFigure 8.2: This density plot is essentially a smoothed histogram. The example bacteria dataset shows a heavy right skew and appears to be log-normally distributed. It should be assessed using non-parametric methods.\n\n\n\n\nThe Wilcoxon test is conducted using the wilcox.test() function. When your data is in “tidy” long format like above, you can use the formula notation in wilcox.test(), eg. y ~ x where y represents the response variable and x is the variable representing the factors you are comparing. You can also use use x and y arguments if your are comparing two numeric vectors. Both examples are shown below:\n\n## formula notation\nwilcox.test(value~station_id, data = df,\n            conf.int = TRUE)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  value by station_id\nW = 1041, p-value = 0.005188\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -169.99994  -26.00004\nsample estimates:\ndifference in location \n             -84.00001 \n\n## default notation\nx &lt;- df |&gt; \n  filter(station_id == \"12517\")\ny &lt;- df |&gt; \n  filter(station_id == \"15325\")\n\nwilcox.test(x$value, y$value,\n            conf.int = TRUE)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  x$value and y$value\nW = 1041, p-value = 0.005188\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -169.99994  -26.00004\nsample estimates:\ndifference in location \n             -84.00001 \n\n\nChapter 5.2 in Helsel et al. (2020) provide an excellent explanation of permutation tests. The permutation test works by resampling the data for all (or thousands of) possible permutations of group assignments. Assuming the null hypothesis is correct, it makes no difference which group any particular observation gets assigned to. The difference between groups and test statistics are calculated for each permutation. Then the proportion of permutation results that equal or exceed the difference calculated in the original data is the permutation p-value.\n\nlibrary(coin)\n\nLoading required package: survival\n\noneway_test(value~station_id, \n            data = df,\n            distribution = approximate())\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\n\ndata:  value by station_id (12517, 15325)\nZ = 0.57127, p-value = 0.6822\nalternative hypothesis: true mu is not equal to 0\n\n\nNotice that we end up with a different result from the Wilcoxon test. This is because the Wilcoxon test does not compare the means, but the ranked values (ie. does one group tend to have higher or lower ranked values than the other). The permutation test evaluates for differences in the mean of each group.\nWilcoxon tells us one site is likely to have a higher median, but the permutation test tells us that the means are approximately the same.\n\n8.1.2 Matched pairs\n\n8.1.3 Three or more groups\n\n8.1.4 Two-factor group comparisons\nWhen you have two-(non-nested)factors that may simultaneously influence observations, the factorial ANOVA and non-parametric alternatives can be used.\nIn the example below, we retrieve data for two water quality stations using the USGS dataRetrieval package. In 2011, an artifical wetland was completed to treat wastewater effluent discharged between stations 13079 (upstream side) and 13074 (downstream side). The first factor is station location, either upstream or downstream of the effluent discharge. We expect the upstream station to have “better” water quality than the downstream station. The second factor is before and after the wetland was completed. We expect the downstream station to have better water quality after the wetland than before, but no impact on the upstream water quality.\n\n## download the data\nlibrary(dataRetrieval)\ndf &lt;- readWQPdata(siteid=c(\"TCEQMAIN-13079\", \"TCEQMAIN-13074\"), \n                  characteristicName = \"Total suspended solids\",\n                  startDateLo = as.Date(\"1990-01-01\"),\n                  startDateHigh = as.Date(\"2023-07-30\"))\n\n## prepare the data for anlaysis\ndf &lt;- df |&gt;\n  clean_names() |&gt; \n  filter(activity_type_code != \"Sample-Field Split\") |&gt; \n  ## assign upstream and downstream variables\n  mutate(location = case_when(\n    monitoring_location_identifier  == \"TCEQMAIN-13074\" ~ \"Below\",\n    monitoring_location_identifier  == \"TCEQMAIN-13079\" ~ \"Above\",\n    .default = monitoring_location_identifier \n  )) |&gt; \n  ## assign pre- and post-wetland variable\n  mutate(wetland = case_when(\n    activity_start_date  &lt;= as.Date(\"2011-12-31\") ~ \"Pre\",\n    activity_start_date  &gt; as.Date(\"2011-12-31\") ~ \"Post\"\n  )) |&gt; \n  ## make wetland a factor so it orders correctly in the plots\n  ## default order is alphabetical, but it makes more sense to\n  ## specify \"Pre\" before \"Post\"\n  mutate(wetland = forcats::fct_relevel(wetland, \"Pre\", \"Post\")) |&gt; \n  select(monitoring_location_identifier, result_measure_value, location, activity_start_date, wetland)\n\nBefore we analyze the data, take a look at the boxplots of TSS values. The distributions suggest a log-normal distribution (the y-axis is log transformed). It appears that post-wetland TSS values were reduced downstream of the wetland but not upstream of the wetland.\n\nlibrary(ggbeeswarm)\nggplot(df, aes(x = location,\n               y = result_measure_value,\n               fill = wetland)) +\n  geom_boxplot(alpha = 0.5, outlier.shape = NA) +\n  geom_quasirandom(aes(color = wetland), dodge.width = 0.75, alpha = 0.5) +\n  scale_color_brewer(name = \"\",\n                     palette = \"Set2\") +\n  scale_fill_brewer(name = \"\",\n                    palette = \"Set2\") +\n  scale_y_log10(\"TSS (mg/L)\") +\n  theme_TWRI_print()\n\n\n\n\n\n\n\n\n\nThe aov() function fits the ANOVA model using the formula notation. The formula notation is of form response ~ factor where factor is a series of factors specified in the model. The specification factor1 + factor2 indicates all the factors are taken together, while factor1:factor2indicates the interactions. The notation factor1*factor2 is equivalent to factor1 + factor2 + factor1:factor2.\n\nm1 &lt;- aov(log(result_measure_value) ~ wetland * location,\n          data = df)\n\nsummary(m1)\n\n                  Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nwetland            1  0.155   0.155   1.048 0.308288    \nlocation           1 19.016  19.016 128.900  &lt; 2e-16 ***\nwetland:location   1  1.809   1.809  12.264 0.000684 ***\nResiduals        103 15.195   0.148                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere we fit the ANOVA to log-transformed TSS values. The results indicate a difference in geometric means (because we used the log values in the ANOVA) between upstream and downstream location and a difference in the interaction terms.\nWe follow up the ANOVA with a multiple comparisons test (Tukey’s Honest Significant Difference, or Tukey’s HSD) on the factor(s) of interest.\n\nTukeyHSD(m1, \"wetland:location\")\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = log(result_measure_value) ~ wetland * location, data = df)\n\n$`wetland:location`\n                             diff        lwr        upr     p adj\nPost:Above-Pre:Above   0.08065095 -0.3558692  0.5171711 0.9628194\nPre:Below-Pre:Above   -0.34843318 -0.8062593  0.1093930 0.1994032\nPost:Below-Pre:Above  -0.95543536 -1.4006003 -0.5102705 0.0000010\nPre:Below-Post:Above  -0.42908413 -0.6836169 -0.1745514 0.0001524\nPost:Below-Post:Above -1.03608631 -1.2670710 -0.8051016 0.0000000\nPost:Below-Pre:Below  -0.60700218 -0.8760912 -0.3379131 0.0000003\n\n\nThe TukeyHSD() function takes the output from aov() and optionally the factor you are interested in evaluating the difference in means. The output provide the estimate difference in means between each level of the factor, the 95% confidence interval and the multiple comparisons adjusted p-value. Figure 8.3 is an example of how the data can be plotted for easier interpretation.\n\n\n\n\n\n\n\nFigure 8.3: The 95% confidence intervals on differences in means (log scale) for different factor interactions.\n\n\n\n\nThe non-parametric version of the ANOVA model is the two-factor Brunner-Dette-Munk (BDM) test. The BDM test is implemented in the asbio package using the BDM.2way() function.\n\nlibrary(asbio)\n\nLoading required package: tcltk\n\n\nRegistered S3 method overwritten by 'asbio':\n  method   from\n  print.ci coin\n\nbdm_output &lt;- BDM.2way(Y = df$result_measure_value, \n                       X1 = as.factor(df$location), \n                       X2 = as.factor(df$wetland))\n\nbdm_output$BDM.Table\n\n      df1     df2        F*    P(F &gt; F*)\nX1      1 7.45395 31.373306 0.0006543175\nX2      1 7.45395  4.020217 0.0825010167\nX1:X2   1 7.45395  6.504502 0.0361551078\n\nbdm_output$Q\n\n      Levels Rel.effects\n1  Above.Pre   0.6806854\n2 Above.Post   0.7158029\n3  Below.Pre   0.4842290\n4 Below.Post   0.1908808\n\n\nPost-hoc isn’t straightforward depending on what factors you want to compare. Recommend using GLM here or permutation test here.\n\nperm.fact.test(Y = df$result_measure_value, \n               X1 = as.factor(df$location), \n               X2 = as.factor(df$wetland),\n               perm = 5000)\n\n$Table\n         Initial.F  Df   pval\nX1       43.727189   1 0.0002\nX2        6.026275   1 0.0170\nX1:X2     3.383129   1 0.0684\nResidual        NA 103     NA\n\n\n\n8.1.5 Correlation between two independent variables\nUsing the estuary water quality example data from #sec-plotclean we will explore correlations between two independent variables:\n\nfiles &lt;- c(\"data/marabwq2020.csv\", \"data/marabwq2021.csv\", \"data/marcewq2020.csv\", \"data/marcewq2021.csv\")\n\ndf &lt;- read_csv(file = files, \n               col_types = cols_only(\n                 StationCode = col_factor(),\n                 DateTimeStamp = col_datetime(format = \"%m/%e/%Y %H:%M\"),\n                 Temp = col_number(),\n                 F_Temp = col_character(),\n                 SpCond = col_number(),\n                 F_SpCond = col_character(),\n                 Sal = col_number(),\n                 F_Sal = col_character(),\n                 DO_mgl = col_number(),\n                 F_DO_mgl = col_character()\n               )) |&gt; \n  filter(F_Temp == \"&lt;0&gt;\",\n         F_DO_mgl == \"&lt;0&gt;\")\n\nNew names:\n• `` -&gt; `...31`\n\n\n\nggplot(df) +\n  geom_point(aes(Temp, DO_mgl), alpha = 0.1) +\n  theme_TWRI_print() +\n  labs(x = \"Temperature °C\", y = \"Dissolved Oxygen (mg/L)\")\n\n\n\n\n\n\n\n\n\nA quick glance at the data Figure 8.4 indicates a probable relationship between dissolved oxygen and temperature. Pearson’s r is the linear correlation coefficient that measures the linear association between two variables. Values of r range from -1 to 1 (indicate perfectly positive or negative linear relationships). Use the cor.test() function to return Pearson’s r and associated p-value:\n\necoli_df &lt;- read_delim(\"data/swqmispublicdata.txt\",\n                 delim = \"|\",\n                 # see awkward column names that have to be wrapped\n                 # in between \"`\" (escape) marks.\n                 col_types = cols_only(\n                   `Segment ID` = col_character(),\n                   `Station ID` = col_factor(),\n                   `Station Description` = col_character(),\n                   `End Date` = col_date(format = \"%m/%d/%Y\"),\n                   `Collecting Entity` = col_character(),\n                   `Monitoring Type` = col_character(),\n                   `Composite Category` = col_character(),\n                   `Parameter Name` = col_character(),\n                   `Parameter Code` = col_character(),\n                   `Value` = col_number(),\n                   `RFA/Tag ID` = col_character()\n                 )) |&gt; \n  clean_names() |&gt; \n  filter(station_id == \"12517\") |&gt; \n  filter(parameter_code == \"31699\") |&gt; \n  select(end_date, value)\n\ndf &lt;- readNWISdv(siteNumbers = \"08162600\",\n           startDate = \"2000-01-01\",\n           endDate = \"2020-12-31\",\n           parameterCd = \"00060\",\n           statCd = \"00003\") |&gt; \n  renameNWISColumns() |&gt; \n  clean_names() |&gt; \n  left_join(ecoli_df, by = c(\"date\" = \"end_date\")) |&gt; \n  filter(!is.na(value))\n\nggplot(df, aes(flow, value)) +\n  geom_point() + \n  scale_y_log10() + scale_x_log10() +\n  labs(x = \"Streamflow (cfs)\", y = \"E. coli (MPN/100mL)\") +\n  theme_TWRI_print()\n\n\n\n\n\n\n\n\n\n\n## create a permutation function for 2 sided pearson's r\n\npermutate_cor &lt;- function(x, y, n = 1000) {\n  data.name &lt;- paste(deparse(substitute(x)),\" and \",deparse(substitute(y)),\"\\n\",n,\" permutations\",sep=\"\")\n  cor_test_output &lt;- cor.test(x = x, y = y, method = \"pearson\")\n  obs_r &lt;- cor_test_output$estimate\n  obs_stat &lt;- cor_test_output$statistic\n  \n  cor_permutations &lt;- sapply(1:n,\n                             FUN = function(i) cor(x = x, y = sample(y)))\n  \n  cor_permutations &lt;- c(cor_permutations, obs_r)\n  \n  \n  p_val &lt;- sum(abs(cor_permutations) &gt;= abs(obs_r))/(n+1)\n  \n  result &lt;- list(statistic = obs_stat, \n                 data.name = data.name, \n                 estimate = obs_r,\n                 p.value = p_val,\n                 method = \"Pearson's product-moment correlation - Permutation test\",\n                 data = data.frame(r = unname(cor_permutations)))\n  class(result) &lt;- \"htest\"\n  return(result)\n}\n\n\ntempt &lt;- permutate_cor(log(df$flow), log(df$value), n = 500)\ntempt\n\n\n    Pearson's product-moment correlation - Permutation test\n\ndata:  log(df$flow) and log(df$value)\n500 permutations\nt = 4.5955, p-value = 0.001996\nsample estimates:\n      cor \n0.4788015 \n\nggplot(tempt$data) +\n  geom_histogram(aes(r), bins = 50) +\n  geom_vline(xintercept = tempt$estimate)",
    "crumbs": [
      "Water Quality",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Water Quality Statistics</span>"
    ]
  },
  {
    "objectID": "water-quality-statistics.html#regression",
    "href": "water-quality-statistics.html#regression",
    "title": "8  Water Quality Statistics",
    "section": "\n8.2 Regression",
    "text": "8.2 Regression",
    "crumbs": [
      "Water Quality",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Water Quality Statistics</span>"
    ]
  },
  {
    "objectID": "water-quality-statistics.html#trend-analysis",
    "href": "water-quality-statistics.html#trend-analysis",
    "title": "8  Water Quality Statistics",
    "section": "\n8.3 Trend Analysis",
    "text": "8.3 Trend Analysis\nMann-Kendall test Linear regression on numeric dates Flow adjustments\n\n\n\n\nHelsel, D.R., Hirsch, R.M., Ryberg, K.R., Archfield, S.A., and Gilroy, E.J. 2020. Statistical Methods in Water Resources: U.S. Geological Survey Techniques and Methods, Book 4, Chapter A3. Reston, VA: USGS. https://doi.org/10.3133/tm4a3.\n\n\nSchramm, M.P. 2021. Estimating statistical power for detecting long term trends in surface water Escherichia coli concentrations. Texas Water Journal 12 (1): 140–50. https://doi.org/10.21423/txj.v12i1.7126.",
    "crumbs": [
      "Water Quality",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Water Quality Statistics</span>"
    ]
  },
  {
    "objectID": "maps.html",
    "href": "maps.html",
    "title": "9  Spatial Data",
    "section": "",
    "text": "Incomplete\n\n\n\nThis has not been written yet.\n\n\n\n\n\n\n\n\nNote\n\n\n\nSome of our contracted projects require ESRI ArcMap, ArcPro formatted deliverable. It is generally recommended to do your spatial processing and save projects in ESRI formats to deliver projects as ESRI map documents.",
    "crumbs": [
      "Geospatial",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Spatial Data</span>"
    ]
  },
  {
    "objectID": "watershed-delineation.html",
    "href": "watershed-delineation.html",
    "title": "10  Delineate Watersheds",
    "section": "",
    "text": "Incomplete\n\n\n\nThis has not been written yet.",
    "crumbs": [
      "Geospatial",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Delineate Watersheds</span>"
    ]
  },
  {
    "objectID": "raster-summary.html",
    "href": "raster-summary.html",
    "title": "11  Extract Raster Summaries",
    "section": "",
    "text": "Incomplete\n\n\n\nThis has not been written yet.",
    "crumbs": [
      "Geospatial",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Extract Raster Summaries</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Gumbel, E.J. 1958. Statistics of Extreme. New York: Columbia University\nPress.\n\n\nHelsel, D.R., Hirsch, R.M., Ryberg, K.R., Archfield, S.A., and Gilroy,\nE.J. 2020. Statistical Methods in Water Resources: U.S. Geological\nSurvey Techniques and Methods, Book 4, Chapter A3. Reston, VA: USGS. https://doi.org/10.3133/tm4a3.\n\n\nMorrison, M.A., and Bonta, J.V. 2008. Development of Duration-Curve\nBased Methods for Quantifying Variability and Change in Watershed\nHydrology and Water Quality. EPA/600/R-08/065. U.S. Environmental\nProtection Agency. https://nepis.epa.gov/Exe/ZyPURL.cgi?Dockey=P1000VR4.txt.\n\n\nNEON (National Ecological Observatory Network). n.d. Discharge Field\nCollection (DP1.20048.001), RELEASE-2022. https://doi.org/10.48443/eaak-tt31.\n\n\nNOAA National Estuarine Research Reserve System (NERRS). 2022.\nSystem-Wide Monitoring Program. NOAA NERRS Centralized Data Management\nOffice. http://cdmo.baruch.sc.edu/.\n\n\nSchramm, M.P. 2021. Estimating statistical power for detecting long term\ntrends in surface water Escherichia coli concentrations. Texas\nWater Journal 12 (1): 140–50. https://doi.org/10.21423/txj.v12i1.7126.\n\n\nSmith, E.P., Ye, K., Hughes, C., and Shabman, L. 2001. Statistical\nAssessment of Violations of Water Quality Standards under Section 303(d)\nof the Clean Water Act. Environmental Science & Technology 35 (3):\n606–12. https://doi.org/10.1021/es001159e.\n\n\nU.S. Environmental Protection Agency. 2007. An Approach for Using Load\nDuration Curves in the Development of TMDLs. EPA 841-B-07-006. U.S.\nEnvironmental Protection Agency. https://nepis.epa.gov/Exe/ZyPURL.cgi?Dockey=P1008ZQA.txt.\n\n\nVogel, R.M., and Fennessey, N.M. 1994. Flow‐Duration Curves. I: New\nInterpretation and Confidence Intervals. Journal of Water Resources\nPlanning and Management 120 (4): 485–504. https://doi.org/10.1061/(ASCE)0733-9496(1994)120:4(485).",
    "crumbs": [
      "References"
    ]
  }
]