[
  {
    "objectID": "index.html#requirements",
    "href": "index.html#requirements",
    "title": "twri r-manual",
    "section": "Requirements",
    "text": "Requirements\n\nR version 4.2 or higher. This can be installed from https://cran.r-project.org/ or using the Ivanti Installation Manager if you are on an AgriLife computer.\nRStudio Desktop is the integrated development environment used to write and execute R code. Available for free from RStudio or Posit. Admin privileges are required to install, so install from Ivanti on AgriLife computers.\nThe manual assumes some basic understanding of using R and RStudio, reach out to colleagues if you are having difficulty.\nMost of the functions and code make heavy use of the tidyverse ecosystem of packages and functions. In particular tibble, dplyr, ggplot2. You are encouraged to become familiar with these particular packages. If you are just starting out with R, the R for Data Science book (free!) is a recommended reading.\nProject oriented workflows are an absolute requirement. Create projects in RStudio that align with each project you work on. Store your source and raw data within the project. Export your processed data and figures within the project. The complexity of the project will vary according to the project needs and the analysts comfort in various R packages."
  },
  {
    "objectID": "intro.html#project-oriented-workflow",
    "href": "intro.html#project-oriented-workflow",
    "title": "1  Fundamentals",
    "section": "\n1.1 Project-oriented workflow",
    "text": "1.1 Project-oriented workflow\n\n\n\n\n\n\nNote\n\n\n\nThis subsection is largely based on Jenny Bryan’s What They Forgot (WTF) to Teach You About R.\n\n\n1 - Organize your work into a project. This means within the file system store all your data, code, figures, notes, and related material within the same folder and subfolders.\n2 - RStudio Projects enforces this approach. When you create a new project in RStudio, it creates a folder with some metadata and user options for that specific project (stored in the .Rproj file inside the folder it created for the project).\n3 - RStudio Projects establish a working directory and use relative file paths by default. Usually this is what you want so when you share a project or move it from one computer to the next, it. just. works. This is also why it is critical to store your data and scripts within the project.\nA typical project might have a file and folder structure like this:\n\n\n\n Project Folder\n|\n|-- Data\n|   |\n|   |-- Raw Data\n|   |-- More Raw Data\n|\n|-- Scripts\n|   |\n|   |-- Analysis.R file\n|   |-- Figures.R file\n|\n|-- Figures\n|   |\n|   |-- Plot\n|   |-- Another figure\n|\n|-- Exported Data\n|   |\n|   |-- Results\n|\n|--- Reports\n|   |\n|   |-- Report\n|\n|- Readme file (usually .Rmd, .md, or .txt file)\n\n\n\n\n\n1.1.1 Your RStudio Project\nStart a new project! Open RStudio, in the upper left click “File” -> “New Project.” We generally want to start a project in a New Directory, so click that. One the next window click New Project. Now you can choose the subdirectory name of your project (folder name) followed by where you want that subdirectory to be stored. Click “Create Project” and RStudio create the subdirectory and puts a .Rproj file with specific project info in there for you.\n\n\nExample RStudio workspace\n\n\nThe RStudio workspace includes 3 major components. In the upper left, the script area shows the content of open R scripts (or any text based file that you open will show up here). You can edit, save, and run lines of code from this window.\nAt the bottom left, is the R console. This is where R operates. The code you wrote in the script gets loaded into the console and R does whatever is in the script. Output, messages, and warnings from your R code will probably show up here.\nAt the bottom right, are a couple of tabs. This is where graphical outputs are displayed. There are also tabs for files, packages, and help. The file tab lets you navigate, create, delete, and open files and folders. It defaults to your projects working directory. The packages tab is for exploring the packages you have installed, more on that below. Technically you can load and unload packages from here by clicking boxes next to each package. Don’t do that. The help tab is just that, it lets you search functions in each package and displays the documentation for packages and functions. Learn to use this tab, it will help you just like it says!\n\n1.1.2 Running Code\nYou should generally write your code in the script window and execute it from there. This will save you from retyping code again and again.\nIf you have your cursor on an expression in your R script, use the keyboard shortcut: Ctrl+Enter to execute that expression. The cursor will automatically move to the next statement and the code will run in the console. If you want to execute the entire script at once, use the keyboard shortcut: Ctrl+Shift+S.\n\n1.1.3 Basic coding\nBoxes with the grey background and blue vertical bar indicate chunks of R code. If there is an output when that code chunk is run by R, the output (text output, tables or figures) will follow directly below the chunk. For example, here is a code chunk:\n\n10*100\n\nAnd this is the output:\n\n\n[1] 1000\n\n\n\n\n\n\n\n\nNote\n\n\n\nMuch of this subsection is from R for Data Science which you are encouraged to explore.\n\n\nThis: <-, is called an assignment operator in R. We use it to create objects by assigning a value to a variable name. We name objects so we can easily refer to whatever you assigned later on in your script:\n\nx <- 10\ny <- 100\n\nx * y\n\n[1] 1000\n\n\nYou don’t have to assign numbers:\n\nx <- \"Hello\"\n\nprint(x)\n\n[1] \"Hello\"\n\n\nAssignment operators go either direction, you might find it useful to use the left to right assinment operator in some situations:\n\n\"Hello\" -> x\n\nprint(x)\n\n[1] \"Hello\"\n\n\nHowever, for the most part, standard practice is to assign right to left so you can easily find the variable name receiving the value. Whatever you choose, use the same direction throughout your project.\nAs your scripts get more complicated, it is important to use descriptive object names. Object names can only contain letters, numbers, _, and ., so we recommend using “snake_case” to create object names:\nstreamflow\nstreamflow_site_a\nObject names are case sensitive, streamflow_site_a is not the same as streamflow_site_A.\nThe # symbol is ignored by R and used to include human readable comments in your script. Use comments liberally in your code.\n\n## I can write what I want\n## and R does not evaluate\n## this\na <- 1\na"
  },
  {
    "objectID": "intro.html#functions",
    "href": "intro.html#functions",
    "title": "1  Fundamentals",
    "section": "\n1.2 Functions",
    "text": "1.2 Functions\nFunctions are essentially tools that take input arguments and output some kind of value. Functions are the basis for most everything you do in R. For example, seq() is a function to generate a regular sequence of numbers. You can get to the help documentation by entering ?seq() in the console. It takes the arguments from, to, by, length.out, along.with. Use = for argument values:\n\nseq(from = 0, to = 10, by = 2)\n\n[1]  0  2  4  6  8 10\n\n\nWriting your own functions is one of the reasons for using R. Here is a simplistic function that generates a message in the console screen depending on the condition of the first argument.\n\nprint_hello <- function(x) {\n  if (x < 1) message(\"Hello!\")\n  else message(\"Bye\")\n}\n\nprint_hello(x = -1)\n\nHello!\n\nprint_hello(x = 1)\n\nBye\n\n\nWhy write a function in the first place? Sometimes you might need to repeatedly run the same set of functions on different data or subsets of data. You will find yourself copy and pasting code and changing some values within. If the output is dependent on some values you forgot to change when you cut and paste, instant problems! Functions let you skip that copy and paste action, and just update the arguments. Here is an example of some code to calculate the confidence interval around the mean for a vector of numbers:\n\nmin <- 0\nmax <- 10\nn <- 1000\nci <- 0.95\nx <- runif(n = n, min = min, max = max)\nse <- sd(x)/sqrt(length(x))\nalpha <- 1 - ci\nmean(x) + se * qnorm(c(alpha/2, 1-alpha/2))\n\n[1] 4.650073 5.007223\n\n\nIf we need to recalculate the confidence interval for different values or combinations of values of x, n, and ci we would have to cut and paste the chunk each time with the potential for data entry errors if the wrong values are entered. Instead, create a function and change the arguments as needed.\n\nci <- function(min, max, n, ci) {\n  x <- runif(n = n, min = min, max = max)\n  se <- sd(x)/sqrt(length(x))\n  alpha <- 1 - ci\n  mean(x) + se * qnorm(c(alpha/2, 1-alpha/2))\n}\n\nci(min = 0, max = 10, n = 1000, ci = 0.95)\n\n[1] 4.794984 5.140893\n\nci(min = 10, max = 100, n = 1000, ci = 0.90)\n\n[1] 54.11848 56.78024\n\nci(min = 10, max = 1000, n = 1000, ci = 0.80)\n\n[1] 488.4017 511.2086"
  },
  {
    "objectID": "intro.html#packages",
    "href": "intro.html#packages",
    "title": "1  Fundamentals",
    "section": "\n1.3 Packages",
    "text": "1.3 Packages\nPackages might be considered the toolboxes of R. They are generally a collection of functions and classes the expand the capabilities of the base R functions. Many packages have dependencies from other packages. This mean when you install one package, you may end up installing multiple other packages automatically that are required for the package that you chose to work. Normally this works without hiccup. However, before installing packages, I suggest restarting your R session and make sure no packages are currently loaded to prevent issues.\nMost packages can and should be installed from the CRAN repository. These are a network of repositories that host the official, up-to-date and approved packages for R. This packages are pre-built, meaning you are unlikely to run into issues on installation. To install packages from CRAN, you typically do something like the following:\n\n## install one package\ninstall.packages(\"ggplot2\")\n\n## install multiple packages\ninstall.packages(\"dplyr\", \"tidyr\")\n\nSometimes you need a package or package version that is not currently available on CRAN. There are various justifiable reasons the packages might not be available on CRAN; however, one of the benefits of using CRAN packages is that they are all reviewed by a person before acceptance. This provides a safety mechanism for not only the quality of the package but potential security issues.\n\n\n\n\n\n\nNote\n\n\n\nIf you are installing a package from GitHub or other source, please review it for safety and quality before installation.\n\n\nThere are two primary way to install non-CRAN packages. The preferred method is to install pre-built packages from an alternative repository like r-universe. The readme file associated with the package will generally inform you if the package is available on a different repository and how to install it from that repository.\nAn example of this is shown below for the adc package:\n\ninstall.packages('adc', repos = c(txwri = 'https://txwri.r-universe.dev'))\n\nAn alternative option is to download and build the packages from the source, such as GitHub. For those on Windows, you will need to install the RTools toolchain. Then, we can use the remotes package to download, build and install a package from GitHub:\n\ninstall.packages(\"remotes\")\nremotes::install_github(\"mps9506/rATTAINS\")\n\nAfter you install a package, you need to load the package in order to use the functions. Confusingly, you use the library() function to accomplish this. Standard practice is to load libraries at the top of your script:\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)"
  },
  {
    "objectID": "intro.html#other-coding-conventions",
    "href": "intro.html#other-coding-conventions",
    "title": "1  Fundamentals",
    "section": "\n1.4 Other Coding Conventions",
    "text": "1.4 Other Coding Conventions\n\n\n\n\n\n\nIncomplete\n\n\n\nI still need to add discussion on strings, date formats, and the pipe function."
  },
  {
    "objectID": "intro.html#suggested-rstudio-settings",
    "href": "intro.html#suggested-rstudio-settings",
    "title": "1  Fundamentals",
    "section": "\n1.5 Suggested RStudio Settings",
    "text": "1.5 Suggested RStudio Settings\n\n\n\n\n\n\nIncomplete\n\n\n\nI still need to add discussion on\n\nsetting global and project options in RStudio\nusing ragg graphics device (this might be better in figure section)"
  },
  {
    "objectID": "data_exploration.html#import-data",
    "href": "data_exploration.html#import-data",
    "title": "2  Data Exploration",
    "section": "\n2.1 Import Data",
    "text": "2.1 Import Data\nOne of the first steps in any data analysis project is importing or reading data into R. For now, we will focus on reading in data from comma separated value (CSV), tab seperated value (TSV), and similar delimited text files. For most projects, it is useful to work off a singular data snapshot. So once you download your data and have it in your project, do not manipulate the data file1.1 Later on we will discuss loading web data sources directly into R, but for most of our work your best bet is to store raw data files inside the project\n\n\n\n\n\n\nDownload Data\n\n\n\nIf you want to work along, download the following example data export it directly into the data folder in your R project: Data\n\n\nUse the read_csv() function from the readr package to import the data and assign it to a variable. The console will print some information telling you what type of variable it made each column of the csv and if there are any problems.\n\ndf <- read_csv(file = \"data/easterwood.csv\")\n\nRows: 4045 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): name\ndbl  (3): station, dailymaximumdrybulbtemperature, dailyprecipitation\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nread_csv() guesses the column types and sometimes it can guess wrong, so the safest option is to tell it what to expect using the col_types argument. This argument takes a string where each character represents one column. Here we will tell it to expect a character, Date, character, number, number which is represented as cDcnn.\n\ndf <- read_csv(file = \"data/easterwood.csv\",\n               col_types = \"cDcnn\")\ndf\n\n# A tibble: 4,045 × 5\n   station     date       name                 dailymaximumdry… dailyprecipitat…\n   <chr>       <date>     <chr>                           <dbl>            <dbl>\n 1 74746003904 2010-07-31 COLLEGE STATION EAS…               99                0\n 2 74746003904 2010-08-01 COLLEGE STATION EAS…              102                0\n 3 74746003904 2010-08-02 COLLEGE STATION EAS…              101                0\n 4 74746003904 2010-08-03 COLLEGE STATION EAS…              100                0\n 5 74746003904 2010-08-04 COLLEGE STATION EAS…               99                0\n 6 74746003904 2010-08-05 COLLEGE STATION EAS…               99                0\n 7 74746003904 2010-08-06 COLLEGE STATION EAS…               99                0\n 8 74746003904 2010-08-07 COLLEGE STATION EAS…               99                0\n 9 74746003904 2010-08-08 COLLEGE STATION EAS…               99                0\n10 74746003904 2010-08-09 COLLEGE STATION EAS…              100                0\n# … with 4,035 more rows\n\n\nTSV and other delimited files are read in the same way but with read_delim() or read_tsv(). If your data is in .xlsx format, the readxl pacakge is required. In readxl there is a read_xlsx() function that works similar to the read_csv() function except you can specify the sheet and range of cells to read from. The col_types argument also needs to be spelled out as a character vector such as col_types = c(\"text\", \"date\", \"text\", \"numeric\", \"numeric\").\nIf you have multiple files with the same column names you can read them into the same data frame. Here we read in multiple files and are more explicit about defining the column types:\n\nfiles <- c(\"data/marabwq2020.csv\", \"data/marabwq2021.csv\", \"data/marcewq2020.csv\", \"data/marcewq2021.csv\")\ndf <- read_csv(file = files, \n               col_types = cols_only(\n                 StationCode = col_factor(),\n                 DateTimeStamp = col_datetime(format = \"%m/%e/%Y %H:%M\"),\n                 Temp = col_number(),\n                 F_Temp = col_character(),\n                 SpCond = col_number(),\n                 F_SpCond = col_character(),\n                 Sal = col_number(),\n                 F_Sal = col_character(),\n                 DO_mgl = col_number(),\n                 F_DO_mgl = col_character()\n               ))\ndf\n\n# A tibble: 140,352 × 10\n   StationCode DateTimeStamp        Temp F_Temp SpCond F_SpCond   Sal F_Sal\n   <fct>       <dttm>              <dbl> <chr>   <dbl> <chr>    <dbl> <chr>\n 1 marabwq     2020-01-01 00:00:00  15.5 <0>      38.2 <0>       24.3 <0>  \n 2 marabwq     2020-01-01 00:15:00  15.5 <0>      38.2 <0>       24.3 <0>  \n 3 marabwq     2020-01-01 00:30:00  15.5 <0>      38.2 <0>       24.3 <0>  \n 4 marabwq     2020-01-01 00:45:00  15.5 <0>      38.2 <0>       24.3 <0>  \n 5 marabwq     2020-01-01 01:00:00  15.5 <0>      38.2 <0>       24.3 <0>  \n 6 marabwq     2020-01-01 01:15:00  15.5 <0>      38.2 <0>       24.4 <0>  \n 7 marabwq     2020-01-01 01:30:00  15.5 <0>      38.3 <0>       24.4 <0>  \n 8 marabwq     2020-01-01 01:45:00  15.5 <0>      38.3 <0>       24.4 <0>  \n 9 marabwq     2020-01-01 02:00:00  15.5 <0>      38.3 <0>       24.4 <0>  \n10 marabwq     2020-01-01 02:15:00  15.5 <0>      38.3 <0>       24.4 <0>  \n# … with 140,342 more rows, and 2 more variables: DO_mgl <dbl>, F_DO_mgl <chr>\n\n\n\n\nThis dataset is from the NOAA NERRS Centralized Data Management Office and provides measures of water quality parameters in the Mission and Aransas (MAR) National Estuarine Research Reserve (NERR)(NOAA National Estuarine Research Reserve System (NERRS) 2022).\nIn this data we have a row for each date/time observation with associated columns for Station, Temperature, Specific Conductance, Salinity, and Dissolved Oxygen. The columns with the F_ prefix are qa/qc flags, <0> is accepted data."
  },
  {
    "objectID": "data_exploration.html#plot-and-clean-data",
    "href": "data_exploration.html#plot-and-clean-data",
    "title": "2  Data Exploration",
    "section": "\n2.2 Plot and Clean Data",
    "text": "2.2 Plot and Clean Data\nggplot2 is the graphics package for producing layered plots. The underlying philosophy of ggplot2 is to iteratively build your plots layer by layer which allows for some sophisticated plots. I won’t go into full details of using ggplot2 but lots of information is available in the ggplot2 book.\nThree key things you need to learn: data, aesthetics, geoms.\n\ndata: a data frame or tibble with the data you want to plot.\naesthetics mappings (or aes): specify how variables will be visually depicted (x, y, color, shape, size, etc.).\ngeoms: are the layers that define how each layer is rendered (points, lines, bars, etc.).\n\nUsing the data we imported from above we can quickly create a basic scatter plot. Note the use of the + symbol to iteratively add layers to our plot. First we specify the data, then the geometry, and the aesthetic for that geom:\n\np1 <- ggplot(data = df, aes(x = DateTimeStamp, y = Temp)) +\n  geom_point()\np1\n\nWarning: Removed 4975 rows containing missing values (geom_point).\n\n\n\n\n\nThe above figure demonstrates the importance of plotting your data, the -100 value is clearly an issue. One thing we can do is filter the data to try and track down the issue. The dplyr package provides a number of functions to explore your data. Here, the data is filtered on the Temp column to include data less than -99:\n\ndf |> \n  filter(Temp < -99)\n\n# A tibble: 1 × 10\n  StationCode DateTimeStamp        Temp F_Temp       SpCond F_SpCond   Sal F_Sal\n  <fct>       <dttm>              <dbl> <chr>         <dbl> <chr>    <dbl> <chr>\n1 marcewq     2021-08-23 20:45:00  -100 <-3> [GIM] … -1211. <-3> [G…   -99 <-3>…\n# … with 2 more variables: DO_mgl <dbl>, F_DO_mgl <chr>\n\n\nThe F_Temp qa flag is < -3 > indicating QA rejected the data point. So, let’s update the data to remove data that doesn’t have the < 0 > data flag:\n\ndf <- df |> \n  filter(F_Temp == \"<0>\")\ndf\n\n# A tibble: 111,090 × 10\n   StationCode DateTimeStamp        Temp F_Temp SpCond F_SpCond   Sal F_Sal\n   <fct>       <dttm>              <dbl> <chr>   <dbl> <chr>    <dbl> <chr>\n 1 marabwq     2020-01-01 00:00:00  15.5 <0>      38.2 <0>       24.3 <0>  \n 2 marabwq     2020-01-01 00:15:00  15.5 <0>      38.2 <0>       24.3 <0>  \n 3 marabwq     2020-01-01 00:30:00  15.5 <0>      38.2 <0>       24.3 <0>  \n 4 marabwq     2020-01-01 00:45:00  15.5 <0>      38.2 <0>       24.3 <0>  \n 5 marabwq     2020-01-01 01:00:00  15.5 <0>      38.2 <0>       24.3 <0>  \n 6 marabwq     2020-01-01 01:15:00  15.5 <0>      38.2 <0>       24.4 <0>  \n 7 marabwq     2020-01-01 01:30:00  15.5 <0>      38.3 <0>       24.4 <0>  \n 8 marabwq     2020-01-01 01:45:00  15.5 <0>      38.3 <0>       24.4 <0>  \n 9 marabwq     2020-01-01 02:00:00  15.5 <0>      38.3 <0>       24.4 <0>  \n10 marabwq     2020-01-01 02:15:00  15.5 <0>      38.3 <0>       24.4 <0>  \n# … with 111,080 more rows, and 2 more variables: DO_mgl <dbl>, F_DO_mgl <chr>\n\n\nThis removes about 29,263 observations. Try plotting again:\n\np1 <- ggplot(data = df, aes(x = DateTimeStamp, y = Temp)) +\n  geom_point()\np1\n\n\n\n\nWe can use different geoms to explore the data:\n\np2 <- ggplot(data = df, aes(x = Temp)) +\n  geom_histogram(binwidth = 1)\np2\n\n\n\n\n\nggplot(data = df, aes(y = Temp, x = StationCode)) +\n  geom_boxplot()\n\n\n\n\nTo explore relationships between two variables, use geom_point and geom_smooth with each variable mapped to x and y. In this example, ggplot2 prints a message indicating 834 rows of data had missing or NA values that could not be plotted. geom_mooth will plot the smooth line (using a loess or gam smooth) between two variables. If you want the linear regression drawn use the argument method = \"lm\".\n\np3 <- ggplot(data = df, aes(x = Temp, y = DO_mgl)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(se = FALSE)\np3\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 834 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 834 rows containing missing values (geom_point).\n\n\n\n\n\nThis is a good example to introduce the other important aesthetics in ggplot2. There is a clear negative relationship between Temperature and Dissolved Oxygen. Our data includes two sites, is there a difference between the two sites? We can map color to the site variable which will color each observation and each smooth a different color. Although shape and color can be used inside the aes() mapping function, you can assign them a value in the geom directly. Here we asign values for the shape and alpha properties in the point geom.\n\np3 <- ggplot(data = df, aes(x = Temp, y = DO_mgl, color = StationCode)) +\n  geom_point(shape = 21, alpha = 0.05) +\n  geom_smooth(se = FALSE)\np3\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 834 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 834 rows containing missing values (geom_point).\n\n\n\n\n\nThere was no reason to expect a difference and there isn’t. However the plot is muddy because there are so many overlying data points. We can also facet the graph by a variable. The following code also shows how you can just add another layer to your existing ggplot object:\n\np3 <- p3 +\n  facet_wrap(~StationCode)\n\np3\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 834 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 834 rows containing missing values (geom_point).\n\n\n\n\n\nAs you can see, ggplot2 allows you to rapidly iterate plots to explore data. When exploring the data, the formatting might not matter much, but if you want to export plots, we also need to take care of labels and general plot visual preferences.\n\np3 <- p3 +\n  labs(x = \"Temperature [°F]\", y = \"Dissolved Oxygen [mg/L]\") +\n  scale_colour_brewer(name = \"Stations\",\n                      palette = \"Dark2\",\n                      labels = c(\"Aransas Bay\", \"Copano East\"))\np3\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 834 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 834 rows containing missing values (geom_point).\n\n\n\n\n\n\n2.2.1 Themes\nggplot2 has a number of built-in themes. For most of our use cases (reports and papers), the grey background and white lines are not great choices. We recommend using the theme_bw() function at the bare minimum:\n\np3 +\n  theme_bw()\n\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 834 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 834 rows containing missing values (geom_point).\n\n\n\n\n\nThe twriTemplates package is available with a custom theme and some color palettes that are consistent with the Institute’s branding:\n\ninstall.packages(\"twriTemplates\", \n                 repos = c(txwri = 'https://txwri.r-universe.dev',\n                           CRAN = 'https://cloud.r-project.org'))\n\n\nlibrary(twriTemplates)\np3 +\n  theme_TWRI_print() +\n  scale_color_discrete_twri(name = \"Stations\",\n                            labels = c(\"Aransas Bay\", \"Copano East\"))\n\nWarning: Removed 834 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 834 rows containing missing values (geom_point).\n\n\n\n\n\nThe facet labels are still not very useful. There isn’t an easy way to change this using ggplot2. Instead we can create a new variable using the mutate() function in dplyr. There is more discussion about dplyr in the next section.\n\ndf <- df |> \n  mutate(StationName = case_when(\n    StationCode == \"marabwq\" ~ \"Aransas Bay\",\n    StationCode == \"marcewq\" ~ \"Copano Bay East\"\n  ))\n\nggplot(data = df,\n       aes(x = Temp, y = DO_mgl, color = StationName)) +\n  geom_point(shape = 21, alpha = 0.05) +\n  geom_smooth(se = FALSE) +\n  facet_wrap(~StationName) +\n  labs(x = \"Temperature [°F]\", y = \"Dissolved Oxygen [mg/L]\") +\n  scale_color_discrete_twri(name = \"Stations\") +\n  theme_TWRI_print()\n\nWarning: Removed 834 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 834 rows containing missing values (geom_point)."
  },
  {
    "objectID": "data_exploration.html#manipulate-and-summarize-data",
    "href": "data_exploration.html#manipulate-and-summarize-data",
    "title": "2  Data Exploration",
    "section": "\n2.3 Manipulate and Summarize Data",
    "text": "2.3 Manipulate and Summarize Data\ndplyr is a powerful package designed for working with dataframes and tibbles. Read the dplyr introduction to get a better sense of all the available functions. Essentially dplyr provides “verb” functions that correspond to how you want to manipulate the dataframe.\nCommonly used verbs include:\n\n\nfilter(): subset the rows based on value\n\nmutate(): create a new column\n\nselect(): subset columns based on names or types\n\ngroup_by(): group the data by variable values\n\nsummarise(): summarise the grouped or ungrouped data based on user provided functions\n\nThere are many other functions, but these cover many use cases for people just getting started. dplyr also provides functions for joining datasets based on common variables, changing the order of a dataframe, calculating cumulative statistics, calculating lag and leading values, and rolling or window rank functions among others. All of these can be very useful to become familiar with.\nIn this example I want to explore seasonal differences in the water quality parameters. mutate() is used to create new variables. ::: {.cell}\nsummary_plot <- df |> \n  # returns DateTimeStamp Value as the month only\n  # see ?strptime() for more info\n  mutate(Month = as_factor(format(DateTimeStamp, \"%B\")),\n         StationName = case_when(\n           StationCode == \"marabwq\" ~ \"Aransas Bay\",\n           StationCode == \"marcewq\" ~ \"Copano Bay East\")) |> \n  ## months have an inherent order, right now it is alphabetical\n  ## this adds the correct order so it plots nicely\n  mutate(Month = lvls_revalue(Month, month.name)) |> \n  ggplot() +\n  geom_boxplot(aes(x = Month, y = DO_mgl, fill = StationName)) +\n  scale_fill_discrete_twri(name = \"Stations\") +\n  theme_TWRI_print()\nsummary_plot\n\nWarning: Removed 834 rows containing non-finite values (stat_boxplot).\n\n\n\n\n:::\nNow we have a summary figure, but what about the data? We can use the summarize() or summarise() function (depending on your English preference) to calculate some summary statistics for each group.\n\nsummary_table <- df |> \n  mutate(Month = as_factor(format(DateTimeStamp, \"%B\")),\n         StationName = case_when(\n           StationCode == \"marabwq\" ~ \"Aransas Bay\",\n           StationCode == \"marcewq\" ~ \"Copano Bay East\")) |> \n  mutate(Month = lvls_revalue(Month, month.name)) |> \n  ## create groups to calculate summary stats on\n  group_by(StationName, Month) |> \n  ## calculate the summary stats\n  ## returns a column for each stat with a row for each group\n  summarize(Mean = mean(DO_mgl),\n            Max = max(DO_mgl),\n            Min = min(DO_mgl),\n            ## no standard error function in base R\n            ## can calculate manually here\n            SE = sd(DO_mgl)/sqrt(length(DO_mgl)))\n\n`summarise()` has grouped output by 'StationName'. You can override using the\n`.groups` argument.\n\nsummary_table\n\n# A tibble: 24 × 6\n# Groups:   StationName [2]\n   StationName Month      Mean   Max   Min      SE\n   <chr>       <fct>     <dbl> <dbl> <dbl>   <dbl>\n 1 Aransas Bay January    8.47  12.1   6.7 0.00870\n 2 Aransas Bay February   8.96  13.7   6.4 0.0171 \n 3 Aransas Bay March      7.57  10.4   5.6 0.00722\n 4 Aransas Bay April      7.12   8.5   5.9 0.00738\n 5 Aransas Bay May        6.75   9     4.8 0.00839\n 6 Aransas Bay June       5.98   8.1   2.2 0.0109 \n 7 Aransas Bay July       5.32   8.2   0.2 0.0180 \n 8 Aransas Bay August     5.45   7.4   0.2 0.0114 \n 9 Aransas Bay September  5.88   9.2   2.3 0.0111 \n10 Aransas Bay October    6.67   9.6   3.2 0.00959\n# … with 14 more rows"
  },
  {
    "objectID": "data_exploration.html#export-data",
    "href": "data_exploration.html#export-data",
    "title": "2  Data Exploration",
    "section": "\n2.4 Export Data",
    "text": "2.4 Export Data\n\n2.4.1 Figures\nYou have created a figure and a table in R. Now you probably need to put it in a report, website, or other self contained file for your boss to look at. For figures created with ggplot2, the ggsave() option is the easiest option. First I reccomend installing, the ragg package because if it available, ggsave() will use it. ragg provides higher performance and higher quality images than the graphics devices available in base R.\n\n# install ragg\ninstall.packages(\"ragg\")\nlibrary(ragg)\n\n## ggsave defaults to the last plot displayed\n## it is safer to specify which plot you want saved\nggsave(filename = \"figures/boxplot_mare.png\",\n       plot = summary_plot,\n       width = 8,\n       height = 6,\n       dpi = 300,\n       units = \"in\")\n\nIf you have a figure created from something other than a ggplot object, then we need to use the ragg graphics device to save it as a file (this also works with ggplot as demonstrated here). If you don’t have ragg installed, substitute png() for agg_png().\n\n# first function creates the device\nagg_png(filename = \"figures/boxplot_mare.png\",\n    width = 8,\n    height = 6,\n    res = 300,\n    units = \"in\")\n# copy the plot to the file\nsummary_plot\n# turns off the device\ndev.off()\n\n\n2.4.2 Tables\nreadr and writexl packages provide the functions required to export data frames to text delimited or Microsoft Excel files.\n\n## write a csv\nwrite_csv(x = summary_table,\n          file = \"export_data/monthly_summary_mare.csv\")\n\n## or an Excel file\n## uncomment the next line if writexl is not installed\n# install.packages(\"writexl\")\n\nlibrary(writexl)\n# one sheet\nwrite_xlsx(x = summary_table,\n           path = \"export_data/monthly_summary_mare.xlsx\")\n\n# or specify sheets\nwrite_xlsx(x = list(summarysheet = summary_table,\n                    rawdatasheet = df),\n           path = \"export_data/monthly_summary_mare.xlsx\")"
  },
  {
    "objectID": "data_exploration.html#workflow",
    "href": "data_exploration.html#workflow",
    "title": "2  Data Exploration",
    "section": "\n2.5 Workflow",
    "text": "2.5 Workflow\n\n\n\n\n\n\nIncomplete\n\n\n\nPlan to add information on proper workflow. One script to download, clean, and save data that analysis is conducted on.\nOne script to run the analysis and export data/tables/figures\nMetadata, readme, etc.\n\n\n\n\n\n\nNOAA National Estuarine Research Reserve System (NERRS). 2022. System-Wide Monitoring Program. NOAA NERRS Centralized Data Management Office. http://cdmo.baruch.sc.edu/."
  },
  {
    "objectID": "streamflow.html#data-sources",
    "href": "streamflow.html#data-sources",
    "title": "3  Streamflow",
    "section": "3.1 Data Sources",
    "text": "3.1 Data Sources"
  },
  {
    "objectID": "streamflow.html#summarise",
    "href": "streamflow.html#summarise",
    "title": "3  Streamflow",
    "section": "3.2 Summarise",
    "text": "3.2 Summarise"
  },
  {
    "objectID": "streamflow.html#flow-duration-curve",
    "href": "streamflow.html#flow-duration-curve",
    "title": "3  Streamflow",
    "section": "3.3 Flow Duration Curve",
    "text": "3.3 Flow Duration Curve"
  },
  {
    "objectID": "stage-discharge.html#data",
    "href": "stage-discharge.html#data",
    "title": "4  Stage-Discharge Rating Curves",
    "section": "\n4.1 Data",
    "text": "4.1 Data\nThis example uses field measurements of stream stage and instantaneous discharge measured at the COMO NEON field site (NEON (National Ecological Observatory Network), n.d.). Raw data is available in the neon.rds file within the tutorial download.\n\nlibrary(tidyverse)\nlibrary(twriTemplates)\n\nexample_data <- read_rds(\"data/neon.rds\")\nstage_discharge <- example_data$dsc_fieldData\nglimpse(stage_discharge)\n\nRows: 134\nColumns: 36\n$ uid                     <chr> \"1068e9ad-37bd-42ac-96d2-b999545adefe\", \"06d63…\n$ recorduid               <chr> \"3b2d4830-50cd-40c8-a772-790d2386c33d\", \"5ed72…\n$ domainID                <chr> \"D13\", \"D13\", \"D13\", \"D13\", \"D13\", \"D13\", \"D13…\n$ siteID                  <chr> \"COMO\", \"COMO\", \"COMO\", \"COMO\", \"COMO\", \"COMO\"…\n$ namedLocation           <chr> \"COMO.AOS.discharge\", \"COMO.AOS.discharge\", \"C…\n$ collectedBy             <chr> \"LCLARK\", \"HSCHARTEL\", \"DMONAHAN\", \"HSCHARTEL\"…\n$ startDate               <dttm> 2015-08-11 15:30:00, 2015-08-24 16:16:00, 201…\n$ collectDate             <dttm> 2015-08-11 15:30:00, 2015-08-24 16:16:00, 201…\n$ streamStage             <dbl> 0.30, 0.34, 0.33, 0.31, 0.32, 0.35, 0.37, 0.74…\n$ streamStageUnits        <chr> \"meter\", \"meter\", \"meter\", \"meter\", \"meter\", \"…\n$ handheldDeviceID        <chr> \"122581001261\", \"122581001261\", \"122581001261\"…\n$ velocitySensorID        <chr> \"132660300436\", \"132660300436\", \"132660300436\"…\n$ filterParamTime         <dbl> 10, 10, 10, 10, 10, 10, 10, 10, 10, 20, 10, 10…\n$ stationEntryTest        <chr> \"Non-fixed\", \"Non-fixed\", \"Non-fixed\", \"Non-fi…\n$ flowCalculation         <chr> \"Mid-section\", \"Mid-section\", \"Mid-section\", \"…\n$ waterEdge               <chr> \"Right edge water\", \"Right edge water\", \"Right…\n$ totalDischarge          <dbl> 10.81, 5.71, 6.58, 3.49, 3.15, 9.39, 12.75, 77…\n$ totalDischargeUnits     <chr> \"litersPerSecond\", \"litersPerSecond\", \"litersP…\n$ samplingProtocolVersion <chr> \"NEON.DOC.001085vB\", \"NEON.DOC.001085vB\", \"NEO…\n$ averageVelocityUnits    <chr> \"meterPerSecond\", \"meterPerSecond\", \"meterPerS…\n$ waterDepthUnits         <chr> \"meter\", \"meter\", \"meter\", \"meter\", \"meter\", \"…\n$ tapeDistanceUnits       <chr> \"meter\", \"meter\", \"meter\", \"meter\", \"meter\", \"…\n$ flowCalcQF              <chr> \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"…\n$ dischargeUnitsQF        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ streamStageUnitsQF      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ averageVelocityUnitsQF  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ waterDepthUnitsQF       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ tapeDistanceUnitsQF     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ lowVelocityFinalQF      <dbl> 5, 20, 0, 10, 0, 17, 0, 45, 0, 11, 25, 11, 6, …\n$ finalDischarge          <dbl> 10.71, 5.62, 6.52, 3.44, 3.07, 9.29, 12.75, 78…\n$ totalDischargeCalcQF    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ profileName             <chr> \"COMO\", \"COMO\", \"COMO\", \"COMO\", \"COMO\", \"COMO\"…\n$ stageImpractical        <chr> \"OK\", \"OK\", \"OK\", \"OK\", \"OK\", \"OK\", \"OK\", \"OK\"…\n$ dataQF                  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ publicationDate         <chr> \"20211222T003739Z\", \"20211222T003739Z\", \"20211…\n$ release                 <chr> \"RELEASE-2022\", \"RELEASE-2022\", \"RELEASE-2022\"…"
  },
  {
    "objectID": "stage-discharge.html#plot-data",
    "href": "stage-discharge.html#plot-data",
    "title": "4  Stage-Discharge Rating Curves",
    "section": "\n4.2 Plot Data",
    "text": "4.2 Plot Data\nFirst step is to plot the data.\n\n# plot observations over time\nggplot(stage_discharge) +\n  geom_point(aes(collectDate, finalDischarge)) +\n  labs(x = \"Date\", y = \"Discharge [L/s]\") +\n  theme_TWRI_print()\n\nWarning: Removed 9 rows containing missing values (geom_point).\n\n\n\n\n# plot the stage and discharge relationship\nggplot(stage_discharge) +\n  geom_point(aes(streamStage, finalDischarge)) +\n  labs(x = \"Stage [m]\", y = \"Discharge [L/s]\") +\n  scale_x_log10() +\n  scale_y_log10() +\n  theme_TWRI_print()\n\nWarning: Removed 9 rows containing missing values (geom_point).\n\n\n\n\n\nLooks like there might be a shift in the rating curve at some point. Use dplyr to create a new year variable and the gghighlight package to explore it.\n\n# install.packages(\"gghighlight\")\nlibrary(gghighlight)\nstage_discharge |> \n  filter(!is.na(finalDischarge)) |> \n  mutate(year = format(collectDate, format = \"%Y\")) |> \n  ggplot() +\n  geom_point(aes(streamStage, finalDischarge, color = year)) +\n  facet_wrap(~year) +\n  gghighlight(max_highlight = 8,\n              use_direct_label = FALSE,\n              calculate_per_facet = FALSE)   +\n  labs(x = \"Stage [m]\", y = \"Discharge [L/s]\") +\n  theme_TWRI_print()"
  },
  {
    "objectID": "stage-discharge.html#fit-rating-curve",
    "href": "stage-discharge.html#fit-rating-curve",
    "title": "4  Stage-Discharge Rating Curves",
    "section": "\n4.3 Fit Rating Curve",
    "text": "4.3 Fit Rating Curve\nIt appears the rating curve shifted after 2016 and possibly after 2018. In practice we would fit different rating curves based on this information. For this example the rating curve will only be fit to 2019-2021 data. We will use this data to estimate the \\(K\\), \\(H_0\\) and \\(Z\\) parameters in the power function described at the top of the chapter using nonlinear least squares.\n\n# install.packages(\"nls.multstart\")\n# install.packages(\"nlstools\")\nlibrary(nls.multstart)\nlibrary(nlstools)\n\n# clean the data a little bit and filter\nstage_discharge <- stage_discharge |> \n  filter(!is.na(finalDischarge)) |>\n  mutate(year = format(startDate, \"%Y\")) |> \n  filter(year >= 2018) |> \n  # convert units to feet and cfs\n  mutate(streamStage = streamStage * 3.28084,\n         finalDischarge = finalDischarge * 0.0353147)\n\n# Set the equation\nf_Q <- formula(finalDischarge ~ K*(streamStage - H_0)^Z)\n\n# Some initial starting values\nstart_lower <- c(K = -10, Z = -10, H_0 = 0.02)\nstart_upper <- c(K = 10, Z = 10, H_0 = 1)\n\n# nonlinear least squares\nm1 <- nls_multstart(f_Q,\n          data = stage_discharge,\n          iter = 1000,\n          start_lower = start_lower,\n          start_upper = start_upper,\n          supp_errors = 'Y',\n          lower = c(K = -10, Z = -10, H_0 = 0),\n          control = minpack.lm::nls.lm.control(maxiter = 1000L))\n\nsummary(m1)\n\n\nFormula: finalDischarge ~ K * (streamStage - H_0)^Z\n\nParameters:\n    Estimate Std. Error t value Pr(>|t|)    \nK     5.0399     1.3421   3.755 0.000335 ***\nH_0   0.7874     0.1315   5.990 6.30e-08 ***\nZ     1.9097     0.2568   7.436 1.23e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.889 on 77 degrees of freedom\n\nNumber of iterations to convergence: 37 \nAchieved convergence tolerance: 1.49e-08\n\n\nNLS estimated parameters are: \\(K =\\) 5.039872, \\(H_0 =\\) 0.7874016, and \\(Z =\\) 0.7874016. Before using these parameter, evaluated the goodness of fit using the model residuals (add citation or note with more resources here).\n\nShow the code# for easy multipanel plots, use the patchwork package\n#install.packages(\"patchwork\")\nlibrary(patchwork)\n\nstd_resids <- as_tibble(nlsResiduals(m1)$resi2)\n\nstage_discharge <- stage_discharge |> \n    mutate(fits = std_resids$`Fitted values`,\n         residuals = std_resids$`Standardized residuals`)\n\n\np1 <- ggplot(stage_discharge) +\n  geom_density(aes(residuals)) +\n  labs(x = \"Standardized Residuals\",\n       y = \"Count\",\n       subtitle = \"Distribution of standardized residuals\") +\n  theme_TWRI_print()\n\n\np2 <- ggplot(stage_discharge) +\n  geom_point(aes(streamStage, residuals), color = \"steelblue\", alpha = 0.4) +\n  labs(x = \"Stream Height [ft]\",\n       y = \"Standardized Residuals\",\n       subtitle = \"Residuals against stream height\") +\n  theme_TWRI_print()\n\np3 <- ggplot(stage_discharge) +\n  geom_point(aes(fits, finalDischarge), color = \"steelblue\", alpha = 0.4) +\n  labs(x = \"Model Fits\",\n       y = \"Measured Discharge [cfs]\",\n       subtitle = \"Measured against fitted\") +\n  theme_TWRI_print()\n\np4 <- ggplot(stage_discharge) +\n  stat_qq(aes(sample = residuals), color = \"steelblue\", alpha = 0.4) +\n  stat_qq_line(aes(sample = residuals)) +\n  labs(x = \"Theoretical\",\n       y = \"Standardized Residuals\",\n       subtitle = \"Sample Quantile against theoretical quantile\") +\n  theme_TWRI_print()\n\n\n# patchwork allows us to assemble plots using + and /\n\n(p1 + p2) / (p3 + p4)\n\n\n\n\nThe plots indicate increasing residual variance as stream height increases and heavy tails in Q-Q plot. Two options from here are to (1) fit a rating curve per year or (2) fit a piece-wise log-linear model that assumes a different relationship along different parts of the rating curve. Because of the large residual variance at the top of curve, I think fitting a curve per year will do the job. You may also choose to fit models by season (small streams in particular may see large changes in rating curves do to changes in stream bank vegetation)."
  },
  {
    "objectID": "stage-discharge.html#fit-multiple-curves",
    "href": "stage-discharge.html#fit-multiple-curves",
    "title": "4  Stage-Discharge Rating Curves",
    "section": "\n4.4 Fit Multiple Curves",
    "text": "4.4 Fit Multiple Curves\n\n\n\n\n\n\nNote\n\n\n\nThis section is slightly more advanced and requires some understanding of list structures in R and nested dataframes.\n\n\nThe purrr package facilitates running a function on a list of nested data. The idea here is to subset the dataframe by year, create a list with each item in the list being a subset of the dataframe, then running nls_multstart() on each item in that list. Sounds like a loop huh? We achieve this using for() or lapply() functions in base R. The nice thing about doing it with purrr is that we can keep everything together in a single dataframe. The first step is to create a nested dataframe:\n\nnested_data <- example_data$dsc_fieldData |> \n  filter(!is.na(finalDischarge)) |>\n  mutate(year = format(startDate, \"%Y\")) |> \n  filter(year >= 2018) |> \n  mutate(streamStage = streamStage * 3.28084,\n         finalDischarge = finalDischarge * 0.0353147) |> \n  ## group data by year\n  group_by(year) |> \n  ## nest the data by year\n  nest()\n\nnested_data\n\n# A tibble: 4 × 2\n# Groups:   year [4]\n  year  data              \n  <chr> <list>            \n1 2018  <tibble [20 × 36]>\n2 2019  <tibble [24 × 36]>\n3 2020  <tibble [15 × 36]>\n4 2021  <tibble [21 × 36]>\n\n\nNow we use the map() function in purrr to iterate the nls.multstart() function on each nested dataframe:\n\nnested_data <- nested_data |> \n  mutate(model_output = map(.x = data,\n                            ~nls_multstart(formula = f_Q,\n                                           data = .x,\n                                           iter = 1000,\n                                           start_lower = start_lower,\n                                           start_upper = start_upper,\n                                           supp_errors = 'Y',\n                                           lower = c(K = -10, Z = -10, H_0 = 0),\n                                           control = minpack.lm::nls.lm.control(maxiter = 1000L))))\nnested_data\n\n# A tibble: 4 × 3\n# Groups:   year [4]\n  year  data               model_output\n  <chr> <list>             <list>      \n1 2018  <tibble [20 × 36]> <nls>       \n2 2019  <tibble [24 × 36]> <nls>       \n3 2020  <tibble [15 × 36]> <nls>       \n4 2021  <tibble [21 × 36]> <nls>       \n\n\nWe created a new column called model_output that is a list of the output from the nls_multstart() function that was run on each item in the data column. Grab the residuals and fits from each model:\n\nShow the codenested_data <- nested_data |> \n  mutate(residuals = map(model_output,\n                         ~as_tibble(nlsResiduals(.x)$resi2))) |> \n  unnest(c(data,residuals))\n\np1 <- ggplot(nested_data) +\n  geom_point(aes(`Fitted values`, `Standardized residuals`)) +\n  facet_wrap(~year) +\n  labs(x = \"Fitted\",\n       y = \"Standardized Residuals\",\n       subtitle = \"Sample Quantile against theoretical quantile\") +\n  theme_TWRI_print()\n\np2 <- ggplot(nested_data) +\n  geom_density(aes(`Standardized residuals`)) +\n  facet_wrap(~year) +\n  labs(x = \"Standardized Residuals\",\n       y = \"Count\",\n       subtitle = \"Distribution of standardized residuals\") +\n  theme_TWRI_print()\n\np3 <- ggplot(nested_data) +\n  geom_point(aes(`Fitted values`, finalDischarge), color = \"steelblue\", alpha = 0.4) +\n  facet_wrap(~year) +\n  labs(x = \"Model Fits\",\n       y = \"Measured Discharge [cfs]\",\n       subtitle = \"Measured against fitted\") +\n  theme_TWRI_print()\n\np4 <- ggplot(nested_data) +\n  stat_qq(aes(sample = `Standardized residuals`), color = \"steelblue\", alpha = 0.4) +\n  stat_qq_line(aes(sample = `Standardized residuals`)) +\n  facet_wrap(~year) +\n  labs(x = \"Theoretical\",\n       y = \"Standardized Residuals\",\n       subtitle = \"Sample Quantile against theoretical quantile\") +\n  theme_TWRI_print()\n\np1 / p2 / p3 / p4\n\n\n\n\nThe results are a mixed bag. For the most part, the residuals are tighter to mean zero and the tails are not as heavy as the first example. We will assume the data is good enough to continue the example. In practice, I might explore the use of seasonal rating curves or piece-wise functions.\nNext lets make a nice plot showing the rating curve with the observed data.\n\nfits <- nested_data |> \n  nest(data = -c(year, model_output)) |> \n  # create a new dataframe by group\n  # this includes the full range of the predictor variable\n  # so we can draw a nice smooth line using predictions\n  mutate(newdata = map(data,\n                       ~{\n                         tibble(streamStage = seq(min(nested_data$streamStage),\n                                                  max(nested_data$streamStage),\n                                                  length.out = 100))\n                       })) |> \n  mutate(fits = map2(newdata, model_output,\n                     ~{predict(.y, .x)})) |> \n  unnest(c(newdata, fits))\n\nggplot() +\n  geom_point(data = nested_data,\n             aes(streamStage, finalDischarge, color = year)) +\n  geom_line(data = fits,\n            aes(streamStage, fits, color = year)) +\n  facet_wrap(~year) +\n  gghighlight(max_highlight = 8,\n              use_direct_label = FALSE,\n              calculate_per_facet = FALSE)   +\n  labs(x = \"Stage [ft]\", y = \"Discharge [cfs]\") +\n  theme_TWRI_print()\n\n\n\n\n\n\n\n\nNEON (National Ecological Observatory Network). n.d. Discharge Field Collection (Dp1.20048.001), RELEASE-2022. https://doi.org/10.48443/eaak-tt31."
  },
  {
    "objectID": "drainage-area-ratio.html#basic-approach",
    "href": "drainage-area-ratio.html#basic-approach",
    "title": "5  Drainage Area Ratio",
    "section": "\n5.1 Basic Approach",
    "text": "5.1 Basic Approach\nThe basic approach where the exponent \\(\\phi\\) equals one is shown below. We will use known data from two USGS gages to evaluate performance.\n\n## load packages\nlibrary(dataRetrieval)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(twriTemplates)\n\nFor this example, I am using two USGS sites. In application you would only use a single site.\n\n## Get drainage areas for two watersheds with dataRetrieval\nsiteInfo <- readNWISsite(c('08110100', '08109800'))\nsiteInfo$contrib_drain_area_va\n\n[1] 244 195\n\n\nDownload the mean daily streamflow from 08110100 (Davidson Creek):\n\nDavidson <- readNWISdv('08110100', \"00060\", \"2015-01-01\", \"2020-12-31\")\nDavidson <- renameNWISColumns(Davidson)\nhead(Davidson)\n\n  agency_cd  site_no       Date   Flow Flow_cd\n1      USGS 08110100 2015-01-01   4.82       A\n2      USGS 08110100 2015-01-02  17.20       A\n3      USGS 08110100 2015-01-03 258.00       A\n4      USGS 08110100 2015-01-04 288.00       A\n5      USGS 08110100 2015-01-05  91.80       A\n6      USGS 08110100 2015-01-06  40.90       A\n\n\nDownload mean daily streamflows at 08109800 (Yegua Creek):\n\nYegua <- readNWISdv('08109800', \"00060\", \"2015-01-01\", \"2020-12-31\")\nYegua <- renameNWISColumns(Yegua)\n\nNow create a dataframe to compare measured streamflows at Yegua Creek with DAR estimated flows using Davidson as the source creek:\n\n# calculate the area ratio\nDAR <- siteInfo$contrib_drain_area_va[2] / siteInfo$contrib_drain_area_va[1]\n\n# create columns for the area ratio and DAR*Davidson Flow\nDavidson <- Davidson |> \n  mutate(DAR = DAR,\n         est_Yegua_Q = DAR * Flow)\n\n# join the data by date\nestimates <- Yegua |> \n  left_join(Davidson |>  select(Date, Davidson_Flow = Flow, DAR, est_Yegua_Q),\n            by = c(\"Date\" = \"Date\"))\n\nggplot(estimates) +\n  geom_point(aes(Flow, est_Yegua_Q), \n             color = \"steelblue\",\n             alpha = 0.5) +\n  geom_abline(slope = 1) +\n  coord_equal() +\n  scale_x_continuous(trans = \"pseudo_log\",\n                     breaks = c(1,10,100,1000,10000)) +\n  scale_y_continuous(trans = \"pseudo_log\",\n                     breaks = c(1,10,100,1000,10000)) +\n  theme_TWRI_print() +\n  labs(x = \"Measured Flow [cfs]\",\n       y = \"DAR Estimate [cfs]\")"
  },
  {
    "objectID": "drainage-area-ratio.html#parameterized-exponent",
    "href": "drainage-area-ratio.html#parameterized-exponent",
    "title": "5  Drainage Area Ratio",
    "section": "\n5.2 Parameterized Exponent",
    "text": "5.2 Parameterized Exponent\n\n\n\n\n\n\nIncomplete\n\n\n\nStill needs to be written."
  },
  {
    "objectID": "drainage-area-ratio.html#other-data-considerations",
    "href": "drainage-area-ratio.html#other-data-considerations",
    "title": "5  Drainage Area Ratio",
    "section": "\n5.3 Other Data Considerations",
    "text": "5.3 Other Data Considerations\n\n\n\n\n\n\nIncomplete\n\n\n\nAdd discussion about naturalized flows."
  },
  {
    "objectID": "water-quality.html#sources-of-data",
    "href": "water-quality.html#sources-of-data",
    "title": "6  Water Quality Data",
    "section": "\n6.1 Sources of Data",
    "text": "6.1 Sources of Data\n\n6.1.1 Sampling Data\nThe official data source for state water quality data used in surface water quality assessments in Texas is the Surface Water Quality Monitoring Information System (SWQMIS). My suggestion for obtaining station or water body specific data is to ask our employees that have authorized access to SWQMIS to make a data pull for you. There are two other options. First, TCEQ has a spatial explorer that you can point and click to download data by stream segment https://www80.tceq.texas.gov/SwqmisPublic/index.htm. This will download a pipe (|) delimited text file with all the water quality monitoring data for the segment. Second, the CRP Data Tool provides a way to query the SWQMIS by date, parameter code, parameter group, basin, segment, and station. Data is also provided as a pipe delimited text file with the queried data. Currently, there are no capabilities to make queries from within R.\nThe Water Quality Portal (WQP) is the EPA and USGS national database (formally STORET) for storing and providing discrete water quality monitoring data collected by states, tribes, federal and other partners. The WQP is handy for large or automated data downloads for various projects. One drawback is that the WQP and SWQMIS do not have common variables across both databases. For example, data from the WQP does not include Project Type codes that indicate the purpose for the monitoring event (flow biased storm samples or unbiased ambient samples) that might impact data analysis. WQP and SWQMIS do not use the same parameter codes and sometimes important data such as analysis methods or units get omitted in the data uploads to WQP. In general, for our state funded projects, use the SWQMIS database.\n\n6.1.2 Standards\nThe most up to date EPA approved standards are available at EPA or at TCEQ. The applicable standard for a waterbody will vary based on assessed use and sometimes season. Do not assume the standard based on a nearby or upstream/downstream water body, double check the standards.\n\n6.1.3 Assessment Summaries\nTCEQ doesn’t publicly publish the data that goes into each water body assessment. However, summaries of assessed data by assessment unit are available. The full integrated report for the current cycle is available at https://www.tceq.texas.gov/waterquality/assessment. Within each integrated report is a link to the water body assessments by basin: https://www.tceq.texas.gov/waterquality/assessment/22twqi/22basinlist. These pdfs provide information about the use, criteria, data range, number of data assessed, number of excrescences, mean (or geomean) of data assessed and additional listing information."
  },
  {
    "objectID": "water-quality.html#summarizing-data",
    "href": "water-quality.html#summarizing-data",
    "title": "6  Water Quality Data",
    "section": "\n6.2 Summarizing Data",
    "text": "6.2 Summarizing Data"
  },
  {
    "objectID": "water-quality.html#data",
    "href": "water-quality.html#data",
    "title": "6  Water Quality Data",
    "section": "\n6.3 Data",
    "text": "6.3 Data\n\n\nGet the data: These examples use the swqmispublicdata.txt data in the example data\nSWQMIS column names have spaces and characters in them. This make it difficult to refer to those variables because object names cannot have spaces in R. The janitor package has a handy function to automatically format column/variable names in a dataframe and we will use it here.\n\n# install.packages(\"janitor\")\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(twriTemplates)\n\ndf <- read_delim(\"data/swqmispublicdata.txt\",\n                 delim = \"|\",\n                 # see awkward column names that have to be wrapped\n                 # in between \"`\" (escape) marks.\n                 col_types = cols_only(\n                   `Segment ID` = col_character(),\n                   `Station ID` = col_factor(),\n                   `Station Description` = col_character(),\n                   `End Date` = col_date(format = \"%m/%d/%Y\"),\n                   `Collecting Entity` = col_character(),\n                   `Monitoring Type` = col_character(),\n                   `Composite Category` = col_character(),\n                   `Parameter Name` = col_character(),\n                   `Parameter Code` = col_character(),\n                   `Value` = col_number(),\n                   `RFA/Tag ID` = col_character()\n                 )) |> \n  clean_names()\n  \ndf\n\n# A tibble: 5,648 × 11\n   segment_id station_id station_description         end_date   collecting_enti…\n   <chr>      <fct>      <chr>                       <date>     <chr>           \n 1 1502       12517      TRES PALACIOS CREEK AT FM … 1987-09-22 TCEQ REGIONAL O…\n 2 1502       12517      TRES PALACIOS CREEK AT FM … 1987-09-22 TCEQ REGIONAL O…\n 3 1502       12517      TRES PALACIOS CREEK AT FM … 1987-09-22 TCEQ REGIONAL O…\n 4 1502       12517      TRES PALACIOS CREEK AT FM … 1987-09-22 TCEQ REGIONAL O…\n 5 1502       12517      TRES PALACIOS CREEK AT FM … 1987-09-22 TCEQ REGIONAL O…\n 6 1502       12517      TRES PALACIOS CREEK AT FM … 1987-09-22 TCEQ REGIONAL O…\n 7 1502       12517      TRES PALACIOS CREEK AT FM … 1987-09-22 TCEQ REGIONAL O…\n 8 1502       12517      TRES PALACIOS CREEK AT FM … 1987-09-22 TCEQ REGIONAL O…\n 9 1502       12517      TRES PALACIOS CREEK AT FM … 1987-09-22 TCEQ REGIONAL O…\n10 1502       12517      TRES PALACIOS CREEK AT FM … 1987-09-22 TCEQ REGIONAL O…\n# … with 5,638 more rows, and 6 more variables: monitoring_type <chr>,\n#   composite_category <chr>, parameter_name <chr>, parameter_code <chr>,\n#   value <dbl>, rfa_tag_id <chr>\n\n\n\n6.3.1 Summary Stats\nThe most common summary statistics are mean or geometric means. How we report this info depends on the project. For example, some projects might group all the stations within a single assessment unit together and report a mean. In other projects, station-specific means might be relevant. It is usually useful to explore both. An example of calculating station-specific geometric means is below:\n\n#install.packages(DescTools)\nlibrary(DescTools)\ndf |> \n  # parameter code for E. coli\n  filter(parameter_code == \"31699\") |> \n  group_by(station_id) |> \n  summarise(geomean = Gmean(value),\n            min = min(value),\n            max = max(value),\n            n = n(),\n            first_date = min(end_date),\n            last_date = max(end_date))\n\n# A tibble: 2 × 7\n  station_id geomean   min   max     n first_date last_date \n  <fct>        <dbl> <dbl> <dbl> <int> <date>     <date>    \n1 12517         118.   6.3 24192    88 2000-12-20 2022-05-17\n2 15325         210.   1    2400    35 2019-06-25 2022-05-17\n\n\n\n6.3.2 Exceedance Probabilities\nAnother relevant measure is the number of exceedances of the criterion. This is relevant because we can use a binomial test to evaluate the probability that a water body is above an acceptable exceedance rate (typically 0.1 or 0.2 depending on the parameter). Smith et al. (2001) is highly recommended reading about why and how the binomal test is applied to assess water quality conditions.\nThe following example evaluates the exceedance rate for total phosphorus. For total phosphorus, the screening level in Texas is currently 0.69 mg/L which is evaluated using the binomal test and a 20% exceedance rate.\n(https://www.tceq.texas.gov/downloads/water-quality/assessment/integrated-report-2022/2022-guidance.pdf)\n\nexceedance_df <- df |> \n  ## filter to total phos\n  filter(parameter_code == \"00665\") |>\n  filter(end_date >= as.Date(\"2013-12-01\") &\n           end_date <= as.Date(\"2020-11-30\")) |> \n  filter(monitoring_type == \"Routine - Monitoring not intentionally targeted toward any environmental condition or event\") |> \n  ## create a new variable 1 = exceedance, 0 = non-exceedance\n  mutate(\n    exceedance = case_when(\n      value < 0.69 ~ 0,\n      value >= 0.69 ~ 1\n    )) |> \n  summarize(total = n(),\n            n_exceedance = sum(exceedance),\n            probability = n_exceedance/total)\n\nexceedance_df\n\n# A tibble: 1 × 3\n  total n_exceedance probability\n  <int>        <dbl>       <dbl>\n1    18            2       0.111\n\n\nThis information nicely matches the assessment summary information provided for AU 1502_01 in the 2022 Texas Integrated Report.\nIf we want to run the binomial test on this data:\n\nbinom.test(x = exceedance_df$n_exceedance,\n           n = exceedance_df$total,\n           p = 0.2, alternative = \"g\")\n\n\n    Exact binomial test\n\ndata:  exceedance_df$n_exceedance and exceedance_df$total\nnumber of successes = 2, number of trials = 18, p-value = 0.9009\nalternative hypothesis: true probability of success is greater than 0.2\n95 percent confidence interval:\n 0.02011068 1.00000000\nsample estimates:\nprobability of success \n             0.1111111 \n\n\nThe null hypothesis is that the probability of TP exceeding 0.69 is less than or equal to 0.2, which the binomial test failed to reject (p>0.05). Thus, the assessment unit meets the water quality screening level."
  },
  {
    "objectID": "water-quality.html#figures",
    "href": "water-quality.html#figures",
    "title": "6  Water Quality Data",
    "section": "\n6.4 Figures",
    "text": "6.4 Figures\nTypically, we develop at least the following water quality figures:\n\nscatterplots of measured values over time by waterbody and station\nboxplots or histograms displaying the distribution of measured values (upstream to downstream is particularly useful)\ndepending on the amount of data, a “rolling” statistic like exceedance probability, mean or geometric mean\n\n\n# install.packages(\"ggtext\")\nlibrary(ggtext)\n\necoli <- df |> \n  filter(parameter_code == \"31699\")\n\n## the E. coli critera is 126 MPN/100mL\ncritera <- 126\n\nggplot(ecoli) +\n  ## add a dotted line to show the water quality criteria\n  geom_hline(yintercept = critera, linetype = 2) +\n  ## measured points colored by the station\n  geom_point(aes(end_date, value, color = station_id)) +\n  ## this will add a label to the dotted line\n  annotate(\"label\", \n           x = max(ecoli$end_date)+60, \n           y = critera,\n           label = \"126 MPN/100 mL\",\n           hjust = 0,\n           size = 4,\n           family = \"OpenSansCondensed_TWRI\") +\n  ## need to make some space at the end of the plot for\n  ## the label (adds 5% and 20% space to start and end)\n  scale_x_date(expand = expansion(mult = c(0.05, 0.20))) +\n  ## use log transformed spacing on the y-axis\n  scale_y_log10() +\n  scale_color_discrete_twri(name = \"Stations\") +\n  labs(x = \"Date\", y = \"*E. coli* [MPN/100 mL]\") +\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown())\n\n\n\n\nBoxplots are used to display distribution of values. The twriTemplates package has a function to include a boxplot legend that explains the components of a boxplot.\n\n# install.packages(\"patchwork\")\nlibrary(patchwork)\n\nbplot <- ggplot(ecoli) +\n  ## add a dotted line to show the water quality criteria\n  geom_hline(yintercept = critera, linetype = 2) +\n  ## measured points colored by the station\n  geom_boxplot(aes(station_id, value, fill = station_id)) +\n  scale_y_log10() +\n  scale_fill_discrete_twri(name = \"Stations\") +\n  labs(x = \"Station\", y = \"*E. coli* [MPN/100 mL]\") +\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown())\n\nlegend <- ggplot_box_legend(point_label = \" values outside the\\ninterquartile range\")\n\n# the ggplot_box_legend function is in the twriTemplates package\nbplot + legend + plot_layout(design = \"112\")\n\n\n\n\n\n6.4.1 Rolling statistics\nRolling statistics are a useful way of displaying how a statistical measure (mean, geomean, etc.) changes over time. These are typically done with regularly measured data with equal intervals and is easily calculated using dplyr window functions. However, water quality data is unequal and we are generally interested in the statistic over a specific time window regardless of the number of samples in that window (so we can’t just look back \\(n\\) rows and calculate the statistic). The runner packages solves this by letting us set the window by amount of time.1 runner() utilizes the date column and looks back to all the data within the time frame specified in the k argument, then uses that data to calculate the function in argument f.1 Some of our older scripts used the tbrf package to do this. runner is now reccomended because it is much faster and more flexible.\n\n# install.packages(\"runner\")\nlibrary(runner)\n\n# first calculate the rolling statistic\n\nrollingmean <- df %>%\n    # parameter code for E. coli\n  filter(parameter_code == \"31699\") %>% \n  group_by(station_id) %>%\n  arrange(end_date) %>%\n  mutate(Gmean = runner(x = value,\n                        k = \"7 years\",\n                        idx = end_date,\n                        f = function(x) DescTools::Gmean(x)))\n\n\nggplot(rollingmean) +\n  geom_hline(yintercept = critera, linetype = 2) +\n  geom_point(aes(end_date, value, color = station_id)) +\n  geom_step(aes(end_date, Gmean, color = station_id)) +\n  scale_y_log10() +\n  scale_color_discrete_twri(name = \"Stations\") +\n  labs(x = \"Date\", y = \"*E. coli* [MPN/100 mL]\") +\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown())\n\n\n\n\nWe can also add confidence intervals to this plot. This is a good way to communicate how certain we are that the geometric mean is actually above or below the water quality criterion.\n\nrollingmean <- rollingmean %>%\n  mutate(Gmean_lwr = runner(x = value,\n                        k = \"7 years\",\n                        idx = end_date,\n                        f = function(x) {\n                          DescTools::Gmean(x, \n                                           conf.level = 0.95)[2]\n                          }),\n         Gmean_upr = runner(x = value,\n                        k = \"7 years\",\n                        idx = end_date,\n                        f = function(x) {\n                          DescTools::Gmean(x, \n                                           conf.level = 0.95)[3]\n                          }))\n\n\nggplot(rollingmean) +\n  geom_hline(aes(yintercept = critera, linetype = \"Criteria\"), alpha = 0.5) +\n  geom_point(aes(end_date, value, color = \"Measured *E. coli* value\")) +\n  geom_step(aes(end_date, Gmean, linetype = \"7-yr rolling geomean\")) +\n  geom_step(aes(end_date, Gmean_upr, linetype = \"95% confidence interval\")) +\n  geom_step(aes(end_date, Gmean_lwr, linetype = \"95% confidence interval\")) +\n  scale_y_log10(limits = c(1,10000)) +\n  facet_wrap(~station_id, scales = \"free_x\") +\n  scale_color_manual(values = \"steelblue\") +\n  labs(x = \"Date\", y = \"*E. coli* [MPN/100 mL]\") +\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown(),\n        # turn off legend title\n        legend.title = element_blank(),\n        legend.text = element_markdown(),\n        # turn off the horizontal gridlines\n        # because they look similar to the \n        # 95% confidence intervals\n        panel.grid = element_blank())"
  },
  {
    "objectID": "water-quality.html#notes-on-figures",
    "href": "water-quality.html#notes-on-figures",
    "title": "6  Water Quality Data",
    "section": "\n6.5 Notes on figures",
    "text": "6.5 Notes on figures\n\n6.5.1 Log Transformations\nBacteria data is typically log distributed. Analysis of bacteria data almost always involve a log transformation to meet distributional assumptions in many statistical tests or regression models. Although in some fields, it is common to display these results in the log-transformed values, our standard practice is to back-transform the data and display the data with a log-transformed axis or scale.\nA brief example is shown below:\n\nex_data <- tibble(dates = seq.Date(as.Date(\"2005-01-01\"), as.Date(\"2020-12-31\"), length.out = 100),\n             x = rlnorm(100, log(126), log(5)))\n\nggplot(ex_data) +\n    geom_point(aes(dates, log(x)), color = \"#0054a4\") +\n  labs(x = \"Date\", \n       y =\"log *E. coli* [MPN/100 mL]\",\n       title = \"Log-Transformed Values\") +\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown())-> p1\n\nggplot(ex_data) +\n  geom_point(aes(dates, x), color = \"#0054a4\") +\n  scale_y_log10() +\n  labs(x = \"Date\", \n       y = \"*E. coli* [MPN/100mL]\",\n       title = \"Log-10 Y-Axis Scale\") +\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown()) -> p2\n\np1 + p2\n\n\n\n\nThe locations of the points on both plots are the same, but the scale on the left shows the log-transformed values. Conversely, the scale on the left shows the actual values but with scale log-transformed. For general audience it is easier to discern the magnitude difference between \\(10\\) and \\(10,0000\\) E. coli than between \\(2.3026\\) and \\(9.21034\\) \\(log(E. coli)\\). If your audience are microbiologists or some field where reporting log-values are standard and the audience members intuit differences in log vaalues, don’t follow this advice.\n\n6.5.2 Error Bars\nError bars are conventionally used to display the uncertainty in estimates, not the variability or distribution of measured data. Box-plots and histograms are the appropriate graph to show measurement variability. Where error bars are used, indicate if the bars represent the standard error of the mean or confidence intervals. Do not use error bars for standard deviations or maximum/minimum measurements.\n\ndf <- readr::read_csv(\"data/easterwood.csv\",\n                col_types = \"cDcnn\")\n\ndf %>%\n  mutate(month = lubridate::month(date, label = TRUE),\n         year = lubridate::year(date)) %>%\n  ggplot() +\n  geom_boxplot(aes(month,dailymaximumdrybulbtemperature)) +\n  theme_TWRI_print() +\n  labs(x = \"Month\",\n       y = \"Maximum daily temperature [°F]\",\n       title = \"Boxplots Show Distribution\",\n       subtitle = \"Distribution of Daily High Temperature\") -> p1\ndf %>%\n  mutate(month = lubridate::month(date, label = TRUE),\n         year = lubridate::year(date)) %>%\n  group_by(month, year) %>%\n  summarise(mean_monthly =  mean(dailymaximumdrybulbtemperature),\n            .groups = \"drop_last\") %>%\n  summarise(mean_temp = mean(mean_monthly),\n            mean_se = 2*DescTools::MeanSE(mean_monthly)) %>%\n  ggplot() +\n  geom_pointrange(aes(x = month, y = mean_temp, ymin = mean_temp-mean_se, ymax = mean_temp+mean_se)) +\n  theme_TWRI_print() +\n  labs(x = \"Month\",\n       y = \"Maximum daily temperature [°F]\",\n       subtitle = \"Estimated Mean Monthly High +/- 95% CI\",\n       title = \"Error Bars Show Uncertaintity\") -> p2\ndf %>%\n  mutate(month = lubridate::month(date, label = TRUE),\n         year = lubridate::year(date)) %>%\n  group_by(month) %>%\n  summarise(mean_temp = mean(dailymaximumdrybulbtemperature),\n            bad = 2*sd(dailymaximumdrybulbtemperature)) %>%\n  ggplot() +\n  geom_pointrange(aes(x = month, y = mean_temp, ymin = mean_temp-bad, ymax = mean_temp+bad)) +\n  theme_TWRI_print() +\n  labs(x = \"Month\",\n       y = \"Maximum daily temperature [°F]\",\n       subtitle = \"Estimated monthly mean high temperatures +/- 2 sd.\",\n       title = \"Don't Do This!\") -> p3\n(p1 + p2) / (p3 + plot_spacer())\n\n\n\n\n\n\n\n\nSmith, E.P., Ye, K., Hughes, C., and Shabman, L. 2001. Statistical Assessment of Violations of Water Quality Standards under Section 303(d) of the Clean Water Act. Environmental Science & Technology 35 (3): 606–12. https://doi.org/10.1021/es001159e."
  },
  {
    "objectID": "load-duration.html#data",
    "href": "load-duration.html#data",
    "title": "7  Load Duration Curves",
    "section": "\n7.1 Data",
    "text": "7.1 Data\nThis example uses E. coli bacteria concentrations collected at SWQMIS station 12517 on Tres Palacios Creek. Streamflow data comes from the co-located USGS streamgage 08162600.\n\n7.1.1 Water Quality\n\n\nGet the data: These examples use the swqmispublicdata.txt data in the example data\nWe will import water quality data the same way that was covered in Section 6.\n\n# install.packages(\"janitor\")\nlibrary(tidyverse)\nlibrary(ggtext)\nlibrary(janitor)\nlibrary(dataRetrieval)\nlibrary(DescTools)\nlibrary(twriTemplates)\n\ndf <- read_delim(\"data/swqmispublicdata.txt\",\n                 delim = \"|\",\n                 # see awkward column names that have to be wrapped\n                 # in between \"`\" (escape) marks.\n                 col_types = cols_only(\n                   `Segment ID` = col_character(),\n                   `Station ID` = col_factor(),\n                   `Station Description` = col_character(),\n                   `End Date` = col_date(format = \"%m/%d/%Y\"),\n                   `Collecting Entity` = col_character(),\n                   `Monitoring Type` = col_character(),\n                   `Composite Category` = col_character(),\n                   `Parameter Name` = col_character(),\n                   `Parameter Code` = col_character(),\n                   `Value` = col_number(),\n                   `RFA/Tag ID` = col_character()\n                 )) |> \n  clean_names() |> \n  filter(station_id == \"12517\") |> \n  filter(parameter_code == \"31699\")\n  \n\ndf |> glimpse()\n\nRows: 88\nColumns: 11\n$ segment_id          <chr> \"1502\", \"1502\", \"1502\", \"1502\", \"1502\", \"1502\", \"1…\n$ station_id          <fct> 12517, 12517, 12517, 12517, 12517, 12517, 12517, 1…\n$ station_description <chr> \"TRES PALACIOS CREEK AT FM 456\", \"TRES PALACIOS CR…\n$ end_date            <date> 2001-03-14, 2000-12-20, 2001-06-21, 2002-01-10, 2…\n$ collecting_entity   <chr> \"TCEQ REGIONAL OFFICE\", \"TCEQ REGIONAL OFFICE\", \"T…\n$ monitoring_type     <chr> \"Routine - Monitoring not intentionally targeted t…\n$ composite_category  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ parameter_name      <chr> \"E. COLI, COLILERT, IDEXX METHOD, MPN/100ML\", \"E. …\n$ parameter_code      <chr> \"31699\", \"31699\", \"31699\", \"31699\", \"31699\", \"3169…\n$ value               <dbl> 10.0, 85.0, 183.0, 537.0, 52.0, 74.0, 24192.0, 169…\n$ rfa_tag_id          <chr> \"R194283\", \"R194025\", \"R195800\", \"R200079\", \"R2013…\n\n\n\n7.1.2 Hydrology\nStreamflow data is obtained from the USGS NWIS using the dataRetrieval package. It is important to note that streamflow data might not be static. USGS might update the data at some point due to new rating curves, data quality checks, etc. We want to work off a snapshot of the data.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn a project workflow, this would be in a data download script that saves your downloaded data to a csv. Then in your analysis script, read the csv data. You can rerun the analysis script without having to re-download the data and possibly running into changes in the source data. Frequent repeated downloads might also cause server issues or your IP getting temporarily blocked or rate-limited by the NWIS server.\n\n\n\n# This should be in a separate data download script!\nQ_df <- readNWISdv(siteNumbers = \"08162600\",\n           startDate = \"2000-01-01\",\n           endDate = \"2020-12-31\",\n           parameterCd = \"00060\",\n           statCd = \"00003\")\nQ_df <- renameNWISColumns(Q_df) |> \n  clean_names()\n\n# save the data and work off the saved data\n# write_csv(Q_df, \"data/streamflow_0816200.csv\")\n# Q_df <- read_csv(\"data/streamflow_0816200.csv\")\n\nQ_df |> glimpse()\n\nRows: 7,671\nColumns: 5\n$ agency_cd <chr> \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USG…\n$ site_no   <chr> \"08162600\", \"08162600\", \"08162600\", \"08162600\", \"08162600\", …\n$ date      <date> 2000-01-01, 2000-01-02, 2000-01-03, 2000-01-04, 2000-01-05,…\n$ flow      <dbl> 0.84, 3.00, 3.40, 2.60, 1.60, 3.20, 11.00, 17.00, 22.00, 18.…\n$ flow_cd   <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", …"
  },
  {
    "objectID": "load-duration.html#manual-method",
    "href": "load-duration.html#manual-method",
    "title": "7  Load Duration Curves",
    "section": "\n7.2 Manual Method",
    "text": "7.2 Manual Method\nFirst we will go through a manual method for developing the FDC and LDC. I go through this becuae it is worth understanding the specific steps for developing the FDC and LDC. the ldc is available to streamline these steps, but I highly reccomend understanding the fundamentals first.\n\n7.2.1 Flow Duration Curve\nThe primary step in calculating the FDC is calculating the exceedance probability for each streamflow value. Without delving into statistics, there are a surprising number of ways to accomplish this. If you have lots of streamflow values, the method you chose does not matter much (Vogel and Fennessey 1994). For manual calculations, we typically calculate the exceedance probability as the rank value of a given flow divided by the the number of streamflow values plus 1 (Morrison and Bonta 2008):\n\\[\np_i = \\frac{i}{n+1}\n\\] where \\(p_i\\) is the exceedance probability, \\(i\\) is the rank number of a given streamflow and \\(n\\) is the number of observations. \\(p_i\\) is also called the Weibull plotting position which is the mean of the cumulative distribution function of the \\(i\\)th observation (Gumbel 1958).\nThere are two methods for calculating this in dplyr. The first involves the direct calculation by ranking flows in descending order and dividing by length plus one. The second involves using the ppoints() function in R which returns ordered probability points for a given vector. by setting a=0 we specify that the function returns the Weibull plotting positions.\n\nQ_df <- Q_df |> \n  select(date, flow) |> \n  arrange(flow) |> \n  mutate(\n    ## direct calculation if you prefer\n    flow_exceedance = rank(desc(flow), ties.method = \"last\")/(length(flow)+1),\n    ## or weibull pp function, you don't need to do both!\n    flow_exceedance_1 = 1-ppoints(flow, a = 0))\n\nggplot(Q_df) +\n  geom_line(aes(flow_exceedance, flow)) +\n  scale_y_log10() +\n  theme_TWRI_print() +\n  labs(y = \"Mean Daily Flow [cfs]\", x = \"Proportion of Days Flow Exceeded\")\n\n\n\n\nThe resulting FDC shows us the percent of time over the entire period of record that mean daily streamflows were exceeded. For example the above figure shows the max streamflow was around 10,000 cfs and the minimum was less than 1. Also 80% of the time, streamflows exceeded about 10 cfs.\n\n7.2.2 Load Duration Curve\nNow the FDC can be converted to an LDC by multiplying the mean daily streamflow volume by the allowable bacteria concentration. The general steps in your head should be:\n\nconvert mean daily discharge (cfs) to daily volume for water (cubic feet, mL, whatever);\nmultiply the measured pollutant concentration by the daily volume\n\nThis results in total mass or counts of pollutant per day.\n\nQ_df <- Q_df |> \n  # We don't need both flow exceedance columns\n  select(-c(flow_exceedance_1)) |> \n  # MPN/100mL * cubic feet/sec * mL/cubic feet * sec/day = mpn/day\n  mutate(ldc = (126/100) * flow * 28316.8 * 86400)\n\nggplot(Q_df) +\n  geom_line(aes(flow_exceedance, ldc)) +\n  scale_y_log10() +\n  theme_TWRI_print() +\n  labs(y = \"*E. coli* [MPN/day]\", x = \"Proportion of Days Load Exceeded\") +\n  theme(axis.title.y = element_markdown())\n\n\n\n\nThe LDC looks exactly the same as the FDC, the units on the y-axis change to pollutant load per day. The next step is to add the measured concentrations to the figure. We need to join the bacteria data to the flow data, then calculate the measured loads. In order to plot this and label the legends properly, we need to manually set some of the aesthtic values in ggplot2().\n\necoli_df <- df |> \n  select(station_id, end_date, parameter_code, value)\n\nQ_df <- Q_df |> \n  left_join(ecoli_df, by = c(\"date\" = \"end_date\")) |> \n  mutate(measured_load = (value/100) * flow * 28316.8 * 86400)\n\nggplot(Q_df) +\n  geom_line(aes(flow_exceedance, ldc,\n                linetype = \"Allowable Load at Geomean Criterion (126 MPN/100 mL)\")) +\n  geom_point(aes(flow_exceedance, measured_load,\n                 shape = \"Measurement Value (MPN/day)\",\n                 color = \"Measurement Value (MPN/day)\")) +\n  scale_y_log10() +\n  scale_shape_manual(name = \"values\", values = c(21)) +\n  scale_color_manual(name = \"values\", values = c(\"dodgerblue4\")) +\n  theme_TWRI_print() +\n  labs(y = \"*E. coli* [MPN/day]\", x = \"Proportion of Days Load Exceeded\") +\n  theme(axis.title.y = element_markdown(),\n        legend.direction = \"vertical\",\n        legend.title = element_blank())\n\n\n\n\nWe can already see general trends in the LDC and bacteria data. There is clearly a higher variance and probably a higher difference in median measured load and allowable load at high flows. As flows decrease (on the right hand side of the graph), a higher proportion of measured loads appear to be below the allowable load line.\nThere are several ways to quantify this. The easiest to explain to the general audience is to (1) split the flows into different regimes, (2) take the geomean of the loads within the flow regime, and (3) take the difference between geomean measured load and the median allowable load. Alternatively, we could fit a log-regression, LOADEST or generalized additive model to the data to estimate load across all exceedance percentiles. The former approach is shown below:\n\n# create a summary table\nload_summary <- Q_df |> \n  # classify flow conditions based on exceedance\n  mutate(flow_condition = case_when(\n    flow_exceedance >= 0 & flow_exceedance < 0.1 ~ \"Highest Flows\",\n    flow_exceedance >= 0.1 & flow_exceedance < 0.4 ~ \"Moist Conditions\",\n    flow_exceedance >= 0.4 & flow_exceedance < 0.6 ~ \"Mid-range Conditions\",\n    flow_exceedance >= 0.6 & flow_exceedance < 0.9 ~ \"Dry Conditions\",\n    flow_exceedance >= 0.9 & flow_exceedance <= 1 ~ \"Lowest Flows\"\n  )) |> \n  group_by(flow_condition) |> \n  summarize(median_flow = quantile(flow, 0.5, type = 5, \n                                   names = FALSE, na.rm = TRUE),\n            median_p = round(quantile(flow_exceedance, \n                                      .5, type = 5, names = FALSE, \n                                      na.rm = TRUE), 2),\n            geomean_ecoli = Gmean(value, na.rm = TRUE),\n            allowable_load = median_flow * 126/100 * 28316.8 * 86400,\n            geomean_load = median_flow * geomean_ecoli/100 * 28316.8 * 86400,\n            reduction_needed = case_when(\n              allowable_load < geomean_load ~ geomean_load - allowable_load,\n              allowable_load >= geomean_load ~ 0),\n            percent_reduction_needed = reduction_needed/geomean_load *100) |> \n  arrange(median_p) |> \n  mutate(flow_condition = as_factor(flow_condition))\n  \n\nload_summary\n\n# A tibble: 5 × 8\n  flow_condition  median_flow median_p geomean_ecoli allowable_load geomean_load\n  <fct>                 <dbl>    <dbl>         <dbl>          <dbl>        <dbl>\n1 Highest Flows         541       0.05         459.         1.67e12      6.07e12\n2 Moist Conditio…        40.7     0.25         205.         1.25e11      2.04e11\n3 Mid-range Cond…        19.7     0.5           95.8        6.07e10      4.62e10\n4 Dry Conditions         12.6     0.75          68.3        3.88e10      2.10e10\n5 Lowest Flows            6.9     0.95         103.         2.13e10      1.74e10\n# … with 2 more variables: reduction_needed <dbl>,\n#   percent_reduction_needed <dbl>\n\n\nNow we have some summary data that includes median values for each flow regime and estimates of required reductions. The next step is to add some info to the ggplot.\n\n# set the y-axis value for the flow-regime labels\nlabel_max <- max(Q_df$measured_load, na.rm = TRUE) + \n  (0.5 * max(Q_df$measured_load, na.rm = TRUE))\n\nggplot(Q_df) +\n  ## add some lines to indicate flow regimes\n  geom_vline(xintercept = c(.10, .40, .60, .90), color = \"#cccccc\") +\n  ## add ldc line\n  geom_line(aes(flow_exceedance, ldc,\n                linetype = \"Allowable Load at Geomean Criterion (126 MPN/100 mL)\")) +\n  ## add measured loads\n  geom_point(aes(flow_exceedance, measured_load,\n                 shape = \"Measurement Value (MPN/day)\",\n                 color = \"Measurement Value (MPN/day)\")) +\n  ## add summarized measured loads\n  geom_point(data = load_summary, aes(median_p, geomean_load,\n                                      shape = \"Exisiting Geomean Load (MPN/day)\",\n                                      color = \"Exisiting Geomean Load (MPN/day)\")) +\n  ## log10 y-axis\n  scale_y_log10() +\n  ## shrink the ends of the x-axis a little bit\n  scale_x_continuous(expand = c(0.005,0.005)) +\n  ## manually set the shapes for the point aesthetics\n  scale_shape_manual(name = \"values\", values = c(12, 21)) +\n  ## manually set the shapes for the color aesthetic\n  scale_color_manual(name = \"values\", values = c(\"red\", \"dodgerblue4\")) +\n  ## I like this tick marks that indicate a log transformed scale\n  annotation_logticks(sides = \"l\", color = \"#cccccc\") +\n  ## add some labels to the flow-regimes\n  annotate(\"text\", x = .05, y = label_max, \n           label = \"High\\nflows\", hjust = 0.5, size = 3, \n           family = \"OpenSansCondensed_TWRI\", lineheight = 1) +\n  annotate(\"text\", x = .25, y = label_max, \n           label = \"Moist\\nconditions\", hjust = 0.5, size = 3, \n           family = \"OpenSansCondensed_TWRI\", lineheight = 1) +\n  annotate(\"text\", x = .50, y = label_max, \n           label = \"Mid-range\\nflows\", hjust = 0.5, size = 3, \n           family = \"OpenSansCondensed_TWRI\", lineheight = 1) +\n  annotate(\"text\", x = .75, y = label_max, \n           label = \"Dry\\nconditions\", hjust = 0.5, size = 3, \n           family = \"OpenSansCondensed_TWRI\", lineheight = 1) +\n  annotate(\"text\", x = .95, y = label_max, \n           label = \"Low\\nflows\", hjust = 0.5, size = 3, \n           family = \"OpenSansCondensed_TWRI\", lineheight = 1) +\n  ## labels\n  labs(y = \"*E. coli* [MPN/day]\", x = \"Proportion of Days Load Exceeded\") +\n  ## general theme\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown(),\n        legend.direction = \"vertical\",\n        legend.title = element_blank())"
  },
  {
    "objectID": "load-duration.html#ldc-package",
    "href": "load-duration.html#ldc-package",
    "title": "7  Load Duration Curves",
    "section": "\n7.3 ldc Package",
    "text": "7.3 ldc Package\nSome of the steps described above cna be skipped by using the ldc package. The package includes functions for period of record LDCs and annualized LDCs which are not covered here. One important functionality introduced with the package is the use of units.\n\n# install.packages(\"ldc\", \n#                  repos = c(txwri = 'https://txwri.r-universe.dev', \n#                            CRAN = 'https://cloud.r-project.org'))\n\nlibrary(ldc)\nlibrary(units)\n\n## ldc uses the unit package to facilitate unit conversions\n## we need to make the cfu unit first, since it isn't included \n## in the units package\ninstall_unit(\"MPN\")\n\n\n## get a new clean dataframe with column of Date, flows, and E. coli\nQ_df <- readNWISdv(siteNumbers = \"08162600\",\n           startDate = \"2000-01-01\",\n           endDate = \"2020-12-31\",\n           parameterCd = \"00060\",\n           statCd = \"00003\") |> \n  renameNWISColumns(Q_df) |> \n  clean_names() |> \n  left_join(ecoli_df, by = c(\"date\" = \"end_date\")) |> \n  ## attach a unit to streamflow\n  mutate(flow = set_units(flow, \"ft^3/s\"),\n         value = set_units(value, \"MPN/100mL\"))\n\nQ_df |> glimpse()\n\nRows: 7,671\nColumns: 8\n$ agency_cd      <chr> \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\",…\n$ site_no        <chr> \"08162600\", \"08162600\", \"08162600\", \"08162600\", \"081626…\n$ date           <date> 2000-01-01, 2000-01-02, 2000-01-03, 2000-01-04, 2000-0…\n$ flow           [ft^3/s] 0.84 [ft^3/s], 3.00 [ft^3/s], 3.40 [ft^3/s], 2.60 [f…\n$ flow_cd        <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", …\n$ station_id     <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ parameter_code <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ value          [MPN/100mL] NA [MPN/100mL], NA [MPN/100mL], NA [MPN/100mL], N…\n\n\nWith the dataframe setup, the calc_ldc() function will generate a exceedance probabilities and flow regime categories:\n\n## specify the allowable concentration\nallowable_concentration <- 126\n## set the units\nunits(allowable_concentration) <- \"MPN/100mL\"\n\ndf_ldc <- calc_ldc(Q_df, \n                   Q = flow, \n                   C = value, \n                   allowable_concentration = allowable_concentration,\n                   breaks = c(1, 0.9, 0.6, 0.4, 0.1, 0),\n                   labels = c(\"Highest Flows\", \n                              \"Moist Conditions\",\n                              \"Mid-range Flows\",\n                              \"Dry Conditions\",\n                              \"Low Flows\"))\ndf_ldc\n\n# A tibble: 7,671 × 13\n   agency_cd site_no  date          flow flow_cd station_id parameter_code value\n   <chr>     <chr>    <date>     [ft^3/… <chr>   <fct>      <chr>          [MPN…\n 1 USGS      08162600 2000-08-18    0.22 A       <NA>       <NA>              NA\n 2 USGS      08162600 2000-03-07    0.42 A       <NA>       <NA>              NA\n 3 USGS      08162600 2000-03-06    0.6  A       <NA>       <NA>              NA\n 4 USGS      08162600 2000-08-22    0.75 A       <NA>       <NA>              NA\n 5 USGS      08162600 2000-03-08    0.78 A       <NA>       <NA>              NA\n 6 USGS      08162600 2000-08-17    0.78 A       <NA>       <NA>              NA\n 7 USGS      08162600 2000-09-11    0.8  A       <NA>       <NA>              NA\n 8 USGS      08162600 2000-01-01    0.84 A       <NA>       <NA>              NA\n 9 USGS      08162600 2000-08-21    0.93 A       <NA>       <NA>              NA\n10 USGS      08162600 2000-03-05    1    A       <NA>       <NA>              NA\n# … with 7,661 more rows, and 5 more variables: Daily_Flow_Volume [100mL/d],\n#   Daily_Load [MPN/d], Allowable_Daily_Load [MPN/d], P_Exceedance <dbl>,\n#   Flow_Category <fct>\n\n\nWith the LDC information calculated, the summary table can be generated with summ_ldc():\n\ndf_sum <- summ_ldc(df_ldc, \n                   Q = flow, \n                   C = value, \n                   Exceedance = P_Exceedance,\n                   groups = Flow_Category,\n                   method = \"geomean\")\ndf_sum\n\n# A tibble: 5 × 6\n  Flow_Category Median_Flow Median_P Geomean_C Median_Daily_Fl… Median_Flow_Load\n  <fct>            [ft^3/s]    <dbl> [MPN/100…        [100mL/d]          [MPN/d]\n1 Highest Flows       541     0.0501     459.      13235973701.          6.07e12\n2 Moist Condit…        40.7   0.25       205.        995756247.          2.04e11\n3 Mid-range Fl…        19.7   0.5         95.8       481975382.          4.62e10\n4 Dry Conditio…        12.6   0.75        68.3       308268519.          2.10e10\n5 Low Flows             6.9   0.950      103.        168813713.          1.74e10\n\n\nWith the summary table we can finally plot the LDC:\n\ndraw_ldc(df_ldc,\n         df_sum,\n         label_nudge_y = log10(1000)) +\n  labs(y = \"*E. coli* [MPN/day]\") +\n  scale_y_log10() +\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown(),\n        legend.title = element_blank(),\n        legend.direction = \"vertical\")\n\n\n\n\nWe get the same outputs as the manual method with far fewer functions and less change for copy/paste errors. Another advantage of using ldc is the ability to change units on the fly without digging into a formula.\nFor example, the summary table shows median daily flow volume as 100ml/day. That isn’t an inuitive unit. Let’s report in million gallons per day instead by using the set_units() function:\n\ndf_sum <- df_sum |> \n  mutate(Median_Daily_Flow_Volume = set_units(Median_Daily_Flow_Volume,\n                                              \"1E6gallons/day\"))\n\ndf_sum\n\n# A tibble: 5 × 6\n  Flow_Category Median_Flow Median_P Geomean_C Median_Daily_Fl… Median_Flow_Load\n  <fct>            [ft^3/s]    <dbl> [MPN/100…   [1E6gallons/d]          [MPN/d]\n1 Highest Flows       541     0.0501     459.            350.            6.07e12\n2 Moist Condit…        40.7   0.25       205.             26.3           2.04e11\n3 Mid-range Fl…        19.7   0.5         95.8            12.7           4.62e10\n4 Dry Conditio…        12.6   0.75        68.3             8.14          2.10e10\n5 Low Flows             6.9   0.950      103.              4.46          1.74e10\n\n\nOften we report bacteria loads as million or billion counts per day:\n\ndf_sum <- df_sum |> \n  mutate(Median_Flow_Load = set_units(Median_Flow_Load,\n                                      \"1E9MPN/day\"))\n\ndf_sum\n\n# A tibble: 5 × 6\n  Flow_Category Median_Flow Median_P Geomean_C Median_Daily_Fl… Median_Flow_Load\n  <fct>            [ft^3/s]    <dbl> [MPN/100…   [1E6gallons/d]       [1E9MPN/d]\n1 Highest Flows       541     0.0501     459.            350.             6069. \n2 Moist Condit…        40.7   0.25       205.             26.3             204. \n3 Mid-range Fl…        19.7   0.5         95.8            12.7              46.2\n4 Dry Conditio…        12.6   0.75        68.3             8.14             21.0\n5 Low Flows             6.9   0.950      103.              4.46             17.4\n\n\nUpdate units in df_ldc also:\n\ndf_ldc <- df_ldc |> \n  mutate(Daily_Load = set_units(Daily_Load, \n                                \"1E9MPN/day\"),\n         Allowable_Daily_Load = set_units(Allowable_Daily_Load, \n                                          \"1E9MPN/day\"))\ndf_ldc\n\n# A tibble: 7,671 × 13\n   agency_cd site_no  date          flow flow_cd station_id parameter_code value\n   <chr>     <chr>    <date>     [ft^3/… <chr>   <fct>      <chr>          [MPN…\n 1 USGS      08162600 2000-08-18    0.22 A       <NA>       <NA>              NA\n 2 USGS      08162600 2000-03-07    0.42 A       <NA>       <NA>              NA\n 3 USGS      08162600 2000-03-06    0.6  A       <NA>       <NA>              NA\n 4 USGS      08162600 2000-08-22    0.75 A       <NA>       <NA>              NA\n 5 USGS      08162600 2000-03-08    0.78 A       <NA>       <NA>              NA\n 6 USGS      08162600 2000-08-17    0.78 A       <NA>       <NA>              NA\n 7 USGS      08162600 2000-09-11    0.8  A       <NA>       <NA>              NA\n 8 USGS      08162600 2000-01-01    0.84 A       <NA>       <NA>              NA\n 9 USGS      08162600 2000-08-21    0.93 A       <NA>       <NA>              NA\n10 USGS      08162600 2000-03-05    1    A       <NA>       <NA>              NA\n# … with 7,661 more rows, and 5 more variables: Daily_Flow_Volume [100mL/d],\n#   Daily_Load [1E9MPN/d], Allowable_Daily_Load [1E9MPN/d], P_Exceedance <dbl>,\n#   Flow_Category <fct>\n\n\nNow the updated units can be plotted on the LDC again:\n\ndraw_ldc(df_ldc,\n         df_sum,\n         label_nudge_y = log10(1000)) +\n  labs(y = \"*E. coli* [Billion MPN/day]\") +\n  ## make our labels more reader friendly\n  scale_y_log10(labels = scales::comma) +\n  theme_TWRI_print() +\n  theme(axis.title.y = element_markdown(),\n        legend.title = element_blank(),\n        legend.direction = \"vertical\")\n\n\n\n\n\n\n\n\nGumbel, Emil Julius. 1958. Statistics of Extreme. New York: Columbia University Press.\n\n\nMorrison, M.A., and Bonta, J.V. 2008. Development of Duration-Curve Based Methods for Quantifying Variability and Change in Watershed Hydrology and Water Quality. EPA/600/R-08/065. U.S. Environmental Protection Agency. https://nepis.epa.gov/Exe/ZyPURL.cgi?Dockey=P1000VR4.txt.\n\n\nU.S. Environmental Protection Agency. 2007. An Approach for Using Load Duration Curves in the Development of TMDLs. EPA 841-B-07-006. U.S. Environmental Protection Agency. https://nepis.epa.gov/Exe/ZyPURL.cgi?Dockey=P1008ZQA.txt.\n\n\nVogel, R.M., and Fennessey, N.M. 1994. Flow‐Duration Curves. I: New Interpretation and Confidence Intervals. Journal of Water Resources Planning and Management 120 (4): 485–504. https://doi.org/10.1061/(ASCE)0733-9496(1994)120:4(485)."
  },
  {
    "objectID": "maps.html",
    "href": "maps.html",
    "title": "8  Spatial Data",
    "section": "",
    "text": "Incomplete\n\n\n\nThis has not been written yet."
  },
  {
    "objectID": "watershed-delineation.html",
    "href": "watershed-delineation.html",
    "title": "9  Delineate Watersheds",
    "section": "",
    "text": "Incomplete\n\n\n\nThis has not been written yet."
  },
  {
    "objectID": "raster-summary.html",
    "href": "raster-summary.html",
    "title": "10  Extract Raster Summaries",
    "section": "",
    "text": "Incomplete\n\n\n\nThis has not been written yet."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Gumbel, Emil Julius. 1958. Statistics of Extreme. New York: Columbia\nUniversity Press.\n\n\nMorrison, M.A., and Bonta, J.V. 2008. Development of Duration-Curve\nBased Methods for Quantifying Variability and Change in Watershed\nHydrology and Water Quality. EPA/600/R-08/065. U.S. Environmental\nProtection Agency. https://nepis.epa.gov/Exe/ZyPURL.cgi?Dockey=P1000VR4.txt.\n\n\nNEON (National Ecological Observatory Network). n.d. Discharge Field\nCollection (Dp1.20048.001), RELEASE-2022. https://doi.org/10.48443/eaak-tt31.\n\n\nNOAA National Estuarine Research Reserve System (NERRS). 2022.\nSystem-Wide Monitoring Program. NOAA NERRS Centralized Data Management\nOffice. http://cdmo.baruch.sc.edu/.\n\n\nSmith, E.P., Ye, K., Hughes, C., and Shabman, L. 2001. Statistical\nAssessment of Violations of Water Quality Standards under Section 303(d)\nof the Clean Water Act. Environmental Science & Technology 35 (3):\n606–12. https://doi.org/10.1021/es001159e.\n\n\nU.S. Environmental Protection Agency. 2007. An Approach for Using Load\nDuration Curves in the Development of TMDLs. EPA 841-B-07-006. U.S.\nEnvironmental Protection Agency. https://nepis.epa.gov/Exe/ZyPURL.cgi?Dockey=P1008ZQA.txt.\n\n\nVogel, R.M., and Fennessey, N.M. 1994. Flow‐Duration Curves. I: New\nInterpretation and Confidence Intervals. Journal of Water Resources\nPlanning and Management 120 (4): 485–504. https://doi.org/10.1061/(ASCE)0733-9496(1994)120:4(485)."
  }
]